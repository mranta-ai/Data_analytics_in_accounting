## Introduction to Machine learning in accounting

In this chapter we will discus **shallow** machine learning models. Although neural networks can also be shallow, the discussion of them is postponed to Chapter 9.

### Ensemble methods

There is a saying: Two heads are better than one. What about even more heads? At least with machine learning, more heads is helpful (I am not sure about humans. :))

The idea of ensemble methods is to join many weak estimators as one efficient estimator. Using this approach, these methods achieve strong results. It is enough that the weak estimator is only slightly better than pure chance, their ensemble can still be a very efficient machine learning method.

**Example:** Let's assume that we have a weak estimator that can correctly predict the bankcruptcy of a company 52 % of a time. Thus, the predictor is only slightly better than pure chance (50 %).

However, an ensemble consisting of 100 weak estimators is correct 69,2 % of the time and an ensemble consisting of 1000 weak estimators is correct 90,3 %. of the time!

import scipy.stats as ss

binom_rv = ss.binom(100,0.52)
sum([binom_rv.pmf(i) for i in range(50,101)])

binom_rv = ss.binom(1000,0.52)
sum([binom_rv.pmf(i) for i in range(500,1001)])

In the following figure, one ellipse (a weak estimator) would a very bad classifier due to its incompatible shape with the two classes (the dots and diamonds). However, their ensemble is able to classify observations very well.

![Boost_ellips](./images/boost_ellips.png)

There are many options how the aggregate is calculated. It can be weighted average, majority, etc. depending on the application.

Very often the simple estimator in ensemble methods is the decision tree. In a decision tree, with conclusion based on the features of the model a tree structure is concstructed. From the leaves of the tree a prediction for the correct value/class can be inferred. The original decision tree structure gave only predictions for the correct class. The classification and regression trees (CART) have points instead classes in the leaves. This allows much more versatile interpretation and allows regression trees to be used also in regression applications.

Below is an example how decision trees are constructed. We have two features, equity ratio (ER) and return on assets (ROA). Based on these features, the companies are divided into three groups. First they are divided to two groups (ROA over or under *r*). Then companies in the (ROA < *r*) -group are divided based on the equity ratio (over or under *p*).

![dec_tree](./images/dec_tree.png)

The interpretation of symbols: diamond: no bankcruptcy risk, cross: low bankcruptcy risk, circle: high banckruptcy risk

![dec_tree](./images/dec_tree2.png)

The most common ensemble methods are bagging, random forest and boosting. They differ in how they decrease the correlation between their predictions. The benefit of ensemble methods increases when the correlation decreases.

The bagging (bootstrap aggregating) method decreases the correlation by feeding bootstrap samples to the weak estimators.

![bagging](./images/bagging.png)

The original random forest algorithm decreased correlation by feeding a subsample of features to the weak estimators (random subspace method). Later, the bootstrap aggregating of bagging was added to the method.

![random_forest](./images/rand_forest.png)

In recent year, boosting and especially gradient boosting has been a very popular ensemble method in applications.
In Boosting, weak estimators work in series. The idea is to feed the data again to a new weak learner so that the weight of misclassified points is increased. After training, the aggreaget estimate of the weak learners is calculated as a weighted mean. The largest weight is given to those learners, whose error function value was smallest.

![boosting](./images/boost.png)



Xgboost has probably been the most succesfull boosting method. It is very often behind the winning solutions of different machine learning competitions ([www.kaggle.com](https://www.kaggle.com)). Here is a short info from the Xgboost github-page: "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Kubernetes, Hadoop, SGE, MPI, Dask) and can solve problems beyond billions of examples."

Later in the book, we will see an example using Xgboost.

### Support vector machines
The Support Vector Machine (SVM) was previously one of the most popular algorithms in modern machine learning. It often provides very impressive classification performance on reasonably sized datasets. However, SVMs have difficulties with large datasets since the computations donâ€™t scale well with the number of training examples. This poor performance with large datasets hinders somewhat their success in big data and is the reason why neural networks have partly replaced SVMs in that field. However, in accounting we have often datasets of modest size and SVMs work very well with them.

![SVM](./images/SVM_margin.png)

### Key ML libraries in Python

#### Numpy

Although Numpy is not exactly a machine learning library, it is the backbone of many other ML libraries and the most important library for numerical computing in Python. Therefore, we start our journey of ML libraries with the basics of NumPy.

The key feature of Numpy is its flexible and fast multidimensional **ndarray** that can contain large datasets. It enables mathematical operations between arrays in a way that is very similar to calculations with scalars.

import numpy as np

**randint()** from the **random** module can be used to create random integer values from a specified interval.

random_values = np.random.randint(100,200,(4,4))

random_values

Mathematical operations are then very easy to perform. The default is almost always element-vise operations.

random_values*10

random_values/100

random_values + random_values

np.log(random_values)

Every Numpy array has a shape parameter that can be used to check the shape of your array.

random_values.shape

Notice that the array can be, and very commonly is in machine learnig, more than two-dimensional. Here is a four-dimensional array.

randoms = np.random.randint(10,20,(2,2,2,2))

randoms

Numpy has **array()** for creating Numpy arrays. Many kinds of collections are accepted as inputs.

sample_list = [i**2 for i in range(10)]

sample_list

sample_np = np.array(sample_list)

sample_np

Two-dimensional arrays can be built from list of lists, etc.

sample_list2 = [[i,i**2] for i in range(10)]

sample2_np = np.array(sample_list2)

sample2_np.shape

You can check the dimensions with **ndim**.

sample2_np.ndim

You can quickly create arrays of zeros and ones with **zeros()** and **ones()**.

np.zeros((2,3))

np.ones((3,2))

Python is flexible, because you do not need to specify the datatype of your variables. Python will recognise it automatically. However, with low-level languages, like C, you need to specify the type of your data. Therefore, to enable efficient computing and connection to low-level languages, Numpy uses a special **dtype** object to define its arrays.

sample2_np.dtype

np.array([[1.1,2.2],[3.3,4.4]]).dtype

Full list of Numpy datatypes can be found here: [numpy.org/devdocs/user/basics.types.html](https://numpy.org/devdocs/user/basics.types.html). **Astype()** is an important function in Numpy. It can be used to change the **dtype** of an array. It also works with Pandas dataframes.

sample2_np.astype('float64') # Notice the dots.

Note that if you transform floats to integers, they will be truncated.

np.array([[3.4,2.3],[4.5,2.1]]).astype('int64')

*Vectorisation* is an important concept in Numpy. It means that you can do operations to whole arrays without using for loops. This is essential in many kinds of machine learning operations.

sample3_np = np.random.normal(size = (2,3))

sample3_np

sample3_np*sample3_np

Because of the vectorisation, for example simulations are very easy to do in Numpy.

coin_np = np.random.randint(0,2,200)

coin_np = np.where(coin_np>0,1,-1)

import matplotlib.pyplot as plt

plt.style.use('bmh')

plt.plot(coin_np.cumsum())

Dividing with an array works also element-wise.

1 / np.array([[1,2],[3,4]])

Broadcasting means that the operation of a smaller array is repeated through the larger array.

sample4_np = np.array([[1,2,3],[4,5,6]])

sample4_np + [1,1,1]

Comparison between arrays return a boolen ndarray.

rand1_np = np.random.normal(size=(2,2))
rand2_np = np.random.normal(size=(2,2))

rand1_np > rand2_np

Slicing works efficiently with Numpy.

rand3_np = np.random.randint(1,10,(3,4))

rand3_np

rand3_np[1]

Note that the starting value of a slice is not included. Overall, slicing with multi-dimensional arrays is something that needs practice. Experiment with different multidimensional arrays to learn the details of slicing.

rand3_np[1,2:4]

Broadcasting is also applied, when assigning values to slices

rand3_np[1] = 1

rand3_np

You can use booleans to pick values that satisfy a certain criteria. Notice that the result here is transformend as a one-dimensional array.

rand3_np[rand3_np < 5]

You can use also lists to select specific rows/columns.

rand3_np[[0,2]]

rand3_np[:,[0,2]]

You can easily reshape an array with **reshape**.

reshaped_np = np.arange(12).reshape((3,4))

reshaped_np

There is a special attribute **T** that can be used to transpose an array.

reshaped_np.T

Numpy has many other matrix operation functions also. The full list can be found here: [numpy.org/doc/stable/reference/routines.linalg.html](https://numpy.org/doc/stable/reference/routines.linalg.html)

np.dot(reshaped_np,reshaped_np.T) # Inner product

You can also calculate the product like this:

reshaped_np.dot(reshaped_np.T)

np.cross(reshaped_np[:2,:2],reshaped_np[:2,:2].T)

Numpy uses for matrix operations low-level libraries, so they are as efficient as the same operations, for example, in Matlab. 

square_np = np.random.normal(0,1,(3,3))

square_np

np.linalg.inv(square_np)

Here is an example of calculation errors that happens now and then with computers. The off-diagonal values should be exactly zero.

np.dot(square_np,np.linalg.inv(square_np))

Singular value decomposition.

np.linalg.svd(square_np)

There is also **swapaxes** that can be used to swap. So, for two-dimensinal arrays it is the same as transpose. But for higher-dimensional arrays, it gives more possibilities.

more_np = np.random.randint(1,10,size=(2,2,3))

more_np

more_np.swapaxes(0,2)

Numpy has many convenient functions for element-wise operations, called universal functions. Let's look some of them. The full list can be found here: [numpy.org/doc/stable/reference/ufuncs.html](https://numpy.org/doc/stable/reference/ufuncs.html)

np.sqrt(more_np)

np.exp(more_np)

array1_np = np.random.randint(1,10,size=(2,3))
array2_np = np.random.randint(1,10,size=(2,3))

array1_np

array2_np

np.maximum(array1_np,array2_np)

Numpy **where** is a very important function if you want to do conditional operations with arrays.

a_array_np = np.random.randint(1,10,(2,3))
b_array_np = np.random.randint(1,10,(2,3))
bool_np = np.array([[True,False,True],[False,True,False]])

a_array_np

b_array_np

np.where(bool_np,a_array_np,b_array_np)

There is also a sorting function in Numpy. At default, it sorts values along the last axis:

np.sort(b_array_np)

np.sort(b_array_np,axis=0)

np.sort(b_array_np,axis=None)

**Unique** is a Numpy version of **set**.

large_np = np.random.randint(1,10,size=(6,6))

large_np

np.unique(large_np)

#### Scikit-learn

Scikit-learn is a multi-purpose machine learning library. It has modules for many different machine learning approaches. It is not the best library in any machine learning field but very good at most of them. Also, all the approaches use the common workflow approach of the library. Thus, by learning to do one machine learning analysis, you learn to do them all.

Scikit-learn has libraries for classification, regression, clustering, dimensionality reduction and model selection. It also has an extensive library of methods for data pre-processing.

A very convenient feature in Scikit-learn is **pipeline** that you can use to construct full workflows of machine learning analyses.

There should be no difficulties to install Scikit-learn. With Python/Pip you just execute **pip install scikit-learn** and with Anaconda you just install it from the menu (or use **conda install scikit-learn** in the command line). (Actually, you should not need to do that as Scikit-learn is installed in Anaconda by default.)

Again, the best way to learn Scikit-learn is by going through examples. Thus, more  details are in the following examples.

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

plt.style.use('bmh')

Example data from [www.kaggle.com/c/companies-bankruptcy-forecast](https://www.kaggle.com/c/companies-bankruptcy-forecast)

table_df = pd.read_csv('ml_data.csv')[['Attr1','Attr8','Attr21','Attr4',
                                       'Attr5','Attr29','Attr20',
                                       'Attr15','Attr6','Attr44']]

The above link has the explanation for all the variables. The original data has 65 variables, but we are here using a subsample of 10 variables. With **rename()** we can rename the variables to be more informative.

table_df.rename({'Attr1' : 'ROA','Attr8' : 'Leverage','Attr21' : 'Sales-growth',
                 'Attr4' : 'Current ratio','Attr5' : 'Quick ratio','Attr29' : 'Log(Total assets)',
                 'Attr20' : 'Inventory*365/sales','Attr15' : 'Total_liab*365/(gross_prof+depr)',
                 'Attr6' : 'Ret_earnings/TA','Attr44' : 'Receiv*365/sales'},axis=1,inplace=True)

table_df

With the **clip** method, you can winsorise the data. Here extreme values are moved to 1 % and 99 % quantiles.

table_df = table_df.clip(lower=table_df.quantile(0.01),upper=table_df.quantile(0.99),axis=1)

With **hist()** you can check the distributions quickly. The most problematic outliers have been removed by winsorisation.

table_df.hist(figsize=(14,14))
plt.show()

With **corr()** you can check the correlations. There is multicollinearity present, but for example with ensemble methods, multicollinearity is much less of a problem.

table_df.corr()

The predictors are everything else but ROA, which is our predicted variable.

X = table_df.drop(['ROA'],axis=1)

y = table_df['ROA']

from sklearn.model_selection import train_test_split

Let's make things difficult for OLS (very small train set). Here we use only 1 % of the data for training to demonstrate the strengths of ridge and lasso regression, which are usually usefuly only when n is close to p.

# Split data into training and test sets
X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.99, random_state=1)

len(X_train)

### Regression

#### Linear model

Although Scikit-learn is a ML library, it is possible to do a basic linear regression analysis with it. (All ML methods are statistical methods. The separation between them is artificial.)
![Linear_regression](./images/Linear_regression.svg)

import sklearn.linear_model as sk_lm

We define our LinearRegression object.

model = sk_lm.LinearRegression()

**fit()** can be used to fit the data.

model.fit(X_train,y_train)

**coef_** -attribute has the coefficients of each variable and **intercept_** has the intercept of the linear regression model.

model.coef_

model.intercept_

**score()** can be used to measure the coefficient of determination of the trained model. How much our variables are explaining of the variation of the predicted variable.*

model.score(X_test,y_test)

A short code to draw scatter charts between every feature and ROA. The blue dots are the correct values and the red dots are the predictions of the model.

fig, axs = plt.subplots(3,3,figsize=(15,12))
for ax,feature,coef in zip(axs.flat,X_test.columns,model.coef_):
    ax.scatter(X_test[feature],y_test,alpha=0.4)
    ax.plot(X_test[feature],model.predict(X_test),'r.',alpha=0.2)
    ax.set_title(feature)

Mean squared error can be used to the measure the performance. Less is better.

from sklearn.metrics import mean_squared_error

mean_squared_error(y_test,model.predict(X_test))

#### Ridge regression

Ridge regression counters overfitting by adding a penalty on the size if the coefficients of the standard linear regression model. So it is a regularisation method.

![Regularisation](./images/Regularization.svg)

We can optimise the alpha parameter of the error function automatically using **RidgeCV**.

![Ridge_alpha](./images/ridge_alpha.png)

alpha_set = np.logspace(-5,5,20)

ridgecv = sk_lm.RidgeCV(alphas = alpha_set,cv=10, scoring = 'neg_mean_squared_error', normalize = True)

Otherwise similar steps. Define the object, use the **fit()** function, analyse the results with **coef_**, **intercept_**, **score()** and **mean_squared_error()**.

ridgecv.fit(X_train,y_train)

As you can see, the coefficients have decreases. But only a little.

ridgecv.coef_

ridgecv.intercept_

ridgecv.alpha_

The coefficient of determination is now much improved (linear regression ~0.15).

ridgecv.score(X_test,y_test)

Ridge regression decreases  the variation of predictions.

fig, axs = plt.subplots(3,3,figsize=(15,12))
for ax,feature,coef in zip(axs.flat,X_test.columns,model.coef_):
    ax.scatter(X_test[feature],y_test,alpha=0.3)
    ax.plot(X_test[feature],ridgecv.predict(X_test),'r.',alpha=0.3)
    ax.set_title(feature)

MSE has also improved.

mean_squared_error(y_test,ridgecv.predict(X_test))

#### The Lasso

Let's try next the lasso. It uses stronger regularisation (the absolute values of parameters in the regularisation term)

alpha_set = np.logspace(-5,5,21)

lassocv = sk_lm.LassoCV(alphas = None,cv=5,max_iter=100000, normalize = True)

lassocv.fit(X_train,y_train)

Lasso is different in that it decreases the coefficients of variables more easily to zero.

lassocv.coef_

lassocv.intercept_

lassocv.alpha_

Now the coefficient of determination is even better.

lassocv.score(X_test,y_test)

As you can see from the figure below. Regularisation decreases the sizes of parameters and this decreases the variation of predictions.

fig, axs = plt.subplots(3,3,figsize=(15,12))
for ax,feature,coef in zip(axs.flat,X_test.columns,model.coef_):
    ax.scatter(X_test[feature],y_test,alpha=0.3)
    ax.plot(X_test[feature],lassocv.predict(X_test),'r.',alpha=0.3)
    ax.set_title(feature)

MSE has also improved.

mean_squared_error(y_test,lassocv.predict(X_test))

By using larger alpha value, we can force more variables to zero.

lasso_model = sk_lm.Lasso(alpha = 0.003,max_iter=100000, normalize = True)

lasso_model.fit(X_train,y_train)

Now only two coefficients in our model are different from zero (Current ratio and Retained earnings / Total assets)

lasso_model.coef_

lasso_model.intercept_

The score decreases a little because we are forcing alpha to be *too* large.

lasso_model.score(X_test,y_test)

Now the variation is even smaller.

fig, axs = plt.subplots(3,3,figsize=(15,12))
for ax,feature,coef in zip(axs.flat,X_test.columns,model.coef_):
    ax.scatter(X_test[feature],y_test,alpha=0.3)
    ax.plot(X_test[feature],lasso_model.predict(X_test),'r.',alpha=0.3)
    ax.set_title(feature)

mean_squared_error(y_test,lasso_model.predict(X_test))

#### Linear reference model

In the following, we use a more reasonable division between training and testing datasets. With so large data, there is no need for Ridge or Lasso regularisation and we use a basic linear model as a reference. 80% / 20% -split is commonly used.

# Split data into training and test sets
X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

The same steps as before.

ref_model = sk_lm.LinearRegression()

ref_model.fit(X_train,y_train)

With 8000 observations (instead of 100) we get a much better model.

ref_model.score(X_test,y_test)

mean_squared_error(y_test,ref_model.predict(X_test))

#### Random forest

Random forest has proven to be a very powerful prediction model.

from sklearn.ensemble import RandomForestRegressor

The strength of Scikit-learn is that the steps for building a model are similar for every model. Define an object, fit it to data, analyse the results.

r_forest_model = RandomForestRegressor(random_state=0)

r_forest_model.fit(X_train,y_train)

With the RF model, there is a much better fit between the predicted values and the correct test values.

fig, axs = plt.subplots(3,3,figsize=(15,12))
for ax,feature,coef in zip(axs.flat,X_test.columns,model.coef_):
    ax.scatter(X_test[feature],y_test,alpha=0.6)
    ax.plot(X_test[feature],r_forest_model.predict(X_test),'r.',alpha=0.3)
    ax.set_title(feature)

Scikit-Learn has a **feature_importances_** attribute to explain the importance of different parameters in explaining the predictions.

pd.DataFrame([X_train.columns,r_forest_model.feature_importances_]).transpose().sort_values(1,ascending=False)

The coefficient of determination and MSE are significantly better with the RF model.

r_forest_model.score(X_test,y_test)

mean_squared_error(y_test,r_forest_model.predict(X_test))

### Gradient boosting

Random forest and gradient boosting are often the best ensemble models in applications. The gradient boosting model is defined using the same steps.

from sklearn.ensemble import GradientBoostingRegressor

gradient_model = GradientBoostingRegressor(random_state=0)

gradient_model.fit(X_train,y_train)

fig, axs = plt.subplots(3,3,figsize=(15,12))
for ax,feature,coef in zip(axs.flat,X_test.columns,model.coef_):
    ax.scatter(X_test[feature],y_test,alpha=0.6)
    ax.plot(X_test[feature],gradient_model.predict(X_test),'r.',alpha=0.3)
    ax.set_title(feature)

pd.DataFrame([X_train.columns,gradient_model.feature_importances_]).transpose().sort_values(1,ascending=False)

This time, the random forest model wins the competition.

gradient_model.score(X_test,y_test)

mean_squared_error(y_test,gradient_model.predict(X_test))

### Classification

Let's look next ML models can be used for classification.

The following statement constructs an indicator variable to the dataframe, that has a value 1, if the ROA of that company is larger than zero (and zero otherwise). In the following models we try to classify the companies to these two categories.

table_df['ROA_ind'] = table_df['ROA']>0

X_class = table_df.drop(['ROA','ROA_ind'],axis=1)

y_class = table_df['ROA_ind']

Again, a 80%/20% split for training and testing.

# Split data into training and test sets
X_train, X_test , y_train, y_test = train_test_split(X_class, y_class, test_size=0.2, random_state=1)

#### Linear discriminant analysis

Linear discriminant analysis is the simplest approach. It tries to separate classes using a linear combination of features.

![LinDis](./images/LinDis.png)

The models are build like in the regression examples.

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

LDA_class = LinearDiscriminantAnalysis()

LDA_class.fit(X_train,y_train)

Now the score measueres the correct prediction rate. 73.95 % of the companies in the test data are catgorised correctly.

LDA_class.score(X_test,y_test)

A ROC curve is a popular way to analyse classification performance of a model. The more the line bends to the upper-left corner, the better.

from sklearn.metrics import plot_roc_curve
from sklearn.metrics import plot_precision_recall_curve

plot_roc_curve(LDA_class,X_test,y_test)
plt.show()

A precision-recall curve is an alternative and especially suitable for imbalanced data (the sizes of the categories are not equal). The more the line bends to the upper-*right* corner, the better.

plot_precision_recall_curve(LDA_class,X_test,y_test)
plt.show()

#### Support vector machines

Usually SVMs are powerfull classifiers, but for this data, they do not perform as well as the LDA model.

from sklearn import svm

Kernel can be thought as the type of the separation zone between the categories. **Linear**  is the simplest.

svm_classifier = svm.SVC(kernel='linear')

svm_classifier.fit(X_train,y_train)

svm_classifier.score(X_test,y_test)

from sklearn.metrics import plot_roc_curve
from sklearn.metrics import plot_precision_recall_curve

plot_roc_curve(svm_classifier,X_test,y_test)
plt.show()

plot_precision_recall_curve(svm_classifier,X_test,y_test)

Radial basis function is more powerful kernel for SVMs.

svm_classifier = svm.SVC(kernel='rbf')

svm_classifier.fit(X_train,y_train)

The results improve slightly but are still not as good as with the LDA model.

svm_classifier.score(X_test,y_test)

plot_roc_curve(svm_classifier,X_test,y_test)
plt.show()

plot_precision_recall_curve(svm_classifier,X_test,y_test)

#### Decision trees

Decision tree classifies companies by using features to catogorise companies to different branches.

from sklearn import tree

tree_class = tree.DecisionTreeClassifier(max_depth=3)

tree_class.fit(X_train, y_train)

from sklearn import tree

plt.figure(figsize=(15,15))
tree.plot_tree(tree_class,fontsize=12,filled=True)
plt.show()

The decision tree model has the best performance so far.

tree_class.score(X_test,y_test)

plot_roc_curve(tree_class,X_test,y_test)

plot_precision_recall_curve(tree_class,X_test,y_test)

#### Random forest revisited

If the decision tree model was performin so well, we could anticipate that many decision trees (= random forest) would perform even better.

from sklearn.ensemble import RandomForestClassifier

rf_class = RandomForestClassifier()

rf_class.fit(X_train,y_train)

pd.DataFrame([X_train.columns,rf_class.feature_importances_]).transpose().sort_values(1,ascending=False)

The performance is clearly the best one so far when measured usin all three metrics.

rf_class.score(X_test,y_test)

plot_roc_curve(rf_class,X_test,y_test)
plt.show()

plot_precision_recall_curve(rf_class,X_test,y_test)
plt.show()