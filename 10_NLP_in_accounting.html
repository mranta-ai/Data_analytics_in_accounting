
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>4. Introduction to natural language processing &#8212; Data analytics in accounting</title>
    
  <link rel="stylesheet" href="_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="_static/sphinx-book-theme.2d2078699c18a0efb88233928e1cf6ed.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.be0a4a0c39cd630af62a2fcf693f3f06.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="3. Deep learning in accounting" href="9_Deep_learning_in_accounting.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Data analytics in accounting</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="book_intro.html">
   Introduction to data analytics in accounting
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Basic data analytics in accounting
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="0_installing_Python.html">
   1. Installing Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="1_Intro_to_Python.html">
   2. Introduction to Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2_Pandas_basics.html">
   3. Pandas data basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3_Pandas_data_preprocessing.html">
   4. Pandas Data preprocessing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4_Connecting_with_accounting_databases.html">
   5. Connecting with accounting databases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5_Visualisation_with_Python.html">
   6. Visualisations with Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="6_Basic_statistics_with_Python.html">
   7. Basic statistics with Python
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Machine learning in accounting
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="7_Introduction_to_ML_in_accounting.html">
   1. Introduction Machine learning in accounting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="8_Decision_making_in_accounting.html">
   2. Decision making with ML in accounting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="9_Deep_learning_in_accounting.html">
   3. Deep learning in accounting
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   4. Introduction to natural language processing
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/10_NLP_in_accounting.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/10_NLP_in_accounting.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#topic-models">
   4.1. Topic models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#neural-network-models">
   4.2. Neural network models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pretrained-language-models">
   4.3. Pretrained language models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bert">
     4.3.1. BERT
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gpt-3">
     4.3.2. GPT-3
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#nlp-example-lda-and-other-summarisation-tools">
   4.4. NLP example - LDA and other summarisation tools
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#lda">
   4.5. LDA
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summarisation">
   4.6. Summarisation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#similarities">
   4.7. Similarities
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cluster-model">
   4.8. Cluster model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#nlp-example-imdb">
   4.9. NLP example - IMDB
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#densely-connected-network">
   4.10. Densely connected network
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#recurrent-neural-networks">
   4.11. Recurrent neural networks
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#long-short-term-memory">
   4.12. Long short-term memory
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="introduction-to-natural-language-processing">
<h1><span class="section-number">4. </span>Introduction to natural language processing<a class="headerlink" href="#introduction-to-natural-language-processing" title="Permalink to this headline">¶</a></h1>
<p>Natural language processing (NLP) is a collective term referring to computational processing of human languages. It includes methods that analyse human-produced text, and methods that create natural language as output. Compared to many other machine learning tasks, natural language processing is very challenging, as human language is inherently ambiguous, ever-changing, and not well-defined.</p>
<p><img alt="read_robot" src="_images/read_robot.jpg" /></p>
<p>There is a need for better and better NLP-algorithms, as information in the textual format is increasing exponentially.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/10_NLP_in_accounting_1_0.png" src="_images/10_NLP_in_accounting_1_0.png" />
</div>
</div>
<p>Until 2014, core NLP techniques were dominated by linear modelling approaches that use supervised learning. Key algorithms were simple neural networks, support vector machines and logistic regression, trained over high dimensional and sparse feature vectors (bag-of-words -vectors).</p>
<p><img alt="SVM" src="_images/svm.png" /></p>
<p>Around 2014, the field has started to see some success in switching from linear models over sparse inputs to nonlinear complex neural network models over dense inputs. A key difference is how words are presented as relatively low-dimensional vectors that contain semantic information about the word. Two key training algorithms are <strong>continuous-bag-of-words</strong> and <strong>skip-gram</strong> -algorithms.</p>
<p>The CBOW model architecture tries to predict the current target word (the centre word) based on the source context words (surrounding words).</p>
<p>The Skip-gram model architecture usually tries to achieve the reverse of what the CBOW model does. It tries to predict the source context words (surrounding words) given a target word (the centre word).</p>
<p><img alt="word2vec" src="_images/word2vec.png" /></p>
<p>Some of the neural-network techniques are generalisations of the linear models and can be just replaced in place of the linear classifiers. Others have a totally new approach for a natural language processing task and provide new modelling opportunities. In particular, a family of approaches based on recurrent neural networks (RNNs) removes the reliance on the Markov assumption that was prevalent in sequence models, allowing to condition on arbitrarily long sequences and produce effective feature extractors. This enables the models to analyse whole sentences (and even more) instead of words, which has led to breakthroughs in language modelling, automatic machine translation, and various other applications.</p>
<p>Also, recent transformers-based models have achieved revolutionary results. The success of the architecture is based on a concept called attention that improves the learning by focusing on the key features and ignoring features that do not help in the task at hand. This conceptually simple innovation is largely behind the success of pre-trained models like BERT and GPT-3. The transformer is an architecture for transforming one sequence into another one with the help of two parts (Encoder and Decoder), but it differs from the previously described/existing sequence-to-sequence models because it does not imply any recurrent architectures.</p>
<p>(The Markov assumption means that The Markov property holds. A stochastic process has the Markov property if the conditional probability distribution of future states of the process (conditional on both past and present states) depends only upon the present state, not on the sequence of events that preceded it.)</p>
<div class="section" id="topic-models">
<h2><span class="section-number">4.1. </span>Topic models<a class="headerlink" href="#topic-models" title="Permalink to this headline">¶</a></h2>
<p>A topic model is a type of statistical model for inferring the “topics” or “themes” that occur in a collection of documents. Topic modelling is a popular tool for the discovery of hidden semantic structures in a text body. Topic models assume that there are typical words that appear more frequently in a document with a certain topic. Moreover, some words are especially rare for a certain topic and for some words, there is no difference between a document with the topic and other documents. The “topics” produced by topic modelling techniques are clusters of similar words. For example, a very popular topic model called Latent Dirichlet Allocation assumes that documents are distributions of topics and topics are distributions of words.</p>
<p><img alt="topic_model" src="_images/topic_model.gif" /></p>
</div>
<div class="section" id="neural-network-models">
<h2><span class="section-number">4.2. </span>Neural network models<a class="headerlink" href="#neural-network-models" title="Permalink to this headline">¶</a></h2>
<p>Neural language models almost always use continuous representations or embeddings of words to make their predictions. These embeddings are usually implemented as layers in a neural language model. The embeddings help to alleviate the curse of dimensionality in language modelling: larger corpus –&gt; larger vocabulary –&gt; exponentially larger number of possible sequences of words.</p>
<p>Neural language models represent words in a distributed way, as a combination of weights in a neural network. Typical neural network architectures are feed-forward, recurrent, LSTM and transformers architectures.</p>
</div>
<div class="section" id="pretrained-language-models">
<h2><span class="section-number">4.3. </span>Pretrained language models<a class="headerlink" href="#pretrained-language-models" title="Permalink to this headline">¶</a></h2>
<p><img alt="elmo" src="_images/elmo.jpg" /></p>
<div class="section" id="bert">
<h3><span class="section-number">4.3.1. </span>BERT<a class="headerlink" href="#bert" title="Permalink to this headline">¶</a></h3>
<p>Bidirectional Encoder Representations from Transformers (BERT) is a pre-trained NLP model developed by Google.</p>
<p>The original English-language BERT model comes with two pre-trained general types: (1) the BERTBASE which uses the BooksCorpus with 800M words, and (2) the BERTLARGE that uses the English Wikipedia with 2,500M words.</p>
<p>At the time introduction, BERT achieved state-of-the-art in many NLP tasks, like language understanding and question answering. BERT started the revolution of modern language models.</p>
<p>(In the picture above is Elmo, not Bert. However, there is also a language model called Elmo:<a class="reference external" href="https://allennlp.org/elmo">allennlp.org</a></p>
</div>
<div class="section" id="gpt-3">
<h3><span class="section-number">4.3.2. </span>GPT-3<a class="headerlink" href="#gpt-3" title="Permalink to this headline">¶</a></h3>
<p>GPT-3 is the current state-of-the-art language model that has achieved revolutionary results. It is also the largest ML model to date, with 175 billion parameters. It was trained with data that has 499 billion tokens (words). For example, GPT-3 can create news articles that are difficult to distinguish from human-created news. It is also able to have conversations with a human. However, despite its’ stellar performance in creating meaningful text, it still does not understand anything that it is saying.
Below is an example article generated by GPT-3.</p>
<p><img alt="gpt3_text" src="_images/gpt3_desc_text.jpg" /></p>
</div>
</div>
<div class="section" id="nlp-example-lda-and-other-summarisation-tools">
<h2><span class="section-number">4.4. </span>NLP example - LDA and other summarisation tools<a class="headerlink" href="#nlp-example-lda-and-other-summarisation-tools" title="Permalink to this headline">¶</a></h2>
<p>In this example, we analyse a collection of academic journals with Latent Dirichlet Allocation (LDA) and other summarisation tools.</p>
<p>To manipulate and list files in directories, we need the <strong>os</strong> -library.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
</pre></div>
</div>
</div>
</div>
<p>I have the data in the acc_journals -folder that is under the work folder. Unfortunately, this data is not available anywhere. If you want to follow this example, just put a collection of txt-files to a “acc_journals”-folder (under the work folder) and follow the steps.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data_path</span> <span class="o">=</span> <span class="s1">&#39;./acc_journals/&#39;</span>
</pre></div>
</div>
</div>
</div>
<p><strong>listdir()</strong> makes a list of filenames inside <strong>data_path</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">files</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The name of the first text file is “2019_1167.txt”.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">files</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;2019_1167.txt&#39;
</pre></div>
</div>
</div>
</div>
<p>In total, we have 2126 articles.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">files</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2126
</pre></div>
</div>
</div>
</div>
<p>The filenames have a publication year as the first four digits of the name. With the following code we can collect the publication years to a list.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">file_years</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">files</span><span class="p">:</span>
    <span class="n">file_years</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">file</span><span class="p">[:</span><span class="mi">4</span><span class="p">]))</span> <span class="c1"># Pick the first four letters from the filename -string and turn it into a integer.</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">file_years</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">file_years</span><span class="p">)]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[260, 296, 297, 134, 128, 129, 174, 234, 226, 248]
</pre></div>
</div>
</div>
</div>
<p>In this example we will need <strong>numpy</strong> to manipulate arrays, and <strong>Matplotlib</strong> for plots.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s plot the number of documents per year.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;fivethirtyeight&#39;</span><span class="p">)</span> <span class="c1"># Define the style of figures.</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">6</span><span class="p">])</span> <span class="c1"># Define the size of figures.</span>
<span class="c1"># The first argument: years, the second argument: the list of document frequencies for different years (built using list comprehension)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">file_years</span><span class="p">)),[</span><span class="n">file_years</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">file_years</span><span class="p">)])</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">file_years</span><span class="p">)))</span> <span class="c1"># The years below the bars</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/10_NLP_in_accounting_19_0.png" src="_images/10_NLP_in_accounting_19_0.png" />
</div>
</div>
<p>The common practice is to remove <strong>stopwords</strong> from documents. These are common fill words that do not contain information about the content of documents. We use the stopwords-list from the NLTK library (www.nltk.org). Here is a short description of NLTK from their web page: “NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.”</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stop_words</span> <span class="o">=</span> <span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s2">&quot;english&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here are some example stopwords.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stop_words</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;i&#39;, &#39;me&#39;, &#39;my&#39;, &#39;myself&#39;, &#39;we&#39;, &#39;our&#39;, &#39;ours&#39;, &#39;ourselves&#39;, &#39;you&#39;, &quot;you&#39;re&quot;]
</pre></div>
</div>
</div>
</div>
<p>Usually, stopword-lists are extended with useless words specific to the corpus we are analysing. That is done below. These additional words are found by analysing the results at different steps of the analysis. Then the analysis is repeated.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stop_words</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="s1">&#39;fulltext&#39;</span><span class="p">,</span><span class="s1">&#39;document&#39;</span><span class="p">,</span><span class="s1">&#39;downloaded&#39;</span><span class="p">,</span><span class="s1">&#39;download&#39;</span><span class="p">,</span><span class="s1">&#39;emerald&#39;</span><span class="p">,</span><span class="s1">&#39;emeraldinsight&#39;</span><span class="p">,</span>
                   <span class="s1">&#39;accepted&#39;</span><span class="p">,</span><span class="s1">&#39;com&#39;</span><span class="p">,</span><span class="s1">&#39;received&#39;</span><span class="p">,</span><span class="s1">&#39;revised&#39;</span><span class="p">,</span><span class="s1">&#39;archive&#39;</span><span class="p">,</span><span class="s1">&#39;journal&#39;</span><span class="p">,</span><span class="s1">&#39;available&#39;</span><span class="p">,</span><span class="s1">&#39;current&#39;</span><span class="p">,</span>
                   <span class="s1">&#39;issue&#39;</span><span class="p">,</span><span class="s1">&#39;full&#39;</span><span class="p">,</span><span class="s1">&#39;text&#39;</span><span class="p">,</span><span class="s1">&#39;https&#39;</span><span class="p">,</span><span class="s1">&#39;doi&#39;</span><span class="p">,</span><span class="s1">&#39;org&#39;</span><span class="p">,</span><span class="s1">&#39;www&#39;</span><span class="p">,</span><span class="s1">&#39;com&#39;&#39;ieee&#39;</span><span class="p">,</span><span class="s1">&#39;cid&#39;</span><span class="p">,</span><span class="s1">&#39;et&#39;</span><span class="p">,</span><span class="s1">&#39;al&#39;</span><span class="p">,</span><span class="s1">&#39;pp&#39;</span><span class="p">,</span>
                   <span class="s1">&#39;vol&#39;</span><span class="p">,</span><span class="s1">&#39;fig&#39;</span><span class="p">,</span><span class="s1">&#39;reproduction&#39;</span><span class="p">,</span><span class="s1">&#39;prohibited&#39;</span><span class="p">,</span><span class="s1">&#39;reproduced&#39;</span><span class="p">,</span><span class="s1">&#39;permission&#39;</span><span class="p">,</span><span class="s1">&#39;accounting&#39;</span><span class="p">,</span><span class="s1">&#39;figure&#39;</span><span class="p">,</span><span class="s1">&#39;chapter&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>The following code reads every file in the <strong>files</strong> list and reads it content as an item to the <strong>raw_text</strong> list. Thus, we have a list with 2126 items and where each item is a raw text of one document.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">raw_text</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">files</span><span class="p">:</span>
    <span class="n">fd</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span><span class="n">file</span><span class="p">),</span><span class="s1">&#39;r&#39;</span><span class="p">,</span><span class="n">errors</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
    <span class="n">raw_text</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fd</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<p>Here is an example of raw text from the first document.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">raw_text</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">3200</span><span class="p">:</span><span class="mi">3500</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;currency exchanges and markets have daily dollar volume of around $50 billion.1 Over 300 &quot;cryptofunds&quot; have emerged (hedge funds that invest solely in cryptocurrencies), attracting around $10 billion in assets under management (Rooney and Levy 2018).2 Recently, bitcoin futures have commenced trading&#39;
</pre></div>
</div>
</div>
</div>
<p>For the following steps, we need Gensim that is a multipurpose NLP library specially designed for topic modelling.</p>
<p>Here is a short description from the Gensim Github-page (<a class="reference external" href="https://github.com/RaRe-Technologies/gensim">github.com/RaRe-Technologies/gensim</a>):</p>
<p>“Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. Target audience is the natural language processing (NLP) and information retrieval (IR) community.
Features:</p>
<ul class="simple">
<li><p>All algorithms are memory-independent w.r.t. the corpus size (can process input larger than RAM, streamed, out-of-core),</p></li>
<li><p>Intuitive interfaces</p>
<ul>
<li><p>Easy to plug in your own input corpus/datastream (trivial streaming API)</p></li>
<li><p>Easy to extend with other Vector Space algorithms (trivial transformation API)</p></li>
</ul>
</li>
<li><p>Efficient multicore implementations of popular algorithms, such as online Latent Semantic Analysis (LSA/LSI/SVD), Latent Dirichlet Allocation (LDA), Random Projections (RP), Hierarchical Dirichlet Process (HDP) or word2vec deep learning.</p></li>
<li><p>Distributed computing: can run Latent Semantic Analysis and Latent Dirichlet Allocation on a cluster of computers.”</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gensim</span>
</pre></div>
</div>
</div>
</div>
<p>Gensim has a convenient <strong>simple_preprocess()</strong> -function that makes many text cleaning procedures automatically. It converts a document into a list of lowercase tokens, ignoring tokens that are too short (less than two characters) or too long (more than 15 characters). The following code goes through all the raw texts and applies <strong>simple_preprocess()</strong> to them. So, docs_cleaned is a list with lists of tokens as items.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs_cleaned</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">raw_text</span><span class="p">:</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">simple_preprocess</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
    <span class="n">docs_cleaned</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here is an example from the first document after cleaning. The documents are now lists of tokens, and here the list is joined back as a string of text.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">docs_cleaned</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">300</span><span class="p">:</span><span class="mi">400</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;for assistance relating to data acknowledges financial support from the capital markets co operative research centre acknowledges financial support from the australian research council arc de supplementary data can be found on the review of financial studies web site send correspondence to talis putnins uts business school university of technology sydney po box broadway nsw australia telephone mail talis putnins uts edu au the author published by oxford university press on behalf of the society for financial studies all rights reserved for permissions please mail journals permissions oup com doi rfs hhz downloaded from https academic oup com rfs article&#39;
</pre></div>
</div>
</div>
</div>
<p>Next, we remove the stopwords from the documents.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs_nostops</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">docs_cleaned</span><span class="p">:</span>
    <span class="n">red_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">item</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">]</span>
    <span class="n">docs_nostops</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">red_tokens</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>An example text from the first document after removing the stopwords.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">docs_nostops</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">300</span><span class="p">:</span><span class="mi">400</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;hedge funds invest solely attracting around billion assets management rooney levy recently bitcoin futures commenced trading cme cboe catering institutional demand trading hedging bitcoin fringe asset quickly maturing rapid growth anonymity provide users created considerable regulatory challenges application million cryptocurrency exchange traded fund etf rejected securities exchange commission sec march several rejected amid concerns including lack regulation chinese government banned residents trading made initial coin offerings icos illegal september central bank heads bank england mark carney publicly expressed concerns many potential benefits including faster efficient settlement payments regulatory concerns center around use illegal trade drugs hacks thefts illegal pornography even&#39;
</pre></div>
</div>
</div>
</div>
<p>As a next step, we remove everything else but nouns, adjectives, verbs and adverbs recognised by our language model. As our language model, we use the large English model from Spacy (<a class="reference external" href="https://spacy.io/">spacy.io</a>). Here are some key features of Spacy from their web page:</p>
<ul class="simple">
<li><p>Non-destructive tokenisation</p></li>
<li><p>Named entity recognition</p></li>
<li><p>Support for 59+ languages</p></li>
<li><p>46 statistical models for 16 languages</p></li>
<li><p>Pretrained word vectors</p></li>
<li><p>State-of-the-art speed</p></li>
<li><p>Easy deep learning integration</p></li>
<li><p>Part-of-speech tagging</p></li>
<li><p>Labelled dependency parsing</p></li>
<li><p>Syntax-driven sentence segmentation</p></li>
<li><p>Built-in visualisers for syntax and NER</p></li>
<li><p>Convenient string-to-hash mapping</p></li>
<li><p>Export to NumPy data arrays</p></li>
<li><p>Efficient binary serialisation</p></li>
<li><p>Easy model packaging and deployment</p></li>
<li><p>Robust, rigorously evaluated accuracy</p></li>
</ul>
<p>First, we load the library.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>
</pre></div>
</div>
</div>
</div>
<p>Then we define that only certain part-of-speed (POS) -tags are allowed.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">allowed_postags</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;NOUN&#39;</span><span class="p">,</span> <span class="s1">&#39;ADJ&#39;</span><span class="p">,</span> <span class="s1">&#39;VERB&#39;</span><span class="p">,</span> <span class="s1">&#39;ADV&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>The <strong>en_core_web_lg</strong> model is quite large (700MB) so it takes a while to download it. We do not need the dependency parser  or named-entity-recognition, so we disable them.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;en_core_web_lg&#39;</span><span class="p">,</span> <span class="n">disable</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;parser&#39;</span><span class="p">,</span> <span class="s1">&#39;ner&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>The following code goes through the documents and removes words that are not recognised by our language model as nouns, adjectives, verbs or adverbs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs_lemmas</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">red_tokens</span> <span class="ow">in</span> <span class="n">docs_nostops</span><span class="p">:</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">red_tokens</span><span class="p">))</span> <span class="c1"># We need to join the list of tokens back to a single string.</span>
    <span class="n">docs_lemmas</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">token</span><span class="o">.</span><span class="n">lemma_</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span> <span class="k">if</span> <span class="n">token</span><span class="o">.</span><span class="n">pos_</span> <span class="ow">in</span> <span class="n">allowed_postags</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Here is again an example text from the first document. Things are looking good. We have a clean collection of words that are meaningful when we try to figure out what is discussed in the text.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">docs_lemmas</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">300</span><span class="p">:</span><span class="mi">400</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;silk road marketplace combine public nature blockchain provide unique laboratory analyze illegal ecosystem evolve bitcoin network individual identity mask pseudo anonymity character alpha public nature blockchain allow link bitcoin transaction individual user market participant identify user release see next autonomous future commence trade contract bitcoin price approximately bitcoin time future launch contract notional value academic library user review financial study seize authority bitcoin seizure combine source provide sample user know involved illegal activity starting point analysis apply different empirical approach go sample estimate population illegal activity first approach exploit trade network user know involve illegal activity illegal user use bitcoin&#39;
</pre></div>
</div>
</div>
</div>
<p>Next, we build our bigram-model. Bigrams are two-word pairs that naturally belong together. Like the words New and York. We connect these words before we do the LDA analysis.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bigram</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Phrases</span><span class="p">(</span><span class="n">docs_lemmas</span><span class="p">,</span><span class="n">threshold</span> <span class="o">=</span> <span class="mi">80</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span> 
<span class="n">bigram_mod</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">phrases</span><span class="o">.</span><span class="n">Phraser</span><span class="p">(</span><span class="n">bigram</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs_bigrams</span> <span class="o">=</span> <span class="p">[</span><span class="n">bigram_mod</span><span class="p">[</span><span class="n">doc</span><span class="p">]</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs_lemmas</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>In this sample text, the model creates bigrams academic_library, starting_point and bitcoin_seizure. The formed bigrams are quite good. It is very often difficult to set the parameters of <strong>Phrases</strong> so that we have only reasonable bigrams.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">docs_bigrams</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">300</span><span class="p">:</span><span class="mi">400</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;evolve bitcoin network individual identity mask pseudo anonymity character alpha public nature blockchain allow link bitcoin transaction individual user market participant identify user release see next autonomous future commence trade contract bitcoin price approximately bitcoin time future launch contract notional value academic_library user review financial study seize authority bitcoin_seizure combine source provide sample user know involved illegal activity starting_point analysis apply different empirical approach go sample estimate population illegal activity first approach exploit trade network user know involve illegal activity illegal user use bitcoin_blockchain reconstruct complete network transaction market participant apply type network cluster analysis identify distinct community datum legal&#39;
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="lda">
<h2><span class="section-number">4.5. </span>LDA<a class="headerlink" href="#lda" title="Permalink to this headline">¶</a></h2>
<p><img alt="lda" src="_images/topic_model.gif" /></p>
<p>Okay, let’s start our LDA analysis. For this, we need the <strong>corpora</strong> module from Gensim.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gensim.corpora</span> <span class="k">as</span> <span class="nn">corpora</span>
</pre></div>
</div>
</div>
</div>
<p>First, with, <strong>Dictionary</strong>, we build our dictionary using the list docs_bigrams. Then we filter the most extreme cases from the dictionary:</p>
<ul class="simple">
<li><p>no_below = 2 : no words that are in less than two documents</p></li>
<li><p>no_above = 0.7 : no words that are in more than 70 % of the documents</p></li>
<li><p>keep_n = 50000 : keep the 50000 most frequent words</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">id2word</span> <span class="o">=</span> <span class="n">corpora</span><span class="o">.</span><span class="n">Dictionary</span><span class="p">(</span><span class="n">docs_bigrams</span><span class="p">)</span>
<span class="n">id2word</span><span class="o">.</span><span class="n">filter_extremes</span><span class="p">(</span><span class="n">no_below</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">no_above</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">keep_n</span><span class="o">=</span><span class="mi">50000</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Then, we build our corpus by indexing the words of documents using our <strong>id2word</strong> -dictionary</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="n">id2word</span><span class="o">.</span><span class="n">doc2bow</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">docs_bigrams</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p><strong>corpus</strong> contains a list of tuples for every document, where the first item of the tuples is a word-index and the second is the frequency of that word in the document.</p>
<p>For example, in the first document, a word with index 0 in our dictionary is once, a word with index 1 is four times, etc.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">corpus</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(0, 1),
 (1, 4),
 (2, 3),
 (3, 50),
 (4, 5),
 (5, 4),
 (6, 5),
 (7, 1),
 (8, 2),
 (9, 2)]
</pre></div>
</div>
</div>
</div>
<p>We can check what those words are just by indexing our dictionary <strong>id2word</strong>. As you can see, ‘accept’ is 50 times in the first document.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">id2word</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;abnormal&#39;,
 &#39;absolute&#39;,
 &#39;ac&#39;,
 &#39;academic_library&#39;,
 &#39;accept&#39;,
 &#39;access&#39;,
 &#39;accessible&#39;,
 &#39;accompany&#39;,
 &#39;accuracy&#39;,
 &#39;achieve&#39;]
</pre></div>
</div>
</div>
</div>
<p>This is the main step in our analysis; we build the LDA model. There are many parameters in Gensim’s <strong>LdaModel</strong>. Luckily the default parameters work quite well. You can read more about the parameters from <a class="reference external" href="https://radimrehurek.com/gensim/models/ldamodel.html">radimrehurek.com/gensim/models/ldamodel.html</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lda_model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">ldamodel</span><span class="o">.</span><span class="n">LdaModel</span><span class="p">(</span><span class="n">corpus</span><span class="o">=</span><span class="n">corpus</span><span class="p">,</span>
                                           <span class="n">id2word</span><span class="o">=</span><span class="n">id2word</span><span class="p">,</span>
                                           <span class="n">num_topics</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
                                           <span class="n">random_state</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                                           <span class="n">update_every</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                           <span class="n">chunksize</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">corpus</span><span class="p">),</span>
                                           <span class="n">passes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                                           <span class="n">alpha</span><span class="o">=</span><span class="s1">&#39;asymmetric&#39;</span><span class="p">,</span>
                                           <span class="n">per_word_topics</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                            <span class="n">eta</span> <span class="o">=</span> <span class="s1">&#39;auto&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>pyLDAvis</strong> is a useful library to visualise the results: <a class="reference external" href="https://github.com/bmabey/pyLDAvis">github.com/bmabey/pyLDAvis</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyLDAvis.gensim</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pyLDAvis</span><span class="o">.</span><span class="n">enable_notebook</span><span class="p">()</span>
<span class="n">vis</span> <span class="o">=</span> <span class="n">pyLDAvis</span><span class="o">.</span><span class="n">gensim</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">lda_model</span><span class="p">,</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">id2word</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/mikkoranta/python3/gensim/lib/python3.8/site-packages/joblib/numpy_pickle.py:103: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.
  pickler.file_handle.write(chunk.tostring(&#39;C&#39;))
/home/mikkoranta/python3/gensim/lib/python3.8/site-packages/joblib/numpy_pickle.py:103: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.
  pickler.file_handle.write(chunk.tostring(&#39;C&#39;))
</pre></div>
</div>
</div>
</div>
<p>With pyLDAvis, we get an interactive figure with intertopic distances and the most important words for each topic. More separate the topic “bubbles” are, better the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vis</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
<link rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css">


<div id="ldavis_el3333140501354869024505943978"></div>
<script type="text/javascript">

var ldavis_el3333140501354869024505943978_data = {"mdsDat": {"x": [0.05457046426385688, 0.16036865303276177, -0.10104083057291204, -0.05944846947973816, 0.141674166016302, -0.05584184502580461, 0.008914300316167342, -0.004656443280073171, -0.09276898661418019, -0.0517710086563797], "y": [0.09403008711480887, -0.010888130708230272, -0.03815050392677464, -0.054195084525442694, -0.0552140579926239, 0.07898857871733797, -0.06898918660325994, 0.08970678409766146, -0.023838586942209385, -0.01144989923126743], "topics": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "cluster": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "Freq": [12.68339729309082, 12.214055061340332, 11.917990684509277, 10.419438362121582, 9.61135196685791, 9.2654447555542, 9.096837043762207, 8.586726188659668, 8.56439208984375, 7.6403632164001465]}, "tinfo": {"Term": ["consumer", "team", "price", "patient", "brand", "network", "project", "marketing", "optimal", "capability", "privacy", "demand", "innovation", "organizational", "task", "security", "sale", "investment", "member", "retailer", "employee", "hospital", "purchase", "supplier", "inventory", "operation", "perceive", "job", "participant", "policy", "voluntary_profiling", "ocrs", "virality", "political_conservatism", "expert_blog", "online_firestorm", "perpetual_software", "valence_elasticity", "organic_listing", "connective_action", "dsmm", "paywall", "yoplait", "wom", "lock_trial", "og", "retransmission_intention", "earn_own", "target_couponing", "purchase_funnel", "employer_commit", "retweete", "click_conversion", "wom_valence", "organic_sponsor", "mindset_metric", "locational_target", "targeting", "last_fm", "ctr", "album", "harm_crisis", "tweet", "seeding", "proprietary_software", "retweet", "brand", "advertiser", "can_graphically", "right_restriction", "ewom", "twitter", "due_image", "advertising", "movie", "viral", "consumer", "buzz", "ad", "seller", "sentiment", "click", "keyword", "song", "valence", "purchase", "word_mouth", "rating", "marketing", "medium", "promotion", "conversion", "reputation", "search", "music", "blog", "content", "equilibrium", "message", "display", "negative", "price", "sale", "post", "channel", "network", "community", "platform", "equation", "category", "estimate", "feature", "web_delivery", "period", "target", "profit", "word", "entitativity", "auctioneer", "proportion_pragmatist", "gradient_estimator", "uncertainty_quantification", "discriminatory_auction", "polytope", "ocba", "probabilistic_expander", "pluck", "assembler", "bid_increment", "partially_observable", "bidder", "combinatorial_auction", "ascend_auction", "glaucoma", "unconcerned_pragmatist", "stochastic_kriging", "nonnetworked_prediction", "countable_state", "nxn", "primal_dual", "prot", "ambiguity_averse", "common_fate", "polygon", "bettor", "finitely_many", "continuous_combinatorial", "xn", "hybrid_seed", "exponential_smooth", "newsvendor", "debris", "affine", "convex_optimization", "chance_constrain", "summary_pile", "parent_seed", "theorem", "bid", "auction", "reformulation", "bidding", "optimality", "vertex", "optimization", "proof_theorem", "feedstock", "bound", "optimal", "contract", "procurement", "quantity", "supplier", "lemma", "algorithm", "convex", "distribution", "parameter", "approximation", "probability", "constraint", "finite", "let", "vector", "inequality", "solution", "solve", "formulation", "demand", "policy", "state", "operation", "inform", "price", "production", "random", "profit", "assume", "experiment", "instance", "estimate", "period", "risk", "sample", "thinklet", "implementer_response", "vendor_silence", "organize_vision", "chromatography_step", "epistemic_virtue", "functional_affordance", "controllee", "motorway", "moral_courage", "ternary_relationship", "chromatography", "lexicographic_strategy", "inscription", "taskforce", "eam", "protein_impurity", "reinvention", "sociomaterial_routine", "abductive_reasoning", "mindful_mindless", "critical_realism", "liquefaction", "temperance", "ontologically_clear", "ontologically_unclear", "archetypal_problem", "sapse", "polycentric_governance", "purity_yield", "sociomaterial", "ontological", "faithful_representation", "leader_behaviour", "ideation", "librarian", "idiographic", "affordance", "materiality", "generativity", "epistemic", "sustainability", "stigmatize", "clan", "grammar", "stakeholder", "project", "controller", "actor", "idea", "object", "ethic", "artifact", "moral", "ethical", "enact", "collaboration", "resistance", "innovation", "reuse", "sustainable", "governance", "action", "digital", "organizational", "concept", "innovator", "principle", "goal", "researcher", "community", "open", "software", "engineering", "identity", "contribution", "activity", "conceptual", "tool", "interview", "platform", "mechanism", "theoretical", "construct", "emerge", "manager", "solution", "participant", "task", "audit_litigation", "stretch_goal", "auditor_independence", "fiscal_quarter", "end_concession", "authorize_ebscohost", "female_director", "course_pack", "persistent_linking", "electronic_reserve", "newsletter_content", "syllabus_mean", "learning_please", "academic_licensee", "reporting_aggressiveness", "discretionary_accrual", "coeff_stat", "license_private", "desksale", "desk_salesperson", "neurodiverse", "entravision", "board_independence", "mssp", "micromarket", "misstatement", "attainment_discrepancy", "ethnographic_story", "confucian_culture", "color_photo", "licensee", "digitally_mature", "auditor", "restatement", "analytical_innovator", "reprint_quantity", "reproduce_transmit", "ceo", "include_photocopy", "permission_back", "audit", "academic_institution", "electronic_mechanical", "reprint_copy", "talent", "hr", "analytic", "board", "earning", "shareholder", "big", "executive", "say", "announcement", "investor", "analyst", "corporate", "really", "manager", "financial", "get", "people", "client", "employee", "thing", "think", "training", "director", "want", "marketing", "stock", "return", "leader", "risk", "look", "skill", "asset", "event", "sale", "human", "investment", "content", "team", "organizational", "strategic", "banknote", "raw_signal", "fintech_lender", "store_liquidation", "non_urgent", "overdraft", "ambulance_diversion", "uninformed_trader", "overdraft_fee", "regulatory_burden", "kernel_smooth", "subsidy_scheme", "augment_triage", "opaque_channel", "speculator", "size_overbooking", "interbank", "interbank_payment", "appointment_slot", "resupply", "battery_swap", "queue_length", "sell_solicitation", "capital_adequacy", "serviceability", "informed_trader", "semantic_theme", "opaque_selling", "predictable_variability", "loan_officer", "markdown", "triage", "ice_breaker", "round_trip", "railcar", "boat", "remanufacturing", "icu", "appointment", "ed", "wardrobe", "liquidation", "queue", "algorithmic_trading", "inpatient", "bank", "fintech", "emergency_department", "arrival", "clinic", "patient", "clearance", "bed", "discharge", "liquidity", "trading", "flight", "length_stay", "loan", "scheduling", "deprivation", "inventory", "schedule", "capacity", "pricing", "wait", "readmission", "delay", "price", "hospital", "demand", "physician", "trade", "period", "hour", "operation", "day", "policy", "stock", "optimal", "risk", "profit", "financial", "return", "production", "estimate", "inform", "revenue", "probability", "size", "distribution", "fix", "assume", "sample", "dynamic", "filler_interface", "effectuation", "breadth_sampling", "breach_occurrence", "attestation", "affective_appraisal", "topicality", "health_literacy", "gifting", "temporal_dissociation", "feeling_violation", "proposer_responder", "isp_violation", "nmsvs", "scent", "silk_road", "navigability", "safe_computing", "interpersonal_injustice", "telemedicine_consultation", "workgroup_norm", "privacy_seal", "remunerative", "intention_comply", "enticement", "topical_crawler", "topicality_status", "sequentiality", "hacker_forum", "ransomware", "darknet", "forage", "readoption", "advert", "breach", "bitcoin", "privacy", "polymorphic_warning", "habituation", "subjective_norm", "illegal", "security", "malicious", "crawler", "virtual_world", "cybercriminal", "proposer", "compliance", "password", "aesthetic", "personalization", "gratification", "attitude", "intention", "fear_appeal", "health", "neutralization", "biometric", "perceive", "website", "care", "disclosure", "attack", "web", "page", "violation", "hit", "threat", "perception", "personal", "medical", "hospital", "participant", "patient", "site", "computer", "item", "self", "construct", "survey", "internet", "interaction", "consumer", "employee", "policy", "risk", "task", "remittance", "malware_propagation", "evolvable", "clinical_hit", "smart_connected", "neutrality_regulation", "credit_grade", "strategic_posture", "outsidership", "sanitization", "augment_clinical", "red_queen", "invs", "franchisor", "alliance_rivalry", "compatriot", "tfp", "tfp_growth", "misinformation", "moss", "mosquito_net", "politeness_courtesy", "digit_naic", "modular_upgrade", "liability_foreignness", "dual_emphasis", "massive_fine", "modular_integral", "liability_outsidership", "default_repayment", "lending_club", "franchising", "stochastic_frontier", "alliance", "springboard", "downside_risk", "imitator", "bvit", "vertical_integration", "borrower", "franchisee", "interconnection", "spillover", "foreign", "net_neutrality", "venture", "country", "investment", "merger", "slant", "district", "domestic", "network", "partnership", "economy", "labor", "productivity", "lender", "innovation", "competitive", "platform", "growth", "strategic", "capability", "standard", "partner", "digital", "chain", "global", "international", "entry", "hotel", "supply_chain", "supply", "economic", "efficiency", "integration", "supplier", "transaction", "software", "risk", "capital", "production", "competition", "estimate", "demand", "hospital", "activity", "hypothesis", "size", "sample", "applicant_reaction", "reluctant_stayer", "enthusiastic_stayer", "reluctant_leaver", "gaming_element", "neural_correlate", "enthusiastic_leaver", "discover_unobserved", "batting_average", "brain_activation", "cortex", "uncover_unobserved", "symbolization", "wsq", "fimix_pls", "biofeedback", "shopping_assistance", "voxel", "hate_crime", "fimix", "overprediction", "cortical_activity", "understaffe_retail", "insular_cortex", "fast_frugal", "battle_channel", "tablet_commerce", "pickup_location", "temporal_landmark", "equally_skilled", "forgiveness", "racial_hate", "dimensional_surface", "chronological_age", "bop", "gamifie", "income_household", "gamified", "app", "helpfulness", "median_split", "piracy", "brain", "salesperson", "gamification", "arousal", "reset", "pirate", "affective", "game", "neuroscience", "st", "retailer", "applicant", "store", "tablet", "computer_anxiety", "trust", "shopping", "sale", "judgment", "turnover", "recommendation", "participant", "player", "selection", "gender", "psychology", "cognitive", "emotion", "personality", "signal", "bias", "subject", "perceive", "attribute", "channel", "sample", "experiment", "treatment", "task", "consumer", "item", "purchase", "preference", "people", "hypothesis", "estimate", "job", "construct", "interaction", "human", "size", "virtuality", "empower_leadership", "offshore_advanced", "cognitive_entrenchment", "spectacle", "mission_orient", "onsite_offshore", "niche_construction", "pitching", "tms", "transactive_memory", "umetric", "multiteam", "accurate_metaknowledge", "catch_fish", "prolific_scholar", "launcher", "engage_onshore", "plagiarism_detection", "translucence", "co_authorship", "retraction", "doctoral_recipient", "salary_premium", "shakedown_phase", "bliese", "stent", "fielder", "ambient_awareness", "divest_stock", "politician", "metaknowledge", "openness_divergent", "carefree", "codification", "geographic_dispersion", "corruption", "division_labor", "pathology", "boundary_spanner", "loaf", "marxist", "team", "grand_challenge", "loafing", "kms", "green", "member", "intellectual_capital", "tie", "leadership", "offshore", "technological_turbulence", "coordination", "coworker", "cohesion", "task", "job", "minority", "expertise", "virtual", "career", "specialization", "employee", "communication", "worker", "organizational", "leader", "project", "network", "conflict", "capital", "cultural", "collective", "contribution", "psychology", "interaction", "professional", "satisfaction", "goal", "human", "people", "trust", "hypothesis", "community", "policy", "mechanism", "deceiver", "detect_deception", "linguistic_cue", "cultural_signifier", "cue_multiplicity", "truth_teller", "credibility_assessment", "capitalize_agility", "stack_classifier", "hot_patent", "oculometric", "vocal_pitch", "physical_countermeasure", "deception_detection", "rigidity_detection", "transmission_velocity", "prevention_mitigation", "defect_discovery", "polygraph", "signifier", "intrinsic_hedonic", "subjective_vitality", "proactive_stance", "distance_humane", "productivity_tenured", "prn", "tmp", "nanotechnology_patent", "mock_crime", "yearly_quarterly", "scm", "expatriate", "forward_citation", "nanotechnology", "contextual_ambidexterity", "agility", "deception", "procedural_rationality", "multitasking", "scientometric", "perceptual_congruence", "intellectual_alignment", "environmental_dynamism", "citation", "humanitarian", "patent", "classifier", "oscm", "disaster", "pupil_dilation", "rigidity", "alignment", "nonverbal", "fraud", "capability", "aid", "scam", "kernel", "competency", "deceptive", "journal", "culture", "detection", "cultural", "construct", "classification", "organizational", "integration", "dimension", "supply_chain", "strategic", "item", "communication", "feature", "word", "measurement", "top", "manager", "score", "indicator", "operation", "human", "activity", "interaction", "assess", "hypothesis", "evaluation", "sample", "partner"], "Freq": [36331.0, 17458.0, 29293.0, 15860.0, 13009.0, 26166.0, 17172.0, 21684.0, 13458.0, 12951.0, 7133.0, 19880.0, 15375.0, 18784.0, 16562.0, 8078.0, 16855.0, 12319.0, 9439.0, 8439.0, 13626.0, 8031.0, 11692.0, 8632.0, 7408.0, 14761.0, 11100.0, 9811.0, 13661.0, 17578.0, 165.88775634765625, 189.69091796875, 171.22389221191406, 114.91133117675781, 298.4919738769531, 73.76009368896484, 308.93505859375, 92.27820587158203, 89.24388885498047, 121.8272476196289, 110.84297943115234, 235.6184539794922, 49.342384338378906, 2358.289794921875, 54.21345520019531, 77.95836639404297, 68.04219055175781, 77.79206848144531, 57.00105285644531, 89.53903198242188, 35.25086212158203, 113.69353485107422, 98.93295288085938, 138.12501525878906, 42.00939178466797, 74.24689483642578, 44.88591384887695, 72.24467468261719, 61.485599517822266, 35.04853820800781, 579.7306518554688, 201.19985961914062, 1749.8658447265625, 336.85797119140625, 257.2587585449219, 159.22662353515625, 11254.2373046875, 1556.17236328125, 904.9564819335938, 895.152587890625, 794.1400756835938, 1537.3211669921875, 892.341552734375, 4640.88671875, 2249.09375, 688.6135864257812, 24507.833984375, 304.4031677246094, 3130.68359375, 3879.6298828125, 2075.897216796875, 1618.453369140625, 2224.20947265625, 762.9327392578125, 1208.1956787109375, 6865.15185546875, 1174.376220703125, 4463.537109375, 11332.681640625, 7513.6728515625, 2332.8994140625, 924.1656494140625, 2364.551513671875, 6340.466796875, 1264.5904541015625, 1235.30029296875, 5518.67041015625, 2417.864013671875, 2664.748046875, 2224.963134765625, 4021.266845703125, 6988.986328125, 4714.35986328125, 2939.5869140625, 2424.743408203125, 5825.05517578125, 3156.863037109375, 3212.270263671875, 2954.06640625, 2968.439697265625, 3604.73876953125, 3140.37060546875, 2625.248291015625, 2999.7421875, 2554.49169921875, 2617.650146484375, 2503.942626953125, 125.84272766113281, 149.08184814453125, 48.787025451660156, 116.63207244873047, 50.74876403808594, 58.66566467285156, 80.37761688232422, 45.54773712158203, 74.34785461425781, 40.787052154541016, 171.0742645263672, 39.4595832824707, 126.45388793945312, 2711.2548828125, 314.302490234375, 37.44245910644531, 154.51939392089844, 26.40996551513672, 55.021034240722656, 37.19148635864258, 40.24698257446289, 31.344167709350586, 63.727237701416016, 24.36753273010254, 27.353473663330078, 174.982421875, 67.14018249511719, 25.333576202392578, 36.06191635131836, 62.95645523071289, 101.70195007324219, 81.74784851074219, 58.32863998413086, 742.1188354492188, 271.5386657714844, 200.79457092285156, 128.7253875732422, 99.00525665283203, 109.20771789550781, 68.77510070800781, 2182.90234375, 2697.0478515625, 3607.493408203125, 266.2630615234375, 883.2583618164062, 781.4544677734375, 302.9576110839844, 3790.022705078125, 270.53759765625, 99.96656036376953, 1330.3768310546875, 8121.10693359375, 5280.2099609375, 954.915283203125, 2913.577880859375, 5121.56591796875, 1031.947265625, 4102.0087890625, 494.4842834472656, 6732.31591796875, 4978.91064453125, 1243.42724609375, 4753.52978515625, 3284.5361328125, 651.6932373046875, 2436.058837890625, 1808.9853515625, 911.8740844726562, 5244.76318359375, 3110.223876953125, 1319.5374755859375, 7058.32470703125, 6126.7470703125, 4226.05810546875, 5270.7119140625, 5435.595703125, 8553.7587890625, 4088.56494140625, 2306.48046875, 3925.189697265625, 3141.49609375, 3458.15283203125, 2619.9794921875, 3605.619140625, 3374.02783203125, 3155.477783203125, 2612.365234375, 211.4652557373047, 77.15692901611328, 108.77220153808594, 148.2920379638672, 151.0751953125, 77.98030853271484, 192.5750274658203, 87.81710052490234, 53.23384475708008, 49.212738037109375, 78.76011657714844, 65.80842590332031, 41.66581726074219, 220.27197265625, 108.9266586303711, 56.7489128112793, 37.99330520629883, 277.6780700683594, 64.46472930908203, 96.74353790283203, 37.064910888671875, 128.90293884277344, 37.051883697509766, 35.099849700927734, 49.74253463745117, 57.514522552490234, 40.8931770324707, 34.07106399536133, 45.774993896484375, 33.95287322998047, 267.6340026855469, 302.2954406738281, 46.63933181762695, 270.9805603027344, 438.8921813964844, 327.30218505859375, 80.02040100097656, 960.4219970703125, 196.2870330810547, 94.95530700683594, 255.72369384765625, 2027.3778076171875, 147.77964782714844, 371.132080078125, 263.6164245605469, 2821.14111328125, 9980.4404296875, 257.2293701171875, 2641.3740234375, 5716.37158203125, 1651.4219970703125, 2054.65185546875, 1797.3343505859375, 868.1731567382812, 1593.831787109375, 593.1690673828125, 2587.043701171875, 674.8887939453125, 6563.7890625, 557.8067016601562, 880.2430419921875, 2057.587646484375, 3887.520751953125, 3101.774658203125, 5112.58544921875, 2210.471435546875, 581.9821166992188, 1340.2320556640625, 2857.52783203125, 2836.06005859375, 3026.70361328125, 2121.15185546875, 2952.0517578125, 1033.3621826171875, 1319.280517578125, 2440.310546875, 3084.094482421875, 1293.357421875, 2061.200439453125, 1511.8369140625, 2328.446044921875, 2159.765869140625, 1994.171875, 2086.556640625, 1660.53076171875, 1989.64453125, 1733.0159912109375, 1723.8409423828125, 1729.0433349609375, 105.87035369873047, 154.67320251464844, 68.3287353515625, 85.71682739257812, 119.60015106201172, 286.9207763671875, 64.44556427001953, 286.8753356933594, 572.9495849609375, 286.5827331542969, 286.5077209472656, 286.4778137207031, 286.4262390136719, 286.3697509765625, 49.58852767944336, 44.96467208862305, 89.08026885986328, 287.00714111328125, 95.54438018798828, 55.9501838684082, 48.528133392333984, 65.92378997802734, 73.09675598144531, 146.26730346679688, 101.42729187011719, 48.30037307739258, 38.23519515991211, 81.10433197021484, 42.73972702026367, 36.42985153198242, 288.40032958984375, 57.26955795288086, 806.6845092773438, 134.63772583007812, 180.65884399414062, 141.97793579101562, 144.32974243164062, 710.5751342773438, 142.2589111328125, 141.12498474121094, 1126.81103515625, 307.6631164550781, 143.7130126953125, 122.4549560546875, 940.8334350585938, 890.85791015625, 4967.33642578125, 1100.490966796875, 846.2622680664062, 604.51123046875, 3068.039306640625, 2289.346435546875, 2427.201904296875, 884.1178588867188, 2060.968017578125, 1152.6048583984375, 3048.29150390625, 1009.145263671875, 5204.6103515625, 4123.83642578125, 2280.589599609375, 4382.0185546875, 2188.69970703125, 4347.9033203125, 1588.4737548828125, 2108.508056640625, 1999.9752197265625, 871.7208251953125, 1624.0091552734375, 5125.34765625, 2225.92236328125, 2535.86669921875, 1832.3026123046875, 3382.240966796875, 1669.9361572265625, 1605.8623046875, 1755.5556640625, 1778.236328125, 2329.89111328125, 1958.302978515625, 1942.1678466796875, 1971.6136474609375, 1971.0130615234375, 1706.116455078125, 1669.900634765625, 135.98495483398438, 209.11558532714844, 72.43405151367188, 102.38838958740234, 74.17669677734375, 129.5301513671875, 83.46916961669922, 112.31452941894531, 118.64737701416016, 49.51589584350586, 51.95988082885742, 63.3400764465332, 68.86676788330078, 60.328853607177734, 78.90088653564453, 50.170677185058594, 60.160587310791016, 53.3955078125, 46.32034683227539, 60.11241912841797, 46.25146484375, 200.64846801757812, 35.93683624267578, 28.532581329345703, 32.24296569824219, 34.942230224609375, 51.18788146972656, 30.232250213623047, 26.591087341308594, 190.74395751953125, 406.74432373046875, 320.9765319824219, 92.09513092041016, 67.54586791992188, 56.62112808227539, 286.0158386230469, 185.49398803710938, 83.44996643066406, 1090.9495849609375, 340.2993469238281, 112.90813446044922, 150.00442504882812, 1202.495849609375, 133.4380340576172, 191.2691650390625, 3209.099853515625, 442.8140869140625, 132.628662109375, 1494.1195068359375, 803.0574951171875, 10001.4443359375, 282.83056640625, 746.0057983398438, 562.8330078125, 575.2144165039062, 1613.2506103515625, 823.4152221679688, 222.7965850830078, 1339.914794921875, 1286.4132080078125, 322.8240661621094, 3921.96533203125, 2160.525390625, 3968.20849609375, 3516.356201171875, 2055.60009765625, 587.4417724609375, 1727.96923828125, 10589.4287109375, 3372.689453125, 6763.8828125, 1674.2786865234375, 2074.30126953125, 4508.0478515625, 1404.0673828125, 3823.85498046875, 2634.979248046875, 4203.6484375, 2204.4912109375, 3234.08740234375, 3832.76220703125, 2733.72900390625, 2629.489990234375, 2111.60107421875, 2327.10107421875, 2653.195068359375, 2503.4765625, 2021.251220703125, 1937.5797119140625, 2013.815185546875, 1983.60693359375, 1773.466552734375, 1798.7698974609375, 1858.908935546875, 1801.66845703125, 305.0329284667969, 83.08160400390625, 56.47529602050781, 52.579952239990234, 151.46286010742188, 81.44477844238281, 84.39906311035156, 58.0582160949707, 61.85364532470703, 66.67687225341797, 52.215431213378906, 76.27764129638672, 64.61029052734375, 34.6805419921875, 197.27806091308594, 39.39784240722656, 217.39501953125, 57.34649658203125, 35.29341125488281, 39.056312561035156, 24.769704818725586, 88.55266571044922, 29.442527770996094, 109.29280090332031, 48.23381042480469, 27.534894943237305, 26.565099716186523, 67.46243286132812, 102.57228088378906, 41.676734924316406, 130.76531982421875, 82.38822174072266, 73.80403137207031, 78.23193359375, 1882.712646484375, 516.6771240234375, 5934.4033203125, 79.82974243164062, 182.1884002685547, 322.18597412109375, 586.9839477539062, 5441.75634765625, 235.1881103515625, 291.6132507324219, 649.4288940429688, 168.61463928222656, 144.17770385742188, 1487.6224365234375, 252.24581909179688, 427.6021423339844, 1653.829833984375, 147.6885528564453, 2631.0107421875, 2806.0625, 483.38970947265625, 2626.62353515625, 196.93504333496094, 246.27720642089844, 4683.31787109375, 3459.578125, 2395.28466796875, 1913.015869140625, 772.895263671875, 3407.029052734375, 2134.9609375, 726.11865234375, 1276.4630126953125, 1367.6990966796875, 2486.955810546875, 1936.1973876953125, 1378.269775390625, 2347.9580078125, 3297.871337890625, 3469.15185546875, 2143.8984375, 2010.2357177734375, 2516.666015625, 1894.8143310546875, 2552.291748046875, 1950.313232421875, 1869.0357666015625, 2447.840576171875, 2823.3896484375, 2078.090576171875, 2133.689453125, 2071.668701171875, 1902.517578125, 286.51171875, 89.67869567871094, 30.933273315429688, 152.64224243164062, 379.92437744140625, 25.5616512298584, 98.3170166015625, 189.60202026367188, 29.585163116455078, 35.1807861328125, 142.60629272460938, 21.196243286132812, 20.218515396118164, 160.68069458007812, 18.830894470214844, 18.091339111328125, 41.10292053222656, 27.00566864013672, 330.9081726074219, 50.78580093383789, 14.734180450439453, 26.65961456298828, 49.65300369262695, 37.66615676879883, 26.740001678466797, 96.8910140991211, 18.511619567871094, 15.715932846069336, 18.647619247436523, 12.71578311920166, 25.377002716064453, 90.84041595458984, 61.379940032958984, 1677.13232421875, 143.3009490966797, 195.03286743164062, 193.69435119628906, 152.92160034179688, 175.73660278320312, 862.8292846679688, 103.82160186767578, 101.57447814941406, 1174.364990234375, 932.84716796875, 330.1643371582031, 1428.14892578125, 4045.907470703125, 6247.04541015625, 402.3372497558594, 132.84632873535156, 350.9981384277344, 518.0409545898438, 9363.8369140625, 852.0218505859375, 1609.4578857421875, 1303.8223876953125, 1471.050537109375, 494.0007629394531, 4859.95703125, 2243.282470703125, 3393.474853515625, 2073.31103515625, 3435.056884765625, 3676.259521484375, 2792.664306640625, 2102.99560546875, 2689.894775390625, 1295.0010986328125, 1976.7877197265625, 1656.139404296875, 1224.5770263671875, 900.5997924804688, 1945.687744140625, 1534.6610107421875, 2516.4521484375, 1717.16845703125, 1488.812744140625, 2008.5269775390625, 1730.958251953125, 2149.29443359375, 2516.86962890625, 1614.2723388671875, 2073.359130859375, 1641.050537109375, 2248.88818359375, 2335.2958984375, 1762.4820556640625, 1853.8511962890625, 1756.856689453125, 1705.4173583984375, 1674.8953857421875, 181.15597534179688, 90.25796508789062, 74.48637390136719, 90.0313491821289, 72.36680603027344, 74.26213836669922, 57.53984451293945, 58.453983306884766, 40.87028884887695, 42.73411178588867, 90.07701110839844, 43.68097686767578, 75.53997802734375, 78.63055419921875, 42.64964294433594, 64.98094177246094, 40.55424499511719, 48.24045944213867, 108.12682342529297, 34.68672561645508, 29.8482723236084, 27.937883377075195, 27.892606735229492, 26.907045364379883, 102.85418701171875, 26.889022827148438, 26.8696231842041, 30.6186580657959, 24.931785583496094, 23.94951057434082, 158.71226501464844, 93.73461151123047, 77.43348693847656, 49.432254791259766, 148.4064178466797, 66.03446197509766, 150.52951049804688, 115.9587631225586, 3033.922607421875, 407.1243591308594, 189.35830688476562, 1001.119873046875, 685.6700439453125, 1076.4219970703125, 328.08251953125, 589.0848388671875, 284.7016296386719, 282.2112731933594, 1240.983154296875, 3006.387451171875, 219.9705047607422, 248.29782104492188, 4137.8916015625, 442.5083923339844, 3314.56103515625, 268.8969421386719, 147.5644073486328, 3503.706787109375, 943.6852416992188, 5635.1591796875, 1479.13232421875, 914.0856323242188, 2232.0810546875, 4202.826171875, 1070.7984619140625, 2363.851806640625, 1032.1544189453125, 2019.07861328125, 1943.783447265625, 1177.097900390625, 1065.807861328125, 1576.6824951171875, 2072.06787109375, 2023.577392578125, 2448.068603515625, 2002.2296142578125, 1575.5006103515625, 2747.360107421875, 2207.48681640625, 1805.2562255859375, 2475.88525390625, 3430.56005859375, 1981.4549560546875, 2029.5511474609375, 1552.9761962890625, 1960.868896484375, 1778.0540771484375, 1987.72607421875, 1710.328369140625, 1835.3642578125, 1873.49365234375, 1670.9903564453125, 1628.36962890625, 471.0023498535156, 183.0958251953125, 56.83424758911133, 78.6043930053711, 153.29852294921875, 62.687557220458984, 72.97396850585938, 53.31700134277344, 27.81684684753418, 104.75582122802734, 182.43069458007812, 36.05010986328125, 22.985157012939453, 20.101245880126953, 36.43926239013672, 38.26487350463867, 16.27714729309082, 19.87354850769043, 15.159305572509766, 15.294570922851562, 63.23234939575195, 93.06641387939453, 18.021177291870117, 45.834476470947266, 48.57811737060547, 14.353053092956543, 47.44398880004883, 13.415549278259277, 14.329024314880371, 14.385773658752441, 473.11761474609375, 27.65154266357422, 21.192829132080078, 32.67601776123047, 345.39251708984375, 70.50199127197266, 575.5015869140625, 195.60366821289062, 173.74395751953125, 101.75000762939453, 57.11780548095703, 53.818660736083984, 12899.0478515625, 112.14009094238281, 272.0143127441406, 340.255615234375, 908.158447265625, 5473.2919921875, 156.9811553955078, 2707.64111328125, 2407.536865234375, 594.3741455078125, 149.67784118652344, 2136.408203125, 398.6363220214844, 217.6671600341797, 6433.75732421875, 3945.640380859375, 363.6861572265625, 1637.8782958984375, 1709.0369873046875, 689.1767578125, 322.87396240234375, 3912.887939453125, 3071.604248046875, 1235.6627197265625, 4369.18017578125, 1654.4600830078125, 4025.050048828125, 5093.34521484375, 1114.3287353515625, 1729.2613525390625, 1446.3316650390625, 992.81201171875, 1944.9520263671875, 1475.93994140625, 2490.332763671875, 1365.5430908203125, 1361.1103515625, 1541.0631103515625, 1642.269287109375, 1785.7628173828125, 1455.1739501953125, 1560.2000732421875, 1525.2608642578125, 1484.1903076171875, 1325.839599609375, 293.8282165527344, 150.2847137451172, 74.00285339355469, 66.96197509765625, 91.76030731201172, 87.86891174316406, 338.3503723144531, 32.33308792114258, 55.144493103027344, 47.530120849609375, 53.196205139160156, 36.02780532836914, 43.56107711791992, 375.2113037109375, 45.38493347167969, 26.4339599609375, 27.36060905456543, 38.64579772949219, 58.55027770996094, 227.98533630371094, 42.026649475097656, 75.97514343261719, 30.9127197265625, 23.43309783935547, 22.48832893371582, 44.93550491333008, 39.13882827758789, 18.60700035095215, 27.862668991088867, 62.05810546875, 262.03173828125, 400.048583984375, 101.0250473022461, 107.61956787109375, 126.24051666259766, 1122.937744140625, 961.6844482421875, 96.14682006835938, 194.32960510253906, 80.95372772216797, 76.15274047851562, 85.47737884521484, 166.5974884033203, 1344.8123779296875, 455.6242980957031, 2269.22265625, 843.6402587890625, 90.890380859375, 1288.064208984375, 141.04019165039062, 254.72802734375, 1297.3570556640625, 131.9281768798828, 791.3231811523438, 5429.27294921875, 990.8256225585938, 143.82981872558594, 403.9404602050781, 709.3619384765625, 213.97401428222656, 1387.6500244140625, 1626.5855712890625, 761.3504638671875, 1564.685791015625, 3199.964111328125, 1171.33154296875, 3806.597412109375, 1412.1534423828125, 1730.452880859375, 1739.2557373046875, 2116.610595703125, 2022.793701171875, 1953.996337890625, 1964.7294921875, 1600.0440673828125, 1183.7508544921875, 1294.991455078125, 1912.642822265625, 1334.0145263671875, 1112.446533203125, 1758.4471435546875, 1420.42529296875, 1437.920166015625, 1426.5458984375, 1213.650390625, 1287.3447265625, 1221.84912109375, 1259.407958984375, 1189.060546875], "Total": [36331.0, 17458.0, 29293.0, 15860.0, 13009.0, 26166.0, 17172.0, 21684.0, 13458.0, 12951.0, 7133.0, 19880.0, 15375.0, 18784.0, 16562.0, 8078.0, 16855.0, 12319.0, 9439.0, 8439.0, 13626.0, 8031.0, 11692.0, 8632.0, 7408.0, 14761.0, 11100.0, 9811.0, 13661.0, 17578.0, 167.71376037597656, 192.11770629882812, 173.7539825439453, 116.76470947265625, 303.7851257324219, 75.13560485839844, 315.1736145019531, 94.43850708007812, 91.39860534667969, 124.86676788330078, 113.65485382080078, 241.66238403320312, 50.70648193359375, 2423.805419921875, 55.74235153198242, 80.21050262451172, 70.01598358154297, 80.17301177978516, 58.787620544433594, 92.35781860351562, 36.42869186401367, 117.64918518066406, 102.408203125, 143.22055053710938, 43.57684326171875, 77.02318572998047, 46.60643768310547, 75.03466796875, 63.8809814453125, 36.47362518310547, 609.5721435546875, 209.99142456054688, 1867.347900390625, 356.12213134765625, 272.2706604003906, 167.30906677246094, 13009.21875, 1730.337158203125, 1000.6280517578125, 993.6142578125, 880.4374389648438, 1748.045654296875, 995.71240234375, 5726.33203125, 2677.88720703125, 775.449951171875, 36331.99609375, 342.3692321777344, 4325.90771484375, 5552.47705078125, 2852.845458984375, 2169.49951171875, 3106.3544921875, 950.927734375, 1615.3250732421875, 11692.5458984375, 1567.75146484375, 7305.98388671875, 21684.58984375, 13812.4609375, 3572.901123046875, 1211.0772705078125, 3893.779541015625, 13382.4853515625, 1805.4490966796875, 1760.9573974609375, 13109.5966796875, 4651.779296875, 5500.82568359375, 4212.4873046875, 11548.173828125, 29293.08203125, 16855.025390625, 7532.44140625, 5529.00390625, 26166.3671875, 10036.0126953125, 10849.4833984375, 9271.515625, 9560.9345703125, 16556.759765625, 11314.3388671875, 7596.181640625, 15195.6591796875, 8466.9326171875, 11436.314453125, 8029.1767578125, 127.79415893554688, 152.5276336669922, 49.9894905090332, 119.68659210205078, 52.116180419921875, 60.33405685424805, 82.81047058105469, 46.988460540771484, 76.71087646484375, 42.1866455078125, 176.9608917236328, 40.85122299194336, 130.93679809570312, 2808.030517578125, 325.5262756347656, 38.79341125488281, 160.2401885986328, 27.447715759277344, 57.19343948364258, 38.67894744873047, 41.868778228759766, 32.642433166503906, 66.38220977783203, 25.398836135864258, 28.5496768951416, 182.71656799316406, 70.13233947753906, 26.470272064208984, 37.728553771972656, 65.99868774414062, 106.96599578857422, 85.88997650146484, 61.20227813720703, 801.7452392578125, 290.8392639160156, 215.03599548339844, 137.67819213867188, 105.06080627441406, 116.6201171875, 72.50514221191406, 2604.813720703125, 3299.970947265625, 4514.46142578125, 299.5514831542969, 1066.8214111328125, 943.8760375976562, 346.44873046875, 5260.453125, 318.5462646484375, 107.9524917602539, 1816.7777099609375, 13458.9599609375, 8522.9599609375, 1295.1934814453125, 4568.16552734375, 8632.96484375, 1451.6566162109375, 6975.3857421875, 638.2931518554688, 13386.556640625, 9680.314453125, 1912.067138671875, 9460.1630859375, 6265.5927734375, 901.0695190429688, 4466.5283203125, 3114.0986328125, 1360.513671875, 11724.4404296875, 6227.3330078125, 2247.855224609375, 19880.55078125, 17578.20703125, 10903.9697265625, 14761.2314453125, 15459.8779296875, 29293.08203125, 11603.2626953125, 4907.3701171875, 11436.314453125, 8384.9990234375, 10869.9208984375, 7715.50927734375, 16556.759765625, 15195.6591796875, 18356.64453125, 15868.1796875, 214.07940673828125, 78.6673583984375, 110.94395446777344, 151.3798828125, 154.3782196044922, 79.71184539794922, 196.86790466308594, 89.78290557861328, 54.44783401489258, 50.3846321105957, 80.70518493652344, 67.4627456665039, 42.724334716796875, 226.1688995361328, 111.93465423583984, 58.46451950073242, 39.16541290283203, 286.4754943847656, 66.53915405273438, 99.87008666992188, 38.27500915527344, 133.12225341796875, 38.26917266845703, 36.25315475463867, 51.40034866333008, 59.48129653930664, 42.295021057128906, 35.240055084228516, 47.34757995605469, 35.1257438659668, 278.4251708984375, 317.86309814453125, 48.29568099975586, 286.143798828125, 468.9517517089844, 351.40142822265625, 83.58507537841797, 1057.751953125, 210.3379669189453, 100.02717590332031, 278.22845458984375, 2421.901611328125, 160.6526641845703, 431.71307373046875, 307.6650390625, 4105.6533203125, 17172.751953125, 303.77508544921875, 4118.302734375, 10146.822265625, 2646.884521484375, 3450.45361328125, 2973.349853515625, 1278.1844482421875, 2637.587890625, 837.9935302734375, 4932.205078125, 977.246826171875, 15375.453125, 794.0491943359375, 1454.579345703125, 4405.298828125, 10965.751953125, 8564.8310546875, 18784.90234375, 5785.609375, 883.96484375, 2915.800048828125, 8816.8359375, 9098.0419921875, 10036.0126953125, 6001.8486328125, 10225.6044921875, 2085.045166015625, 3054.55126953125, 8345.2626953125, 12986.92578125, 3083.06640625, 7599.97509765625, 4282.44140625, 10849.4833984375, 9277.267578125, 7971.08984375, 13220.2451171875, 6235.51611328125, 14407.6318359375, 11724.4404296875, 13661.818359375, 16562.544921875, 107.59750366210938, 157.58621215820312, 69.8240966796875, 87.67787170410156, 122.37117004394531, 293.95867919921875, 66.02696990966797, 293.9691467285156, 587.1767578125, 294.0054626464844, 294.02783203125, 294.0186462402344, 294.0355224609375, 294.0285339355469, 50.92780685424805, 46.1905517578125, 91.5113754272461, 294.9632873535156, 98.20413208007812, 57.56061935424805, 50.02909851074219, 67.9931869506836, 75.43414306640625, 151.1230010986328, 104.86329650878906, 49.93671798706055, 39.54439926147461, 83.97134399414062, 44.2518196105957, 37.730106353759766, 301.94512939453125, 59.461917877197266, 876.6915893554688, 141.68975830078125, 192.8276824951172, 150.42759704589844, 153.3433074951172, 800.59326171875, 151.43812561035156, 150.51779174804688, 1353.3211669921875, 344.4195251464844, 154.33853149414062, 130.61068725585938, 1186.3062744140625, 1167.128662109375, 7915.31005859375, 1487.50048828125, 1114.697021484375, 777.1456298828125, 5515.822265625, 4031.22265625, 4468.93505859375, 1369.50537109375, 3847.819091796875, 1916.70263671875, 6321.2548828125, 1736.9705810546875, 14407.6318359375, 11163.3515625, 5382.97998046875, 13630.3505859375, 5139.79345703125, 13626.283203125, 3318.05078125, 5207.58251953125, 5172.8544921875, 1520.2352294921875, 3859.197509765625, 21684.58984375, 6474.96875, 7981.154296875, 4901.18017578125, 18356.64453125, 4667.1943359375, 4347.072265625, 5265.8955078125, 7512.71435546875, 16855.025390625, 10410.123046875, 12319.8662109375, 13109.5966796875, 17458.208984375, 18784.90234375, 11275.712890625, 137.5137939453125, 212.68772888183594, 73.70310974121094, 104.3698501586914, 75.623291015625, 132.1600341796875, 85.2196273803711, 114.872314453125, 121.38174438476562, 50.68119430541992, 53.38515090942383, 65.10124206542969, 70.86912536621094, 62.1419563293457, 81.27763366699219, 51.695735931396484, 62.124515533447266, 55.14057159423828, 47.860107421875, 62.149723052978516, 47.83284378051758, 207.82481384277344, 37.27248001098633, 29.618749618530273, 33.48157501220703, 36.33621597290039, 53.24402618408203, 31.4851131439209, 27.722259521484375, 199.134033203125, 427.1177673339844, 340.28961181640625, 96.52851104736328, 70.6753921508789, 59.21391677856445, 312.90850830078125, 200.58419799804688, 88.11895751953125, 1255.09716796875, 378.2216796875, 120.66779327392578, 162.239501953125, 1421.9111328125, 143.70535278320312, 211.1455078125, 4243.20849609375, 516.470947265625, 143.88478088378906, 1932.559814453125, 1014.36669921875, 15860.052734375, 329.44146728515625, 973.5870971679688, 712.4609985351562, 734.7767944335938, 2332.635986328125, 1104.3681640625, 257.0250549316406, 1991.3499755859375, 1919.82568359375, 393.1983947753906, 7408.9052734375, 3704.66650390625, 7640.5302734375, 6833.7265625, 3681.962890625, 815.4578247070312, 3078.190185546875, 29293.08203125, 8031.48291015625, 19880.55078125, 3263.501953125, 4845.67529296875, 15195.6591796875, 2991.276123046875, 14761.2314453125, 8460.6767578125, 17578.20703125, 6474.96875, 13458.9599609375, 18356.64453125, 11436.314453125, 11163.3515625, 7981.154296875, 11603.2626953125, 16556.759765625, 15459.8779296875, 8602.421875, 9460.1630859375, 11967.0146484375, 13386.556640625, 6686.6513671875, 8384.9990234375, 15868.1796875, 10756.24609375, 308.8941345214844, 84.54146575927734, 57.72285079956055, 53.74681091308594, 155.2228240966797, 83.57611083984375, 86.65959930419922, 59.67073059082031, 63.65617752075195, 68.6217041015625, 53.739173889160156, 78.5875244140625, 66.66815948486328, 35.807132720947266, 203.94786071777344, 40.752830505371094, 226.15640258789062, 59.67262268066406, 36.77869415283203, 40.73048782348633, 25.837779998779297, 92.40815734863281, 30.804780960083008, 114.36908721923828, 50.49575424194336, 28.82805633544922, 27.8309326171875, 70.68217468261719, 107.50346374511719, 43.733367919921875, 137.50515747070312, 86.50060272216797, 77.55142974853516, 82.49818420410156, 2084.3134765625, 571.2486572265625, 7133.0634765625, 84.63211822509766, 199.23809814453125, 373.84600830078125, 710.9302368164062, 8078.54052734375, 271.2300109863281, 343.5450744628906, 857.6399536132812, 193.5540771484375, 162.71914672851562, 2202.754638671875, 304.6181335449219, 555.917724609375, 2549.8671875, 169.81175231933594, 4694.90625, 5094.06689453125, 669.9024658203125, 5208.6337890625, 239.0875701904297, 312.3891906738281, 11100.62890625, 7917.28125, 5054.8583984375, 3888.2978515625, 1287.478271484375, 8290.0703125, 4980.59912109375, 1262.3638916015625, 2812.633056640625, 3094.676513671875, 7173.4560546875, 5713.79736328125, 3514.02099609375, 8031.48291015625, 13661.818359375, 15860.052734375, 7655.84326171875, 7001.46533203125, 10773.0712890625, 6979.2509765625, 13220.2451171875, 7885.36376953125, 7600.228515625, 14683.076171875, 36331.99609375, 13626.283203125, 17578.20703125, 18356.64453125, 16562.544921875, 292.8619689941406, 91.917236328125, 32.059776306152344, 158.4025115966797, 395.0892028808594, 26.656095504760742, 102.67582702636719, 198.09559631347656, 30.946340560913086, 36.86534118652344, 149.64976501464844, 22.34843635559082, 21.32342529296875, 169.6907501220703, 19.93453025817871, 19.1650333404541, 43.72378158569336, 28.76856803894043, 352.672607421875, 54.15040969848633, 15.77705192565918, 28.558460235595703, 53.22368240356445, 40.41172790527344, 28.700687408447266, 104.33802032470703, 19.939847946166992, 16.937448501586914, 20.13471794128418, 13.743119239807129, 27.465980529785156, 99.90816497802734, 67.89234161376953, 2028.6278076171875, 161.39492797851562, 225.20945739746094, 225.28819274902344, 177.1099395751953, 205.44146728515625, 1137.1773681640625, 120.30712127685547, 119.58628845214844, 1727.923095703125, 1375.0498046875, 439.6208801269531, 2309.689697265625, 7528.841796875, 12319.8662109375, 588.97265625, 166.00653076171875, 520.088623046875, 829.7415771484375, 26166.3671875, 1516.205810546875, 3376.2578125, 2729.201416015625, 3186.637451171875, 815.0689697265625, 15375.453125, 6037.30419921875, 10849.4833984375, 5641.88134765625, 11275.712890625, 12951.541015625, 8948.8466796875, 6038.73291015625, 8564.8310546875, 3246.000244140625, 5944.955078125, 4864.59716796875, 3118.23779296875, 1990.92724609375, 6535.81982421875, 4712.6044921875, 10651.826171875, 5844.64697265625, 4575.82470703125, 8632.96484375, 6447.513671875, 10225.6044921875, 18356.64453125, 5711.01025390625, 11603.2626953125, 6004.37158203125, 16556.759765625, 19880.55078125, 8031.48291015625, 12986.92578125, 10715.5986328125, 11967.0146484375, 15868.1796875, 184.54229736328125, 92.22494506835938, 76.17230224609375, 92.21955108642578, 74.1552963256836, 76.13482666015625, 59.11470031738281, 60.101497650146484, 42.0355339050293, 44.05680465698242, 92.88877868652344, 45.05242919921875, 77.94928741455078, 81.18256378173828, 44.04291915893555, 67.11815643310547, 42.04347610473633, 50.051605224609375, 112.18814086914062, 36.01852035522461, 30.996471405029297, 29.021116256713867, 28.98609161376953, 28.010589599609375, 107.08006286621094, 27.997034072875977, 28.004669189453125, 31.93101692199707, 26.003538131713867, 24.991106033325195, 165.98614501953125, 98.07530212402344, 81.0716552734375, 51.73659133911133, 157.17483520507812, 70.0672607421875, 165.01046752929688, 126.03993225097656, 3919.4853515625, 473.8212585449219, 217.38809204101562, 1314.48193359375, 887.0715942382812, 1487.098388671875, 405.3316650390625, 777.4669189453125, 351.098388671875, 350.0075378417969, 1928.468505859375, 5391.96435546875, 271.54193115234375, 312.1263122558594, 8439.9375, 628.1201171875, 6909.35205078125, 350.15740966796875, 174.76889038085938, 8341.2158203125, 1688.389404296875, 16855.025390625, 3109.325927734375, 1716.9052734375, 5369.49072265625, 13661.818359375, 2333.5693359375, 6781.75146484375, 2229.88720703125, 5729.064453125, 5530.07958984375, 2840.89013671875, 2558.92529296875, 4533.783203125, 7994.41259765625, 8028.279296875, 11100.62890625, 8076.16748046875, 5529.00390625, 15868.1796875, 10869.9208984375, 7785.59619140625, 16562.544921875, 36331.99609375, 10773.0712890625, 11692.5458984375, 6815.1142578125, 13630.3505859375, 10715.5986328125, 16556.759765625, 9811.5205078125, 13220.2451171875, 14683.076171875, 10410.123046875, 11967.0146484375, 477.510009765625, 185.8601837158203, 58.03466033935547, 80.3421859741211, 157.4603271484375, 64.56411743164062, 75.4340591430664, 55.1428337097168, 28.92046546936035, 109.41722106933594, 190.76153564453125, 37.697200775146484, 24.13730239868164, 21.23109245300293, 38.612152099609375, 40.594566345214844, 17.2749080657959, 21.158449172973633, 16.182125091552734, 16.351726531982422, 67.7081069946289, 99.6544189453125, 19.297863006591797, 49.15367126464844, 52.15303421020508, 15.414816856384277, 50.993690490722656, 14.438533782958984, 15.424382209777832, 15.496147155761719, 510.7383728027344, 29.862890243530273, 22.8465518951416, 35.38184356689453, 388.939208984375, 77.43182373046875, 667.2265625, 222.05612182617188, 196.730224609375, 113.4228515625, 62.67764663696289, 58.96653747558594, 17458.208984375, 127.12342834472656, 327.8515319824219, 426.54132080078125, 1282.0155029296875, 9439.1484375, 193.2530059814453, 4592.12451171875, 4110.943359375, 854.9950561523438, 185.6369171142578, 4170.14111328125, 589.800048828125, 289.7357177734375, 16562.544921875, 9811.5205078125, 544.5151977539062, 3452.646484375, 3641.860595703125, 1287.6239013671875, 507.73358154296875, 13626.283203125, 10473.18359375, 3136.341064453125, 18784.90234375, 4901.18017578125, 17172.751953125, 26166.3671875, 2890.279541015625, 5711.01025390625, 4579.173828125, 2580.5576171875, 8345.2626953125, 5729.064453125, 14683.076171875, 5334.9443359375, 6176.6953125, 8816.8359375, 10410.123046875, 13630.3505859375, 8341.2158203125, 10715.5986328125, 10036.0126953125, 17578.20703125, 9277.267578125, 298.53619384765625, 153.64767456054688, 75.80423736572266, 68.87071990966797, 94.4021987915039, 90.45668029785156, 348.7835693359375, 33.44709396362305, 57.09272003173828, 49.221500396728516, 55.127410888671875, 37.393028259277344, 45.2895393371582, 390.2731018066406, 47.24518966674805, 27.52058219909668, 28.49981689453125, 40.27082824707031, 61.025726318359375, 238.31317138671875, 43.994606018066406, 79.5497055053711, 32.37555694580078, 24.547510147094727, 23.5683650970459, 47.11933517456055, 41.19187927246094, 19.646255493164062, 29.48218536376953, 65.77738952636719, 281.4608459472656, 432.7963562011719, 108.27593231201172, 115.98051452636719, 136.9945526123047, 1318.986328125, 1138.93359375, 104.35539245605469, 218.25509643554688, 87.41272735595703, 82.59632873535156, 93.55217742919922, 191.13710021972656, 1823.0361328125, 575.0656127929688, 3350.65185546875, 1142.3739013671875, 100.41394805908203, 1944.794677734375, 164.0096893310547, 320.88909912109375, 2204.54443359375, 154.765625, 1307.193603515625, 12951.541015625, 1772.87353515625, 172.047119140625, 619.8096313476562, 1280.3433837890625, 284.0588684082031, 3181.8720703125, 4661.87890625, 1677.28125, 4579.173828125, 13220.2451171875, 3241.780517578125, 18784.90234375, 4575.82470703125, 6356.087890625, 6535.81982421875, 11275.712890625, 10773.0712890625, 10473.18359375, 11314.3388671875, 8029.1767578125, 4819.10009765625, 5877.205078125, 14407.6318359375, 6644.2568359375, 4432.68994140625, 14761.2314453125, 10410.123046875, 12986.92578125, 14683.076171875, 6812.306640625, 10715.5986328125, 7327.18798828125, 15868.1796875, 6038.73291015625], "Category": ["Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10"], "logprob": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -8.550399780273438, -8.416299819946289, -8.518699645996094, -8.91759967803955, -7.9629998207092285, -9.360899925231934, -7.928599834442139, -9.136899948120117, -9.170299530029297, -8.859100341796875, -8.95359992980957, -8.19950008392334, -9.762900352478027, -5.895999908447266, -9.668800354003906, -9.305500030517578, -9.44159984588623, -9.307700157165527, -9.618599891662598, -9.166999816894531, -10.099200248718262, -8.928199768066406, -9.067299842834473, -8.733599662780762, -9.923800468444824, -9.354299545288086, -9.857600212097168, -9.381699562072754, -9.542900085449219, -10.104999542236328, -7.299200057983398, -8.357399940490723, -6.194399833679199, -7.842100143432617, -8.111599922180176, -8.591400146484375, -4.333199977874756, -6.311699867248535, -6.853799819946289, -6.864699840545654, -6.984499931335449, -6.32390022277832, -6.8678998947143555, -5.219099998474121, -5.943399906158447, -7.126999855041504, -3.555000066757202, -7.943399906158447, -5.61269998550415, -5.398200035095215, -6.023600101470947, -6.272500038146973, -5.954599857330322, -7.024499893188477, -6.564799785614014, -4.827499866485596, -6.593200206756592, -5.257999897003174, -4.326300144195557, -4.737199783325195, -5.906799793243408, -6.832799911499023, -5.893400192260742, -4.9070000648498535, -6.519199848175049, -6.542600154876709, -5.04580020904541, -5.871099948883057, -5.773799896240234, -5.95419979095459, -5.362400054931641, -4.809599876403809, -5.2032999992370605, -5.6757001876831055, -5.868199825286865, -4.991799831390381, -5.604400157928467, -5.586999893188477, -5.67080020904541, -5.665900230407715, -5.471700191497803, -5.609600067138672, -5.78879976272583, -5.655399799346924, -5.816100120544434, -5.7916998863220215, -5.836100101470947, -8.788999557495117, -8.619500160217285, -9.736499786376953, -8.864999771118164, -9.697099685668945, -9.552200317382812, -9.237299919128418, -9.80519962310791, -9.315299987792969, -9.915599822998047, -8.481900215148926, -9.948699951171875, -8.784099578857422, -5.718800067901611, -7.873700141906738, -10.001199722290039, -8.583700180053711, -10.350299835205078, -9.616299629211426, -10.00790023803711, -9.928999900817871, -10.178999900817871, -9.469400405883789, -10.430800437927246, -10.315199851989746, -8.45930004119873, -9.417200088500977, -10.391900062561035, -10.038800239562988, -9.481599807739258, -9.001999855041504, -9.220399856567383, -9.557900428771973, -7.014500141143799, -8.01990032196045, -8.321700096130371, -8.766300201416016, -9.028800010681152, -8.930800437927246, -9.393199920654297, -5.9355998039245605, -5.724100112915039, -5.433199882507324, -8.03950023651123, -6.840400218963623, -6.962900161743164, -7.910399913787842, -5.383900165557861, -8.023599624633789, -9.019200325012207, -6.430799961090088, -4.621799945831299, -5.052299976348877, -6.762400150299072, -5.646900177001953, -5.082799911499023, -6.684800148010254, -5.304800033569336, -7.420499801635742, -4.809299945831299, -5.111000061035156, -6.4984002113342285, -5.157400131225586, -5.5269999504089355, -7.144400119781494, -5.825900077819824, -6.123499870300293, -6.808499813079834, -5.059000015258789, -5.581600189208984, -6.439000129699707, -4.76200008392334, -4.903600215911865, -5.275000095367432, -5.054100036621094, -5.0233001708984375, -4.569900035858154, -5.30810022354126, -5.880499839782715, -5.348800182342529, -5.571599960327148, -5.475500106811523, -5.7530999183654785, -5.433800220489502, -5.500100135803223, -5.5671000480651855, -5.75600004196167, -8.245400428771973, -9.253600120544434, -8.910200119018555, -8.600299835205078, -8.581700325012207, -9.243000030517578, -8.33899974822998, -9.124199867248535, -9.624799728393555, -9.703300476074219, -9.233099937438965, -9.412699699401855, -9.869799613952637, -8.20460033416748, -8.90880012512207, -9.560799598693848, -9.9621000289917, -7.9730000495910645, -9.43340015411377, -9.027400016784668, -9.986800193786621, -8.740400314331055, -9.987199783325195, -10.041299819946289, -9.69260025024414, -9.54740047454834, -9.888500213623047, -10.071000099182129, -9.775699615478516, -10.07450008392334, -8.00979995727539, -7.8881001472473145, -9.756999969482422, -7.997399806976318, -7.515200138092041, -7.808599948883057, -9.21720027923584, -6.732100009918213, -8.319899559020996, -9.046099662780762, -8.055399894714355, -5.985000133514404, -8.603699684143066, -7.6828999519348145, -8.024999618530273, -5.654600143432617, -4.39109992980957, -8.049500465393066, -5.720399856567383, -4.948400020599365, -6.190100193023682, -5.97160005569458, -6.105400085449219, -6.833099842071533, -6.225599765777588, -7.214000225067139, -5.741199970245361, -7.08489990234375, -4.810100078582764, -7.2754998207092285, -6.819300174713135, -5.970200061798096, -5.333899974822998, -5.559700012207031, -5.059999942779541, -5.898499965667725, -7.232999801635742, -6.398900032043457, -5.6417999267578125, -5.6493000984191895, -5.584199905395508, -5.939799785614014, -5.6092000007629395, -6.658899784088135, -6.414599895477295, -5.799600124359131, -5.565499782562256, -6.434500217437744, -5.968400001525879, -6.27839994430542, -5.846499919891357, -5.9217000007629395, -6.001500129699707, -5.956200122833252, -6.184599876403809, -6.003799915313721, -6.141900062561035, -6.147200107574463, -6.144100189208984, -8.802900314331055, -8.423800468444824, -9.240799903869629, -9.013999938964844, -8.680899620056152, -7.8059000968933105, -9.299300193786621, -7.806000232696533, -7.114299774169922, -7.80709981918335, -7.807300090789795, -7.807400226593018, -7.807600021362305, -7.807799816131592, -9.561300277709961, -9.659199714660645, -8.975600242614746, -7.805600166320801, -8.905500411987305, -9.440600395202637, -9.583000183105469, -9.276599884033203, -9.173299789428711, -8.479700088500977, -8.845800399780273, -9.587699890136719, -9.82129955291748, -9.0693998336792, -9.710000038146973, -9.86970043182373, -7.8007001876831055, -9.4173002243042, -6.772200107574463, -8.5625, -8.268500328063965, -8.509400367736816, -8.493000030517578, -6.89900016784668, -8.507399559020996, -8.515399932861328, -6.437900066375732, -7.736100196838379, -8.497300148010254, -8.657400131225586, -6.618299961090088, -6.672900199890137, -4.954500198364258, -6.461599826812744, -6.724299907684326, -7.060699939727783, -5.436299800872803, -5.729100227355957, -5.670599937438965, -6.680500030517578, -5.834199905395508, -6.415299892425537, -5.442800045013428, -6.5482001304626465, -4.907800197601318, -5.140600204467773, -5.732900142669678, -5.079800128936768, -5.77400016784668, -5.087600231170654, -6.094600200653076, -5.811399936676025, -5.864200115203857, -6.6946001052856445, -6.072400093078613, -4.923099994659424, -5.757199764251709, -5.626800060272217, -5.9517998695373535, -5.338799953460693, -6.044600009918213, -6.083700180053711, -5.99459981918335, -5.9816999435424805, -5.71150016784668, -5.885300159454346, -5.893499851226807, -5.878499984741211, -5.878799915313721, -6.023099899291992, -6.044600009918213, -8.471799850463867, -8.041500091552734, -9.101699829101562, -8.755599975585938, -9.077899932861328, -8.520500183105469, -8.95989990234375, -8.663100242614746, -8.608200073242188, -9.482099533081055, -9.433899879455566, -9.235799789428711, -9.152199745178223, -9.284600257873535, -9.016200065612793, -9.468899726867676, -9.287300109863281, -9.406599998474121, -9.548800468444824, -9.288200378417969, -9.550299644470215, -8.082799911499023, -9.802599906921387, -10.033300399780273, -9.911100387573242, -9.830699920654297, -9.44890022277832, -9.975500106811523, -10.103799819946289, -8.133399963378906, -7.376200199127197, -7.61299991607666, -8.861499786376953, -9.171600341796875, -9.347999572753906, -7.728300094604492, -8.161299705505371, -8.960100173950195, -6.389599800109863, -7.554500102996826, -8.65779972076416, -8.373700141906738, -6.292200088500977, -8.490699768066406, -8.13070011138916, -5.3105998039245605, -7.291200160980225, -8.496800422668457, -6.075099945068359, -6.695899963378906, -4.173900127410889, -7.739500045776367, -6.769599914550781, -7.051400184631348, -7.029600143432617, -5.9984002113342285, -6.670899868011475, -7.978099822998047, -6.184000015258789, -6.224800109863281, -7.6072998046875, -5.110000133514404, -5.706299781799316, -5.098299980163574, -5.219200134277344, -5.75600004196167, -7.008600234985352, -5.929699897766113, -4.116799831390381, -5.260900020599365, -4.565000057220459, -5.96120023727417, -5.747000217437744, -4.970699787139893, -6.137199878692627, -5.13539981842041, -5.507699966430664, -5.0406999588012695, -5.686100006103516, -5.3028998374938965, -5.132999897003174, -5.470900058746338, -5.509799957275391, -5.7291998863220215, -5.631999969482422, -5.500800132751465, -5.558899879455566, -5.772900104522705, -5.815199851989746, -5.776599884033203, -5.7916998863220215, -5.90369987487793, -5.889500141143799, -5.856599807739258, -5.887899875640869, -7.627299785614014, -8.927900314331055, -9.313899993896484, -9.38539981842041, -8.327400207519531, -8.947799682617188, -8.912199974060059, -9.286299705505371, -9.222900390625, -9.147899627685547, -9.39229965209961, -9.013299942016602, -9.179300308227539, -9.80150032043457, -8.06309986114502, -9.673999786376953, -7.966000080108643, -9.298600196838379, -9.784000396728516, -9.682700157165527, -10.138099670410156, -8.864100456237793, -9.965299606323242, -8.65369987487793, -9.471699714660645, -10.032299995422363, -10.068099975585938, -9.136099815368652, -8.717100143432617, -9.617799758911133, -8.474300384521484, -8.936300277709961, -9.046299934387207, -8.98799991607666, -5.807199954986572, -7.100299835205078, -4.659200191497803, -8.96780014038086, -8.1427001953125, -7.5725998878479, -6.972700119018555, -4.7459001541137695, -7.88730001449585, -7.672299861907959, -6.871600151062012, -8.220100402832031, -8.376700401306152, -6.042799949645996, -7.817299842834473, -7.2895002365112305, -5.9369001388549805, -8.35260009765625, -5.472599983215332, -5.408199787139893, -7.166900157928467, -5.474299907684326, -8.064800262451172, -7.841300010681152, -4.895999908447266, -5.198800086975098, -5.566500186920166, -5.791299819946289, -6.6975998878479, -5.214099884033203, -5.68149995803833, -6.760000228881836, -6.195899963378906, -6.126800060272217, -5.528900146484375, -5.779200077056885, -6.119100093841553, -5.586400032043457, -5.246699810028076, -5.196000099182129, -5.677299976348877, -5.741700172424316, -5.517000198364258, -5.80079984664917, -5.502999782562256, -5.771999835968018, -5.814499855041504, -5.5447998046875, -5.4019999504089355, -5.708499908447266, -5.68209981918335, -5.711599826812744, -5.796800136566162, -7.671599864959717, -8.833100318908691, -9.897500038146973, -8.301300048828125, -7.389400005340576, -10.088299751281738, -8.74120044708252, -8.084400177001953, -9.942099571228027, -9.768799781799316, -8.36929988861084, -10.275500297546387, -10.322699546813965, -8.249899864196777, -10.393799781799316, -10.433899879455566, -9.613300323486328, -10.033300399780273, -7.527500152587891, -9.401700019836426, -10.639200210571289, -10.046199798583984, -9.424300193786621, -9.700599670410156, -10.04319953918457, -8.755800247192383, -10.41100025177002, -10.574700355529785, -10.403599739074707, -10.786499977111816, -10.095499992370605, -8.820199966430664, -9.212300300598145, -5.9045000076293945, -8.364399909973145, -8.05620002746582, -8.06309986114502, -8.299400329589844, -8.160400390625, -6.5690999031066895, -8.686699867248535, -8.708600044250488, -6.260900020599365, -6.491099834442139, -7.529799938201904, -6.065199851989746, -5.023900032043457, -4.5894999504089355, -7.332099914550781, -8.440199851989746, -7.468599796295166, -7.0792999267578125, -4.184700012207031, -6.581699848175049, -5.945700168609619, -6.156300067901611, -6.035600185394287, -7.126800060272217, -4.84060001373291, -5.613699913024902, -5.199699878692627, -5.692399978637695, -5.187600135803223, -5.119699954986572, -5.394599914550781, -5.678199768066406, -5.43209981918335, -6.163099765777588, -5.740099906921387, -5.917099952697754, -6.218999862670898, -6.526299953460693, -5.75600004196167, -5.993299961090088, -5.498700141906738, -5.880899906158447, -6.023600101470947, -5.7241997718811035, -5.872900009155273, -5.656499862670898, -5.498600006103516, -5.942699909210205, -5.692399978637695, -5.926300048828125, -5.611199855804443, -5.573500156402588, -5.854899883270264, -5.804299831390381, -5.858099937438965, -5.887800216674805, -5.905799865722656, -8.07229995727539, -8.769000053405762, -8.961000442504883, -8.771499633789062, -8.989899635314941, -8.96399974822998, -9.219200134277344, -9.203399658203125, -9.561200141906738, -9.516599655151367, -8.770999908447266, -9.49470043182373, -8.946999549865723, -8.906900405883789, -9.518600463867188, -9.09749984741211, -9.569000244140625, -9.395400047302246, -8.588299751281738, -9.725299835205078, -9.875499725341797, -9.941699981689453, -9.943300247192383, -9.979299545288086, -8.638299942016602, -9.979900360107422, -9.980600357055664, -9.850000381469727, -10.055500030517578, -10.09570026397705, -8.204500198364258, -8.731200218200684, -8.922200202941895, -9.371000289916992, -8.271699905395508, -9.081500053405762, -8.257499694824219, -8.518400192260742, -5.254000186920166, -7.262499809265137, -8.027999877929688, -6.362800121307373, -6.741199970245361, -6.290200233459473, -7.478400230407715, -6.893099784851074, -7.620200157165527, -7.629000186920166, -6.1479997634887695, -5.2631001472473145, -7.8780999183654785, -7.756999969482422, -4.943699836730957, -7.179200172424316, -5.165599822998047, -7.677299976348877, -8.277400016784668, -5.110099792480469, -6.421800136566162, -4.634900093078613, -5.972400188446045, -6.453700065612793, -5.5609002113342285, -4.928100109100342, -6.295499801635742, -5.503600120544434, -6.332200050354004, -5.661200046539307, -5.69920015335083, -6.200799942016602, -6.30019998550415, -5.908599853515625, -5.635300159454346, -5.658999919891357, -5.468599796295166, -5.669600009918213, -5.909299850463867, -5.3531999588012695, -5.572000026702881, -5.773200035095215, -5.457300186157227, -5.131199836730957, -5.680099964141846, -5.656099796295166, -5.923699855804443, -5.690499782562256, -5.788400173187256, -5.6768999099731445, -5.827199935913086, -5.7565999031066895, -5.736100196838379, -5.850500106811523, -5.876299858093262, -7.114200115203857, -8.059000015258789, -9.228899955749512, -8.904600143432617, -8.236599922180176, -9.130900382995605, -8.978899955749512, -9.292799949645996, -9.943400382995605, -8.617400169372559, -8.062700271606445, -9.684100151062012, -10.134200096130371, -10.26830005645752, -9.673399925231934, -9.624500274658203, -10.479299545288086, -10.279600143432617, -10.550399780273438, -10.541500091552734, -9.122200012207031, -8.735699653625488, -10.3774995803833, -9.444000244140625, -9.385899543762207, -10.60509967803955, -9.409500122070312, -10.672599792480469, -10.60669994354248, -10.602800369262695, -7.1097002029418945, -9.949399948120117, -10.215399742126465, -9.782400131225586, -7.4243998527526855, -9.013400077819824, -6.91379976272583, -7.9928998947143555, -8.111499786376953, -8.646499633789062, -9.223899841308594, -9.28339958190918, -3.8041000366210938, -8.549300193786621, -7.6631999015808105, -7.439300060272217, -6.457600116729736, -4.661399841308594, -8.212900161743164, -5.365200042724609, -5.482699871063232, -6.881499767303467, -8.260499954223633, -5.602200031280518, -7.281000137329102, -7.886099815368652, -4.49970006942749, -4.988699913024902, -7.372700214385986, -5.8678998947143555, -5.825300216674805, -6.733500003814697, -7.491799831390381, -4.997000217437744, -5.239099979400635, -6.149700164794922, -4.88670015335083, -5.857800006866455, -4.968699932098389, -4.73330020904541, -6.252999782562256, -5.813600063323975, -5.992199897766113, -6.368500232696533, -5.696000099182129, -5.9720001220703125, -5.44890022277832, -6.049699783325195, -6.052999973297119, -5.928800106048584, -5.865200042724609, -5.781400203704834, -5.986199855804443, -5.916500091552734, -5.9390997886657715, -5.966400146484375, -6.07919979095459, -7.47189998626709, -8.14229965209961, -8.850799560546875, -8.950699806213379, -8.635700225830078, -8.678999900817871, -7.3308000564575195, -9.678799629211426, -9.14490032196045, -9.293499946594238, -9.180899620056152, -9.570599555969238, -9.38070011138916, -7.227399826049805, -9.339699745178223, -9.880200386047363, -9.845800399780273, -9.500399589538574, -9.085000038146973, -7.725599765777588, -9.416600227355957, -8.82450008392334, -9.723699569702148, -10.000699996948242, -10.041899681091309, -9.349599838256836, -9.487799644470215, -10.231300354003906, -9.82759952545166, -9.026800155639648, -7.586400032043457, -7.163300037384033, -8.53950023651123, -8.476300239562988, -8.316699981689453, -6.131199836730957, -6.286200046539307, -8.58899974822998, -7.885300159454346, -8.76099967956543, -8.822099685668945, -8.706600189208984, -8.039299964904785, -5.950900077819824, -7.033199787139893, -5.427700042724609, -6.417099952697754, -8.6451997756958, -5.99399995803833, -8.20580005645752, -7.614699840545654, -5.986800193786621, -8.272600173950195, -6.481200218200684, -4.555300235748291, -6.25629997253418, -8.186200141906738, -7.153600215911865, -6.5904998779296875, -7.789000034332275, -5.91949987411499, -5.7606000900268555, -6.519800186157227, -5.7993998527526855, -5.084000110626221, -6.089000225067139, -4.910399913787842, -5.9019999504089355, -5.698699951171875, -5.693699836730957, -5.497300148010254, -5.542600154876709, -5.577199935913086, -5.571800231933594, -5.777100086212158, -6.078400135040283, -5.98859977722168, -5.598599910736084, -5.958899974822998, -6.140500068664551, -5.682700157165527, -5.896200180053711, -5.883900165557861, -5.891900062561035, -6.053500175476074, -5.994500160217285, -6.0467000007629395, -6.016499996185303, -6.07390022277832], "loglift": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 2.0539000034332275, 2.0522000789642334, 2.0501999855041504, 2.0488998889923096, 2.047300100326538, 2.0464000701904297, 2.0448999404907227, 2.0416998863220215, 2.0409998893737793, 2.0401999950408936, 2.039799928665161, 2.0394999980926514, 2.037600040435791, 2.0374999046325684, 2.037100076675415, 2.036400079727173, 2.03629994392395, 2.0346999168395996, 2.0339999198913574, 2.033900022506714, 2.0320000648498535, 2.0306999683380127, 2.030400037765503, 2.028599977493286, 2.0281999111175537, 2.0281999111175537, 2.0272998809814453, 2.0269999504089355, 2.026700019836426, 2.0250000953674316, 2.014699935913086, 2.0220999717712402, 1.999899983406067, 2.0092999935150146, 2.00819993019104, 2.015399932861328, 1.9199999570846558, 1.958799958229065, 1.964400053024292, 1.9605000019073486, 1.9616999626159668, 1.936400055885315, 1.955299973487854, 1.854699969291687, 1.8904000520706177, 1.9460999965667725, 1.6712000370025635, 1.9472999572753906, 1.7415000200271606, 1.7064000368118286, 1.746999979019165, 1.7719000577926636, 1.7308000326156616, 1.844599962234497, 1.7745000123977661, 1.5324000120162964, 1.7760000228881836, 1.572100043296814, 1.4160000085830688, 1.4559999704360962, 1.6385999917984009, 1.7944999933242798, 1.566100001335144, 1.3178999423980713, 1.708799958229065, 1.7102999687194824, 1.1996999979019165, 1.4105000495910645, 1.3401000499725342, 1.4265999794006348, 1.0098999738693237, 0.6319000124931335, 0.7907999753952026, 1.123900055885315, 1.2405999898910522, 0.5626000165939331, 0.90829998254776, 0.8476999998092651, 0.9211000204086304, 0.8952000141143799, 0.5403000116348267, 0.7831000089645386, 1.0024000406265259, 0.4424000084400177, 0.866599977016449, 0.590399980545044, 0.8996999859809875, 2.0871999263763428, 2.079699993133545, 2.078200101852417, 2.07669997215271, 2.0759999752044678, 2.07450008392334, 2.0727999210357666, 2.0713999271392822, 2.0713000297546387, 2.0687999725341797, 2.0687999725341797, 2.0678999423980713, 2.067699909210205, 2.067500114440918, 2.067500114440918, 2.0671000480651855, 2.066200017929077, 2.063999891281128, 2.0638999938964844, 2.0634000301361084, 2.0631000995635986, 2.062000036239624, 2.061800003051758, 2.0611000061035156, 2.059799909591675, 2.059299945831299, 2.059000015258789, 2.0587000846862793, 2.0573999881744385, 2.0553998947143555, 2.0520999431610107, 2.0532000064849854, 2.054500102996826, 2.0253000259399414, 2.033900022506714, 2.03410005569458, 2.0353000164031982, 2.0432000160217285, 2.036900043487549, 2.049799919128418, 1.9258999824523926, 1.9007999897003174, 1.8782999515533447, 1.9847999811172485, 1.9138000011444092, 1.913699984550476, 1.968400001525879, 1.7747000455856323, 1.9392000436782837, 2.025700092315674, 1.7910000085830688, 1.5973999500274658, 1.6238000392913818, 1.7977999448776245, 1.652899980545044, 1.5805000066757202, 1.761299967765808, 1.5716999769210815, 1.8473000526428223, 1.4153000116348267, 1.4377000331878662, 1.6722999811172485, 1.4143999814987183, 1.4566999673843384, 1.7785999774932861, 1.496399998664856, 1.559399962425232, 1.7024999856948853, 1.2980999946594238, 1.4083000421524048, 1.5699000358581543, 1.0670000314712524, 1.0485999584197998, 1.1547000408172607, 1.072700023651123, 1.0572999715805054, 0.8715999722480774, 1.059499979019165, 1.347599983215332, 1.0332000255584717, 1.1208000183105469, 0.9573000073432922, 1.0225000381469727, 0.5782999992370605, 0.5976999998092651, 0.3416999876499176, 0.2985000014305115, 2.114799976348877, 2.1077001094818115, 2.1073999404907227, 2.1064999103546143, 2.1054999828338623, 2.1052000522613525, 2.10509991645813, 2.1050000190734863, 2.104599952697754, 2.103600025177002, 2.1026999950408936, 2.102299928665161, 2.1019999980926514, 2.1006999015808105, 2.099900007247925, 2.0973000526428223, 2.0966999530792236, 2.095900058746338, 2.095400094985962, 2.0952999591827393, 2.0950000286102295, 2.094899892807007, 2.0947999954223633, 2.0947999954223633, 2.0943000316619873, 2.0934998989105225, 2.093400001525879, 2.093400001525879, 2.0933001041412354, 2.0931999683380127, 2.087599992752075, 2.076900005340576, 2.0922000408172607, 2.072700023651123, 2.0608999729156494, 2.0560998916625977, 2.0834999084472656, 2.030600070953369, 2.058000087738037, 2.0750999450683594, 2.042799949645996, 1.9493000507354736, 2.043600082397461, 1.9759000539779663, 1.972599983215332, 1.7518999576568604, 1.5844000577926636, 1.960800051689148, 1.6829999685287476, 1.5533000230789185, 1.655400037765503, 1.6087000370025635, 1.6237000226974487, 1.7403000593185425, 1.6233999729156494, 1.781599998474121, 1.4818999767303467, 1.7568999528884888, 1.2759000062942505, 1.7740000486373901, 1.6247999668121338, 1.3658000230789185, 1.0901000499725342, 1.1114000082015991, 0.8258000016212463, 1.1649999618530273, 1.7091000080108643, 1.3497999906539917, 1.0003999471664429, 0.9614999890327454, 0.9283999800682068, 1.0870000123977661, 0.8847000002861023, 1.4250999689102173, 1.287600040435791, 0.897599995136261, 0.6894000172615051, 1.2583999633789062, 0.8223000168800354, 1.0858999490737915, 0.5881999731063843, 0.6696000099182129, 0.7415000200271606, 0.2809000015258789, 0.8040000200271606, 0.14730000495910645, 0.21529999375343323, 0.057100001722574234, -0.13249999284744263, 2.245300054550171, 2.242799997329712, 2.239799976348877, 2.2388999462127686, 2.238600015640259, 2.237299919128418, 2.237299919128418, 2.2370998859405518, 2.236999988555908, 2.2358999252319336, 2.235599994659424, 2.2355000972747803, 2.235300064086914, 2.235100030899048, 2.234800100326538, 2.234600067138672, 2.234600067138672, 2.2342000007629395, 2.2339999675750732, 2.233099937438965, 2.2309999465942383, 2.230600118637085, 2.2300000190734863, 2.228800058364868, 2.2281999588012695, 2.2281999588012695, 2.227799892425537, 2.226799964904785, 2.2267000675201416, 2.2263998985290527, 2.21560001373291, 2.223900079727173, 2.178299903869629, 2.210400104522705, 2.1963000297546387, 2.203700065612793, 2.200900077819824, 2.142199993133545, 2.1989998817443848, 2.1970999240875244, 2.0782999992370605, 2.1486001014709473, 2.190200090408325, 2.197000026702881, 2.0297000408172607, 1.9914000034332275, 1.7956000566482544, 1.9601999521255493, 1.9859999418258667, 2.0102999210357666, 1.6749000549316406, 1.6957000494003296, 1.6511000394821167, 1.8238999843597412, 1.6371999979019165, 1.7529000043869019, 1.5321999788284302, 1.718500018119812, 1.243299961090088, 1.2655999660491943, 1.4026999473571777, 1.1267000436782837, 1.4077999591827393, 1.1191999912261963, 1.524899959564209, 1.3573999404907227, 1.3112000226974487, 1.705299973487854, 1.395900011062622, 0.819100022315979, 1.1936999559402466, 1.11489999294281, 1.2776000499725342, 0.5699999928474426, 1.2337000370025635, 1.2656999826431274, 1.1629999876022339, 0.8205000162124634, 0.2827000021934509, 0.5907999873161316, 0.4140999913215637, 0.367000013589859, 0.08020000159740448, -0.13729999959468842, 0.3515999913215637, 2.3310000896453857, 2.3252999782562256, 2.324899911880493, 2.3231000900268555, 2.3229000568389893, 2.3220999240875244, 2.321500062942505, 2.319700002670288, 2.3194000720977783, 2.319000005722046, 2.315200090408325, 2.3148000240325928, 2.3136000633239746, 2.3125998973846436, 2.3125, 2.312299966812134, 2.3101000785827637, 2.3101000785827637, 2.309499979019165, 2.3089001178741455, 2.3085999488830566, 2.3071000576019287, 2.3057000637054443, 2.3048999309539795, 2.304500102996826, 2.303100109100342, 2.302799940109253, 2.3015999794006348, 2.300600051879883, 2.2992000579833984, 2.2934000492095947, 2.283799886703491, 2.2952001094818115, 2.2969000339508057, 2.297499895095825, 2.2523999214172363, 2.2639999389648438, 2.2878000736236572, 2.2021000385284424, 2.236599922180176, 2.2757999897003174, 2.2637999057769775, 2.1745998859405518, 2.2681000232696533, 2.2434000968933105, 2.0629000663757324, 2.1884000301361084, 2.2607998847961426, 2.08489990234375, 2.108599901199341, 1.8811999559402466, 2.18969988822937, 2.0759999752044678, 2.1064999103546143, 2.097399950027466, 1.9735000133514404, 2.0487000942230225, 2.1993000507354736, 1.9459999799728394, 1.9417999982833862, 2.1449999809265137, 1.7060999870300293, 1.8029999732971191, 1.6871000528335571, 1.6778000593185425, 1.7592999935150146, 2.0143001079559326, 1.764799952507019, 1.3246999979019165, 1.4745999574661255, 1.2640999555587769, 1.6748000383377075, 1.4938000440597534, 1.1270999908447266, 1.5858999490737915, 0.9915000200271606, 1.1756999492645264, 0.9114999771118164, 1.264799952507019, 0.9162999987602234, 0.7757999897003174, 0.9110999703407288, 0.896399974822998, 1.0125999450683594, 0.7355999946594238, 0.5112000107765198, 0.5217000246047974, 0.8938999772071838, 0.756600022315979, 0.5601000189781189, 0.43290001153945923, 1.0149999856948853, 0.8029000163078308, 0.19789999723434448, 0.5554999709129333, 2.366300106048584, 2.3615000247955322, 2.3570001125335693, 2.3568999767303467, 2.3543999195098877, 2.3529999256134033, 2.352400064468384, 2.3515000343322754, 2.3501999378204346, 2.350100040435791, 2.350100040435791, 2.3489999771118164, 2.3475000858306885, 2.34689998626709, 2.345599889755249, 2.345099925994873, 2.339400053024292, 2.339099884033203, 2.3376998901367188, 2.336899995803833, 2.336699962615967, 2.3362998962402344, 2.3336000442504883, 2.3334999084472656, 2.3329999446868896, 2.3329999446868896, 2.3322999477386475, 2.3322999477386475, 2.331899881362915, 2.330699920654297, 2.3285999298095703, 2.330199956893921, 2.329400062561035, 2.3257999420166016, 2.2771999835968018, 2.2785000801086426, 2.1949000358581543, 2.320499897003174, 2.289400100708008, 2.2302000522613525, 2.187299966812134, 1.9838000535964966, 2.236299991607666, 2.2149999141693115, 2.100800037384033, 2.2409000396728516, 2.2578999996185303, 1.986299991607666, 2.190200090408325, 2.116499900817871, 1.9458999633789062, 2.239300012588501, 1.7998000383377075, 1.7826000452041626, 2.0525999069213867, 1.6943000555038452, 2.1849000453948975, 2.1410999298095703, 1.5159000158309937, 1.5509999990463257, 1.6319999694824219, 1.669600009918213, 1.8686000108718872, 1.4896999597549438, 1.5318000316619873, 1.8258999586105347, 1.5887999534606934, 1.5622999668121338, 1.319599986076355, 1.2967000007629395, 1.4428999423980713, 1.1490999460220337, 0.9575999975204468, 0.859000027179718, 1.1059999465942383, 1.13100004196167, 0.9247999787330627, 1.0750999450683594, 0.7340999841690063, 0.9818999767303467, 0.9761000275611877, 0.5874000191688538, -0.17589999735355377, 0.4982999861240387, 0.2700999975204468, 0.1972000002861023, 0.21490000188350677, 2.3752999305725098, 2.3726000785827637, 2.3615000247955322, 2.3601999282836914, 2.358099937438965, 2.355299949645996, 2.3538999557495117, 2.3533999919891357, 2.352299928665161, 2.3505001068115234, 2.3489999771118164, 2.3443000316619873, 2.3440001010894775, 2.3427000045776367, 2.3403000831604004, 2.339600086212158, 2.335400104522705, 2.3340001106262207, 2.3334999084472656, 2.3331000804901123, 2.328900098800659, 2.328399896621704, 2.3278000354766846, 2.326900005340576, 2.3264999389648438, 2.323199987411499, 2.3229000568389893, 2.3224000930786133, 2.320499897003174, 2.319499969482422, 2.3180999755859375, 2.3020999431610107, 2.2964000701904297, 2.2070000171661377, 2.2783000469207764, 2.2534000873565674, 2.2460999488830566, 2.2504000663757324, 2.2411000728607178, 2.1212000846862793, 2.2499001026153564, 2.2339999675750732, 2.0111000537872314, 2.009200096130371, 2.1108999252319336, 1.9164999723434448, 1.7762000560760498, 1.7180999517440796, 2.016200065612793, 2.1744000911712646, 2.003999948501587, 1.926200032234192, 1.3696000576019287, 1.8208999633789062, 1.6563999652862549, 1.6584999561309814, 1.6243000030517578, 1.8964999914169312, 1.2454999685287476, 1.4071999788284302, 1.2350000143051147, 1.3961999416351318, 1.2086000442504883, 1.1378999948501587, 1.232699990272522, 1.3423999547958374, 1.2390999794006348, 1.4782999753952026, 1.2962000370025635, 1.319700002670288, 1.4625999927520752, 1.6038999557495117, 1.1856000423431396, 1.2753000259399414, 0.9544000029563904, 1.1723999977111816, 1.274399995803833, 0.9391000270843506, 1.082200050354004, 0.8374999761581421, 0.41029998660087585, 1.1337000131607056, 0.6751000285148621, 1.100100040435791, 0.4009000062942505, 0.2556000053882599, 0.8805999755859375, 0.4505999982357025, 0.5891000032424927, 0.4489000141620636, 0.14869999885559082, 2.4363999366760254, 2.4333999156951904, 2.4326000213623047, 2.4309000968933105, 2.430500030517578, 2.430000066757202, 2.427999973297119, 2.4272000789642334, 2.426800012588501, 2.424499988555908, 2.4242000579833984, 2.4240000247955322, 2.4235999584198, 2.4230000972747803, 2.422800064086914, 2.422600030899048, 2.4189000129699707, 2.418100118637085, 2.418100118637085, 2.41729998588562, 2.4172000885009766, 2.4168999195098877, 2.4165000915527344, 2.414799928665161, 2.4147000312805176, 2.414599895477295, 2.413599967956543, 2.4130001068115234, 2.412899971008301, 2.412400007247925, 2.410099983215332, 2.4096999168395996, 2.4089999198913574, 2.40939998626709, 2.3975000381469727, 2.395699977874756, 2.363100051879883, 2.3715999126434326, 2.1988000869750977, 2.3032000064849854, 2.3169000148773193, 2.1826000213623047, 2.1974000930786133, 2.1317999362945557, 2.243499994277954, 2.177500009536743, 2.245300054550171, 2.2397000789642334, 2.0141000747680664, 1.8708000183105469, 2.24429988861084, 2.2262001037597656, 1.7422000169754028, 2.1047000885009766, 1.7203999757766724, 2.1909000873565674, 2.285799980163574, 1.5875999927520752, 1.8732000589370728, 1.3593000173568726, 1.7120000123977661, 1.8245999813079834, 1.5772000551223755, 1.2761000394821167, 1.6759999990463257, 1.4010000228881836, 1.6845999956130981, 1.4119999408721924, 1.4093999862670898, 1.5738999843597412, 1.5791000127792358, 1.3986999988555908, 1.1047999858856201, 1.076799988746643, 0.9433000087738037, 1.0602999925613403, 1.1994999647140503, 0.7013000249862671, 0.86080002784729, 0.993399977684021, 0.5544000267982483, 0.0949999988079071, 0.7616999745368958, 0.7038000226020813, 0.9760000109672546, 0.515999972820282, 0.6588000059127808, 0.335099995136261, 0.7081000208854675, 0.4803999960422516, 0.3961000144481659, 0.6255999803543091, 0.4603999853134155, 2.4437999725341797, 2.4426000118255615, 2.4367001056671143, 2.435699939727783, 2.430799961090088, 2.428100109100342, 2.4244000911712646, 2.4238998889923096, 2.418600082397461, 2.4140000343322754, 2.412899971008301, 2.412899971008301, 2.408600091934204, 2.402899980545044, 2.399600028991699, 2.3984999656677246, 2.398099899291992, 2.39490008354187, 2.3922998905181885, 2.390700101852417, 2.38919997215271, 2.38919997215271, 2.3891000747680664, 2.3875999450683594, 2.3864998817443848, 2.386199951171875, 2.3854000568389893, 2.3840999603271484, 2.3838999271392822, 2.38319993019104, 2.38100004196167, 2.3805999755859375, 2.3824000358581543, 2.378000020980835, 2.3387999534606934, 2.363800048828125, 2.3097000122070312, 2.330699920654297, 2.3333001136779785, 2.3489999771118164, 2.3647000789642334, 2.3661999702453613, 2.154900074005127, 2.3320999145507812, 2.270900011062622, 2.2314999103546143, 2.112799882888794, 1.912600040435791, 2.2497000694274902, 1.9292999505996704, 1.9225000143051147, 2.0940001010894775, 2.2421998977661133, 1.788699984550476, 2.0657999515533447, 2.171600103378296, 1.5119999647140503, 1.5465999841690063, 2.053999900817871, 1.7117999792099, 1.7009999752044678, 1.8324999809265137, 2.0048999786376953, 1.2098000049591064, 1.2309000492095947, 1.5261000394821167, 0.9991000294685364, 1.3716000318527222, 1.0068000555038452, 0.8209999799728394, 1.5045000314712524, 1.2628999948501587, 1.3050999641418457, 1.5023000240325928, 1.001099944114685, 1.1013000011444092, 0.6833000183105469, 1.0947999954223633, 0.9451000094413757, 0.7134000062942505, 0.6108999848365784, 0.4250999987125397, 0.7114999890327454, 0.5307000279426575, 0.5734999775886536, -0.0142000000923872, 0.5120000243186951, 2.555799961090088, 2.5495998859405518, 2.5476999282836914, 2.543600082397461, 2.543299913406372, 2.5427000522613525, 2.5413999557495117, 2.537899971008301, 2.5369999408721924, 2.536799907684326, 2.536099910736084, 2.5344998836517334, 2.5327999591827393, 2.532399892807007, 2.531599998474121, 2.531399965286255, 2.530900001525879, 2.5304999351501465, 2.5302999019622803, 2.527400016784668, 2.5260000228881836, 2.525700092315674, 2.5255000591278076, 2.5253000259399414, 2.5248000621795654, 2.5243000984191895, 2.5206000804901123, 2.517400026321411, 2.515199899673462, 2.5134999752044678, 2.500200033187866, 2.493000030517578, 2.5023999214172363, 2.4969000816345215, 2.490000009536743, 2.410799980163574, 2.402600049972534, 2.489799976348877, 2.4556000232696533, 2.494999885559082, 2.490499973297119, 2.4814999103546143, 2.434299945831299, 2.2674999237060547, 2.338900089263916, 2.181999921798706, 2.2685999870300293, 2.472100019454956, 2.1596999168395996, 2.420799970626831, 2.3408000469207764, 2.0415000915527344, 2.412100076675415, 2.0697999000549316, 1.702299952507019, 1.98989999294281, 2.3926000595092773, 2.1435999870300293, 1.9811999797821045, 2.2883999347686768, 1.7418999671936035, 1.5188000202178955, 1.7819000482559204, 1.4979000091552734, 1.1531000137329102, 1.5536999702453613, 0.9753999710083008, 1.3961000442504883, 1.2706999778747559, 1.2479000091552734, 0.8988999724388123, 0.8992000222206116, 0.892799973487854, 0.8209999799728394, 0.9587000012397766, 1.167799949645996, 1.0591000318527222, 0.5525000095367432, 0.9661999940872192, 1.1892999410629272, 0.4442000091075897, 0.5799000263214111, 0.3709999918937683, 0.2402999997138977, 0.8465999960899353, 0.45260000228881836, 0.7804999947547913, 0.038100000470876694, 0.9466999769210815]}, "token.table": {"Topic": [3, 8, 1, 3, 4, 5, 6, 7, 8, 9, 10, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 1, 2, 3, 5, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 5, 6, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 9, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 2, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 3, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 5, 6, 7, 1, 5, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 5, 8, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 3, 4, 5, 6, 7, 8, 9, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 5, 1, 2, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 6, 1, 3, 4, 6, 7, 8, 9, 10, 4, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 9, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 2, 3, 3, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 6, 7, 6, 9, 10, 3, 4, 5, 6, 7, 8, 9, 10, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 2, 5, 6, 7, 8, 2, 3, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 6, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 1, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 10, 2, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 2, 3, 4, 5, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 5, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 8, 8, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 8, 10, 7, 9, 3, 5, 9, 10, 1, 6, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 6, 3, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 6, 9, 10, 2, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 4, 6, 8, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 4, 2, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 7, 9, 1, 4, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 5, 7, 8, 9, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 6, 10, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 8, 6, 8, 2, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 7, 9, 10, 1, 3, 4, 5, 6, 7, 8, 9, 10, 3, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 1, 2, 3, 4, 5, 6, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 5, 8, 3, 2, 4, 7, 8, 10, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 5, 9, 6, 4, 9, 4, 6, 8, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 3, 4, 5, 6, 7, 8, 9, 10, 5, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 6, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 7, 9, 10, 1, 4, 7, 8, 1, 2, 4, 7, 8, 1, 4, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 8, 2, 3, 8, 9, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 7, 3, 6, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 2, 3, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 1, 2, 3, 4, 6, 8, 10, 2, 3, 6, 7, 9, 10, 1, 2, 3, 4, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 6, 7, 8, 10, 1, 4, 6, 7, 9, 1, 4, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 5, 10, 2, 5, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 7, 8, 10, 3, 3, 4, 5, 7, 8, 9, 1, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 5, 6, 7, 9, 10, 2, 3, 4, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 7, 9, 10, 1, 2, 3, 4, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 5, 7, 1, 2, 3, 4, 5, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 7, 1, 2, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 7, 9, 7, 1, 3, 4, 5, 6, 7, 9, 1, 3, 4, 5, 7, 8, 9, 1, 4, 5, 6, 7, 8, 9, 10, 3, 2, 4, 5, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 5, 6, 9, 1, 2, 3, 4, 5, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 5, 6, 8, 9, 1, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 7, 8, 10, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 9, 7, 3, 4, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 9, 4, 8, 3, 1, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 9, 5, 9, 4, 10, 7, 1, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 7, 2, 6, 7, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 1, 2, 3, 4, 5, 6, 8, 9, 10, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 6, 7, 8, 10, 10, 1, 2, 3, 6, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 4, 2, 3, 4, 6, 8, 9, 10, 7, 2, 3, 4, 5, 6, 8, 10, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 6, 5, 2, 1, 6, 8, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 2, 2, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 1, 3, 1, 3, 9, 1, 2, 3, 4, 6, 8, 9, 10, 3, 8, 3, 8, 2, 5, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 9, 4, 7, 8, 10, 7, 5, 8, 1, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 10, 1, 2, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 5, 7, 8, 1, 3, 1, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 9, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 7, 1, 1, 3, 4, 5, 6, 7, 8, 9, 10, 3, 2, 6, 10, 2, 8, 10, 6, 8, 2, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 5, 6, 2, 10, 10, 2, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 8, 10, 2, 2, 6, 7, 8, 2, 6, 1, 2, 3, 5, 7, 8, 9, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 5, 7, 10, 1, 6, 8, 5, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 2, 3, 5, 6, 7, 8, 9, 10, 5, 3, 4, 6, 9, 6, 8, 6, 8, 2, 3, 5, 7, 8, 10, 2, 3, 5, 7, 8, 9, 6, 4, 4, 5, 7, 3, 4, 5, 7, 8, 9, 3, 4, 5, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 7, 10, 5, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 5, 9, 1, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 6, 8, 10, 1, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 5, 4, 6, 2, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 7, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 6, 9, 10, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 6, 5, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 9, 10, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 6, 7, 8, 9, 2, 4, 6, 7, 8, 10, 3, 9, 10, 3, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 6, 8, 9, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 9, 10, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 9, 3, 4, 6, 7, 5, 7, 9, 10, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 8, 10, 4, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 7, 8, 9, 10, 8, 9, 10, 5, 1, 2, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 5, 6, 7, 8, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 1, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 7, 8, 9, 10, 6, 3, 6, 8, 3, 8, 7, 9, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 10, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 3, 6, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 6, 7, 8, 9, 10, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 2, 2, 8, 8, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 5, 6, 9, 10, 1, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 6, 8, 2, 5, 8, 9, 10, 1], "Freq": [0.9712617993354797, 0.010013008490204811, 0.005806871689856052, 0.008710308000445366, 0.8942582607269287, 0.002903435844928026, 0.005806871689856052, 0.02613092213869095, 0.005806871689856052, 0.034841232001781464, 0.011613743379712105, 0.9726946949958801, 0.0034010305535048246, 0.0034010305535048246, 0.013604122214019299, 0.0034010305535048246, 0.9420146346092224, 0.08198252320289612, 0.06702686846256256, 0.35455843806266785, 0.07614617049694061, 0.028178641572594643, 0.10551031678915024, 0.09237852692604065, 0.051341667771339417, 0.08243848383426666, 0.06046096980571747, 0.12042880803346634, 0.01786411926150322, 0.2374695986509323, 0.0949416384100914, 0.032725218683481216, 0.07931052893400192, 0.14275896549224854, 0.06190841645002365, 0.10179468244314194, 0.1107267439365387, 0.03593713417649269, 0.0029138217214494944, 0.6412835717201233, 0.011898105032742023, 0.0016997293569147587, 0.016268838196992874, 0.03836531937122345, 0.010926831513643265, 0.21319462358951569, 0.027438487857580185, 0.7237787246704102, 0.15904176235198975, 0.01479458250105381, 0.02311653643846512, 0.009246613830327988, 0.04577074199914932, 0.005316803231835365, 0.013176425360143185, 0.00392981106415391, 0.0020804882515221834, 0.012121479026973248, 0.012121479026973248, 0.9454753398895264, 0.012121479026973248, 0.8992466926574707, 0.054324671626091, 0.001733766170218587, 0.005201298277825117, 0.01791558414697647, 0.011558441445231438, 0.006935064680874348, 0.0005779220373369753, 0.0011558440746739507, 0.0011558440746739507, 0.810466468334198, 0.015018340200185776, 0.00960475206375122, 0.038069743663072586, 0.01152570266276598, 0.056231457740068436, 0.028290361166000366, 0.023749932646751404, 0.0027941097505390644, 0.004365796223282814, 0.016189446672797203, 0.0017988273175433278, 0.1205214336514473, 0.007195309270173311, 0.0017988273175433278, 0.7698981165885925, 0.005396482069045305, 0.06295895576477051, 0.005396482069045305, 0.01079296413809061, 0.03163131698966026, 0.004148369655013084, 0.010370924137532711, 0.01918620988726616, 0.0015556386206299067, 0.18460243940353394, 0.002074184827506542, 0.6435158252716064, 0.08607866615056992, 0.016593478620052338, 0.9691764712333679, 0.004650384187698364, 0.9347272515296936, 0.004650384187698364, 0.037203073501586914, 0.004650384187698364, 0.004650384187698364, 0.004650384187698364, 0.007563209626823664, 0.000945401203352958, 0.9075852036476135, 0.000945401203352958, 0.000945401203352958, 0.021744228899478912, 0.004727005958557129, 0.009454011917114258, 0.028362037613987923, 0.017017222940921783, 0.005307105835527182, 0.0037907897494733334, 0.02729368768632412, 0.03411711007356644, 0.0015163159696385264, 0.0007581579848192632, 0.06899237632751465, 0.003032631939277053, 0.0037907897494733334, 0.8514114022254944, 0.025382520630955696, 0.05696965754032135, 0.05302126705646515, 0.035535529255867004, 0.02707468904554844, 0.07163511216640472, 0.039483923465013504, 0.09250518679618835, 0.038355808705091476, 0.5589795112609863, 0.9514870643615723, 0.0032809898257255554, 0.0016404949128627777, 0.004921484738588333, 0.032809898257255554, 0.0016404949128627777, 0.0032809898257255554, 0.05433391407132149, 0.5880678296089172, 0.020070575177669525, 0.04802601784467697, 0.10508379340171814, 0.03555358946323395, 0.0348367840051651, 0.038707535713911057, 0.009748565033078194, 0.06565944850444794, 0.006958683021366596, 0.027834732085466385, 0.9255048632621765, 0.027834732085466385, 0.0072577353566884995, 0.006804126780480146, 0.14016501605510712, 0.04762888699769974, 0.0077113439328968525, 0.0036288676783442497, 0.12020624428987503, 0.006350518204271793, 0.07212374359369278, 0.5883301496505737, 0.005422384478151798, 0.0014788320986554027, 0.04288613051176071, 0.03499902784824371, 0.005422384478151798, 0.007887104526162148, 0.8266671895980835, 0.012323601171374321, 0.03549197316169739, 0.027111923322081566, 0.9531200528144836, 0.9076538681983948, 0.9457199573516846, 0.9739540219306946, 0.047999098896980286, 0.06469443440437317, 0.0579119548201561, 0.6015539169311523, 0.030260302126407623, 0.043303534388542175, 0.06104233115911484, 0.033912405371665955, 0.018260527402162552, 0.041738346219062805, 0.04169135540723801, 0.041059669107198715, 0.018824279308319092, 0.6275180578231812, 0.033353082835674286, 0.022109052166342735, 0.13467571139335632, 0.0380275696516037, 0.022109052166342735, 0.02046666480600834, 0.9386619329452515, 0.0518597736954689, 0.02993781678378582, 0.00292076263576746, 0.020445337519049644, 0.6454885005950928, 0.07155868411064148, 0.04965296387672424, 0.1613721251487732, 0.009492478333413601, 0.005111334379762411, 0.00438114395365119, 0.0574054941534996, 0.0061232526786625385, 0.026789231225848198, 0.026789231225848198, 0.005102710798382759, 0.02755463868379593, 0.0709276795387268, 0.7740812301635742, 0.0030616263393312693, 0.0025513553991913795, 0.006368208676576614, 0.006368208676576614, 0.049353618174791336, 0.04298540949821472, 0.038209252059459686, 0.007960260845720768, 0.04616951197385788, 0.7052791118621826, 0.06368208676576614, 0.03502514958381653, 0.005418811924755573, 0.980804979801178, 0.005418811924755573, 0.004780506249517202, 0.01354476809501648, 0.02230902947485447, 0.0438213087618351, 0.8692554235458374, 0.01513827033340931, 0.011154514737427235, 0.003187004243955016, 0.011154514737427235, 0.005577257368713617, 0.9611344933509827, 0.015689825639128685, 0.6500817537307739, 0.007321918848901987, 0.0026149710174649954, 0.2599281072616577, 0.008890900760889053, 0.01098287757486105, 0.007321918848901987, 0.007321918848901987, 0.02928767539560795, 0.9693812131881714, 0.10804317891597748, 0.01157605554908514, 0.003858685027807951, 0.005144913215190172, 0.001286228303797543, 0.06431141495704651, 0.001286228303797543, 0.7575885057449341, 0.001286228303797543, 0.048876676708459854, 0.02535497210919857, 0.1366063803434372, 0.009314071387052536, 0.008796622976660728, 0.7730679512023926, 0.015523452311754227, 0.009831519797444344, 0.008279174566268921, 0.003622138872742653, 0.009314071387052536, 0.013116518966853619, 0.010425951331853867, 0.6043688654899597, 0.02017926052212715, 0.0033632100094109774, 0.09080667048692703, 0.006726420018821955, 0.08340761065483093, 0.04237644746899605, 0.1251114159822464, 0.9537702202796936, 0.9663152098655701, 0.01130193192511797, 0.01695289835333824, 0.07897471636533737, 0.04609305039048195, 0.10319559276103973, 0.08558040857315063, 0.04389115422964096, 0.14268295466899872, 0.07134147733449936, 0.16352757811546326, 0.08675475418567657, 0.1782068908214569, 0.0081657525151968, 0.02677607163786888, 0.03627113252878189, 0.3334665596485138, 0.21724699437618256, 0.024687159806489944, 0.24573218822479248, 0.006836444139480591, 0.02316794916987419, 0.07766959816217422, 0.14406679570674896, 0.37459754943847656, 0.05295170471072197, 0.025521768257021904, 0.21454980969429016, 0.03267740458250046, 0.05402505025267601, 0.042456772178411484, 0.027668459340929985, 0.0313655361533165, 0.08466162532567978, 0.007767121307551861, 0.014757530763745308, 0.19806160032749176, 0.008543833158910275, 0.6003984808921814, 0.05903012305498123, 0.006990409456193447, 0.009320545941591263, 0.009320545941591263, 0.960945188999176, 0.00644235173240304, 0.00644235173240304, 0.9727950692176819, 0.05367519333958626, 0.015761762857437134, 0.06943695992231369, 0.042386364191770554, 0.006389903835952282, 0.5603945851325989, 0.004472932778298855, 0.15846961736679077, 0.06751998513936996, 0.021512676030397415, 0.16691085696220398, 0.15576694905757904, 0.12258289754390717, 0.01906844973564148, 0.021421050652861595, 0.13929875195026398, 0.03974657505750656, 0.24788986146450043, 0.041232429444789886, 0.04618527367711067, 0.11385633051395416, 0.798987865447998, 0.005316248629242182, 0.001550572575069964, 0.02724577486515045, 0.002658124314621091, 0.030568430200219154, 0.015284215100109577, 0.0013290621573105454, 0.003544165752828121, 0.006556189153343439, 0.9768721461296082, 0.006556189153343439, 0.023645533248782158, 0.0059113833121955395, 0.01995091885328293, 0.8327661156654358, 0.02142876386642456, 0.05615814030170441, 0.0029556916560977697, 0.0036946143954992294, 0.008867074735462666, 0.023645533248782158, 0.9851529598236084, 0.0011406519915908575, 0.0011406519915908575, 0.033078908920288086, 0.9205061793327332, 0.0011406519915908575, 0.005703259725123644, 0.0034219559747725725, 0.00456260796636343, 0.00456260796636343, 0.022813038900494576, 0.9738758206367493, 0.03341134637594223, 0.006682269275188446, 0.9555644989013672, 0.014110516756772995, 0.973625659942627, 0.9763277173042297, 0.0034018387086689472, 0.0034018387086689472, 0.010205515660345554, 0.0034018387086689472, 0.009426828473806381, 0.006363109219819307, 0.05090487375855446, 0.07871402055025101, 0.7562673687934875, 0.01908932812511921, 0.04359908401966095, 0.0042420728132128716, 0.022624388337135315, 0.008955487050116062, 0.9889916777610779, 0.9616823196411133, 0.9753652811050415, 0.9643878936767578, 0.04108517989516258, 0.03594953194260597, 0.0030813883058726788, 0.013352682814002037, 0.7662385702133179, 0.0606006383895874, 0.02054258994758129, 0.0020542589481920004, 0.0030813883058726788, 0.05341073125600815, 0.9444557428359985, 0.1197086051106453, 0.1314668208360672, 0.03377358987927437, 0.06817261129617691, 0.033398326486349106, 0.0923144742846489, 0.13546961545944214, 0.25918102264404297, 0.04590706154704094, 0.08043117821216583, 0.05727323144674301, 0.8172799348831177, 0.003333362750709057, 0.01515164878219366, 0.06909151375293732, 0.0015151648549363017, 0.02818206697702408, 0.0018181977793574333, 0.0054545933380723, 0.0009090988896787167, 0.954683780670166, 0.01744995266199112, 0.965445339679718, 0.001780607388354838, 0.0010683644795790315, 0.0032050933223217726, 0.0010683644795790315, 0.007478551007807255, 0.001780607388354838, 0.00035612148349173367, 0.00035612148349173367, 0.08811222016811371, 0.827692449092865, 0.0028120919596403837, 0.02062200754880905, 0.012185731902718544, 0.0009373640059493482, 0.03187037631869316, 0.007498912047594786, 0.003749456023797393, 0.004686819855123758, 0.07233735918998718, 0.03390247002243996, 0.03553413972258568, 0.5562180876731873, 0.046593233942985535, 0.019217442721128464, 0.13071487843990326, 0.047137122601270676, 0.033358581364154816, 0.02520023100078106, 0.014899098314344883, 0.968441367149353, 0.014899098314344883, 0.006402270402759314, 0.003201135201379657, 0.11844199895858765, 0.03841362148523331, 0.009603405371308327, 0.7874792218208313, 0.003201135201379657, 0.02240794524550438, 0.003201135201379657, 0.029759369790554047, 0.003501102328300476, 0.001750551164150238, 0.021006613969802856, 0.9050349593162537, 0.015754960477352142, 0.015754960477352142, 0.007002204656600952, 0.9082170724868774, 0.7013230323791504, 0.0022714915685355663, 0.02555428072810173, 0.07041624188423157, 0.0039751105941832066, 0.11925330758094788, 0.006246602162718773, 0.035208120942115784, 0.026690026745200157, 0.009085966274142265, 0.022184867411851883, 0.010084030218422413, 0.05176468938589096, 0.739495575428009, 0.044369734823703766, 0.013445373624563217, 0.036302510648965836, 0.011428567580878735, 0.05176468938589096, 0.018151255324482918, 0.9677315354347229, 0.013256596401333809, 0.003195822238922119, 0.003195822238922119, 0.051133155822753906, 0.9140051603317261, 0.006391644477844238, 0.003195822238922119, 0.006391644477844238, 0.006391644477844238, 0.006391644477844238, 0.006362341810017824, 0.01908702403306961, 0.006362341810017824, 0.006362341810017824, 0.006362341810017824, 0.9416265487670898, 0.012724683620035648, 0.0052762217819690704, 0.0026381108909845352, 0.0017587406327947974, 0.0026381108909845352, 0.20665201544761658, 0.0008793703163973987, 0.7588965892791748, 0.0026381108909845352, 0.017587406560778618, 0.0008793703163973987, 0.04678612947463989, 0.7320653200149536, 0.04458443075418472, 0.008256375789642334, 0.0886184349656105, 0.020916152745485306, 0.017613602802157402, 0.012109351344406605, 0.015962326899170876, 0.012659776955842972, 0.06171595677733421, 0.00881656538695097, 0.8992896676063538, 0.026449697092175484, 0.010145742446184158, 0.005636523477733135, 0.04960140958428383, 0.08342055231332779, 0.0033819142263382673, 0.02818261831998825, 0.01352765690535307, 0.7733310461044312, 0.02141878940165043, 0.012400352396070957, 0.9760127067565918, 0.865078866481781, 0.005457668099552393, 0.015681186690926552, 0.05895819142460823, 0.0056114052422344685, 0.013605736196041107, 0.007379382383078337, 0.02398299239575863, 0.002921005478128791, 0.0013836341677233577, 0.0014393228339031339, 0.0043179686181247234, 0.0033584199845790863, 0.05565381795167923, 0.003838194301351905, 0.9034149646759033, 0.012953905388712883, 0.003838194301351905, 0.006716839969158173, 0.0043179686181247234, 0.9861050248146057, 0.9701530337333679, 0.8879302740097046, 0.01168329268693924, 0.07009975612163544, 0.00584164634346962, 0.00292082317173481, 0.00876246951520443, 0.00292082317173481, 0.01168329268693924, 0.022584842517971992, 0.8638702034950256, 0.1072779968380928, 0.9044319987297058, 0.01099309604614973, 0.0009993723360821605, 0.0009993723360821605, 0.0069956062361598015, 0.023984936997294426, 0.040974266827106476, 0.0069956062361598015, 0.003997489344328642, 0.013280273415148258, 0.008570408448576927, 0.1045435443520546, 0.09543266147375107, 0.010577891953289509, 0.017295239493250847, 0.2838272154331207, 0.014670069329440594, 0.03258299455046654, 0.41917791962623596, 0.007198453415185213, 0.27498090267181396, 0.04201279208064079, 0.016752764582633972, 0.5193356871604919, 0.01518219243735075, 0.05955084040760994, 0.02120271697640419, 0.02224976383149624, 0.021333597600460052, 0.009980721399188042, 0.008930118754506111, 0.03904738277196884, 0.21764975786209106, 0.0821220725774765, 0.02083694376051426, 0.28261199593544006, 0.02293814904987812, 0.30274853110313416, 0.013132527470588684, 0.979109525680542, 0.956734836101532, 0.01345240417867899, 0.018200311809778214, 0.04629209637641907, 0.057766206562519073, 0.28210484981536865, 0.4738016128540039, 0.03264186531305313, 0.009891473688185215, 0.058755356818437576, 0.007121861446648836, 0.013202613219618797, 0.0015532486140727997, 0.05747019872069359, 0.2710418701171875, 0.012425988912582397, 0.030288347974419594, 0.017085734754800797, 0.03650134056806564, 0.535094141960144, 0.024851977825164795, 0.02826308272778988, 0.02826308272778988, 0.9326817393302917, 0.025898583233356476, 0.9323489665985107, 0.3104299008846283, 0.03451545536518097, 0.08764833211898804, 0.04863541200757027, 0.035456784069538116, 0.08084983378648758, 0.0829416811466217, 0.16818439960479736, 0.04487008973956108, 0.10637035220861435, 0.0024981475435197353, 0.0012490737717598677, 0.009992590174078941, 0.8880913853645325, 0.022483326494693756, 0.009992590174078941, 0.029977768659591675, 0.0024981475435197353, 0.014988884329795837, 0.018736105412244797, 0.09581021964550018, 0.18361058831214905, 0.032347504049539566, 0.08194700628519058, 0.06993222236633301, 0.011398643255233765, 0.39895251393318176, 0.03604435920715332, 0.036968573927879333, 0.052988290786743164, 0.9423114657402039, 0.03807318955659866, 0.4385961890220642, 0.014107423834502697, 0.02459755912423134, 0.045396964997053146, 0.05226981267333031, 0.017724711447954178, 0.07632477581501007, 0.28504231572151184, 0.0153734739869833, 0.0307469479739666, 0.9783177375793457, 0.012955195270478725, 0.9781172275543213, 0.019328679889440536, 0.9471052885055542, 0.014810457825660706, 0.0010970709845423698, 0.0318150594830513, 0.011519244872033596, 0.004936819430440664, 0.015358993783593178, 0.014810457825660706, 0.0027426774613559246, 0.16565771400928497, 0.7377802133560181, 0.004632706753909588, 0.004632706753909588, 0.8593670725822449, 0.013898119330406189, 0.002316353376954794, 0.020847178995609283, 0.04632706567645073, 0.004632706753909588, 0.039378006011247635, 0.004632706753909588, 0.111050084233284, 0.08482992649078369, 0.04719628766179085, 0.0194337647408247, 0.10148743540048599, 0.11259244382381439, 0.10117896646261215, 0.045345451682806015, 0.016040567308664322, 0.3612212538719177, 0.09541534632444382, 0.01838277280330658, 0.00350148044526577, 0.00350148044526577, 0.0026261103339493275, 0.11554885655641556, 0.00700296089053154, 0.01488129235804081, 0.001750740222632885, 0.7388123869895935, 0.012141762301325798, 0.07285057753324509, 0.006070881150662899, 0.0030354405753314495, 0.8590297102928162, 0.0030354405753314495, 0.0030354405753314495, 0.009106322191655636, 0.0030354405753314495, 0.027318965643644333, 0.7457941174507141, 0.04332796484231949, 0.00783590879291296, 0.027656149119138718, 0.009218716062605381, 0.13505418598651886, 0.00368748651817441, 0.019820239394903183, 0.00368748651817441, 0.003226550528779626, 0.9667194485664368, 0.009764842689037323, 0.009764842689037323, 0.020428836345672607, 0.010895379818975925, 0.14436377584934235, 0.42589259147644043, 0.06595595926046371, 0.0317133367061615, 0.13288471102714539, 0.0040857670828700066, 0.1433909833431244, 0.020428836345672607, 0.0009858367266133428, 0.06999441236257553, 0.007886693812906742, 0.003943346906453371, 0.7916269302368164, 0.07590942829847336, 0.004929183982312679, 0.0019716734532266855, 0.039433471858501434, 0.00295751029625535, 0.025252124294638634, 0.006313031073659658, 0.9658938050270081, 0.014769280329346657, 0.9304646253585815, 0.04430783912539482, 0.04370862990617752, 0.005142191890627146, 0.007713287603110075, 0.007713287603110075, 0.028282055631279945, 0.002571095945313573, 0.8870280981063843, 0.01799767091870308, 0.9725567102432251, 0.010927603580057621, 0.02857101708650589, 0.024954432621598244, 0.13453693687915802, 0.050813011825084686, 0.0032549260649830103, 0.19782717525959015, 0.0052440473809838295, 0.3515320122241974, 0.1285695731639862, 0.07468246668577194, 0.9832941293716431, 0.02070852741599083, 0.006902842316776514, 0.06902842223644257, 0.013805684633553028, 0.003451421158388257, 0.027611369267106056, 0.031062791123986244, 0.01725710555911064, 0.752409815788269, 0.0586741603910923, 0.01520618051290512, 0.014395183883607388, 0.5245118737220764, 0.060013726353645325, 0.009934704750776291, 0.012570442631840706, 0.06913743168115616, 0.007907213643193245, 0.1739587038755417, 0.11252573132514954, 0.030226025730371475, 0.003100105095654726, 0.3859630823135376, 0.03875131532549858, 0.0050376709550619125, 0.023638302460312843, 0.0728524699807167, 0.027900947257876396, 0.3848005533218384, 0.027900947257876396, 0.9541451930999756, 0.9645918607711792, 0.00614389730617404, 0.00307194865308702, 0.01228779461234808, 0.009215845726430416, 0.9577675461769104, 0.005472957622259855, 0.005472957622259855, 0.01094591524451971, 0.005472957622259855, 0.005472957622259855, 0.15764069557189941, 0.017664160579442978, 0.08994399756193161, 0.05938977375626564, 0.01088494248688221, 0.11734731495380402, 0.03303675353527069, 0.03418253734707832, 0.29332056641578674, 0.18657173216342926, 0.3145671486854553, 0.00478277588263154, 0.30161380767822266, 0.0337783545255661, 0.02072536200284958, 0.09764834493398666, 0.03158624842762947, 0.0190314631909132, 0.15195277333259583, 0.024312444031238556, 0.9392104744911194, 0.00624832371249795, 0.002343121450394392, 0.054672833532094955, 0.23509317636489868, 0.003124161856248975, 0.010153526440262794, 0.05857803672552109, 0.011715606786310673, 0.06560739874839783, 0.5537576675415039, 0.27962958812713623, 0.0852712020277977, 0.02414907142519951, 0.053294502198696136, 0.11574900150299072, 0.011325081810355186, 0.2733008861541748, 0.08760283887386322, 0.04996359720826149, 0.0196523480117321, 0.23023520410060883, 0.03113972581923008, 0.03279609605669975, 0.07155511528253555, 0.04571576789021492, 0.0200420580804348, 0.3715234398841858, 0.039752840995788574, 0.040581025183200836, 0.11660833656787872, 0.0054477243684232235, 0.005901701282709837, 0.05402326583862305, 0.16706354916095734, 0.02678464539349079, 0.6755177974700928, 0.027238622307777405, 0.004085793159902096, 0.01134942565113306, 0.023152828216552734, 0.05113215371966362, 0.0491325706243515, 0.12183164060115814, 0.09512294828891754, 0.016282305121421814, 0.2870827615261078, 0.06470074504613876, 0.1111195981502533, 0.08598200231790543, 0.11754681915044785, 0.005721841938793659, 0.005721841938793659, 0.005721841938793659, 0.017165526747703552, 0.8468326330184937, 0.022887367755174637, 0.09154947102069855, 0.039753805845975876, 0.055655330419540405, 0.3819822371006012, 0.05859365686774254, 0.016938578337430954, 0.10733527690172195, 0.0584208108484745, 0.07172969728708267, 0.09903883188962936, 0.11027360707521439, 0.07719587534666061, 0.013622800819575787, 0.41938766837120056, 0.03762488067150116, 0.01005492452532053, 0.09308914095163345, 0.06032954528927803, 0.0755741074681282, 0.10671193897724152, 0.10606323927640915, 0.02594904787838459, 0.020413249731063843, 0.2491108477115631, 0.06193172559142113, 0.02664102241396904, 0.07127338647842407, 0.02871694602072239, 0.06954344362020493, 0.38542985916137695, 0.06123975291848183, 0.9717114567756653, 0.9770413637161255, 0.008008535951375961, 0.03559120371937752, 0.5242919921875, 0.04516731575131416, 0.011970136314630508, 0.22839020192623138, 0.029685938730835915, 0.04037925973534584, 0.02058863453567028, 0.03463359549641609, 0.02952633611857891, 0.031088681891560555, 0.05937862768769264, 0.15786394476890564, 0.02927328459918499, 0.016716785728931427, 0.1930372714996338, 0.04001438617706299, 0.13880226016044617, 0.09175321459770203, 0.24205300211906433, 0.6745569109916687, 0.03335902467370033, 0.018358474597334862, 0.02967081591486931, 0.04392822086811066, 0.07770010828971863, 0.019789719954133034, 0.09443467110395432, 0.004073544405400753, 0.0041010682471096516, 0.4209893047809601, 0.010831759311258793, 0.06636359542608261, 0.15042415261268616, 0.008238240145146847, 0.11342835426330566, 0.04988711699843407, 0.10633431375026703, 0.03958931937813759, 0.034097157418727875, 0.06569604575634003, 0.007299560122191906, 0.919744610786438, 0.9545644521713257, 0.015151816420257092, 0.015151816420257092, 0.005045195575803518, 0.6195030808448792, 0.050921276211738586, 0.050569284707307816, 0.06359293311834335, 0.016778208315372467, 0.043646808713674545, 0.009386410005390644, 0.08553366363048553, 0.055027831345796585, 0.14103810489177704, 0.05188572406768799, 0.2923814356327057, 0.030676087364554405, 0.0346304252743721, 0.04805121198296547, 0.05584006384015083, 0.05787714570760727, 0.2330663651227951, 0.05440212041139603, 0.9801420569419861, 0.05925436690449715, 0.8460206389427185, 0.026335272938013077, 0.013167636469006538, 0.0032919091172516346, 0.009875727817416191, 0.03621100261807442, 0.7629570960998535, 0.013211377896368504, 0.023945624008774757, 0.018165646120905876, 0.03550557792186737, 0.03220273554325104, 0.026422755792737007, 0.058625489473342896, 0.01651422306895256, 0.011559955775737762, 0.02193349599838257, 0.7739390730857849, 0.0015666782855987549, 0.0015666782855987549, 0.17076793313026428, 0.0031333565711975098, 0.010966747999191284, 0.0015666782855987549, 0.0015666782855987549, 0.010966747999191284, 0.9369675517082214, 0.029053257778286934, 0.0072633144445717335, 0.021789943799376488, 0.006474601104855537, 0.037648607045412064, 0.07337880879640579, 0.012229802086949348, 0.09016481786966324, 0.018464602530002594, 0.10239461809396744, 0.014627802185714245, 0.512212872505188, 0.13260942697525024, 0.03306305408477783, 0.007751625496894121, 0.2559618353843689, 0.48218274116516113, 0.01360489334911108, 0.034328628331422806, 0.08574246615171432, 0.005695071537047625, 0.04714253917336464, 0.03448682278394699, 0.0014987413305789232, 0.0014987413305789232, 0.0164861548691988, 0.07044084370136261, 0.0014987413305789232, 0.013488671742379665, 0.008992447517812252, 0.010491188615560532, 0.8632749915122986, 0.014987412840127945, 0.010765562765300274, 0.9689006805419922, 0.9648147225379944, 0.9553658366203308, 0.011289916932582855, 0.015938706696033478, 0.03400257229804993, 0.1609809398651123, 0.023908060044050217, 0.02231418900191784, 0.5374000668525696, 0.02696297876536846, 0.07570885866880417, 0.09124909341335297, 0.9762929081916809, 0.003401717636734247, 0.003401717636734247, 0.010205152444541454, 0.003401717636734247, 0.018650388345122337, 0.0016954898601397872, 0.02204136736690998, 0.049169205129146576, 0.0033909797202795744, 0.18141742050647736, 0.0033909797202795744, 0.03390979766845703, 0.6765004396438599, 0.013563918881118298, 0.07859230786561966, 0.0029108261223882437, 0.017464956268668175, 0.8499612212181091, 0.02328660897910595, 0.005821652244776487, 0.008732478134334087, 0.011643304489552975, 0.005734215024858713, 0.0028671075124293566, 0.0028671075124293566, 0.0028671075124293566, 0.011468430049717426, 0.0028671075124293566, 0.9690823554992676, 0.9544603228569031, 0.029218172654509544, 0.9690340757369995, 0.007511891890317202, 0.007511891890317202, 0.007511891890317202, 0.9595975279808044, 0.010592973791062832, 0.9745535850524902, 0.011792520061135292, 0.003057320136576891, 0.12120090425014496, 0.0637669637799263, 0.002183800097554922, 0.04848036170005798, 0.06900808215141296, 0.023148279637098312, 0.3157774806022644, 0.3417647182941437, 0.014519958756864071, 0.9728372097015381, 0.007936714217066765, 0.0019305520690977573, 0.11754917353391647, 0.24496561288833618, 0.004933632910251617, 0.048907320946455, 0.028100257739424706, 0.026169706135988235, 0.1707466095685959, 0.3490009009838104, 0.09816378355026245, 0.010333029553294182, 0.005166514776647091, 0.8731409907341003, 0.03636227175593376, 0.9526915550231934, 0.00727245444431901, 0.2128671258687973, 0.0706799253821373, 0.025411678478121758, 0.15388840436935425, 0.3114407956600189, 0.08545415848493576, 0.036521900445222855, 0.05626027658581734, 0.027657361701130867, 0.019856568425893784, 0.0034383253660053015, 0.9352244734764099, 0.02062995173037052, 0.0034383253660053015, 0.006876650732010603, 0.027506602928042412, 0.003349677659571171, 0.006699355319142342, 0.9848052263259888, 0.021950358524918556, 0.0035120572429150343, 0.017560286447405815, 0.0149262435734272, 0.0035120572429150343, 0.02019432932138443, 0.002634042873978615, 0.06497305631637573, 0.006146100349724293, 0.8446497917175293, 0.002562308218330145, 0.002562308218330145, 0.012811541557312012, 0.002562308218330145, 0.01024923287332058, 0.00512461643666029, 0.9608656167984009, 0.10561191290616989, 0.010561191476881504, 0.010561191476881504, 0.042244765907526016, 0.0070407940074801445, 0.028163176029920578, 0.0035203970037400723, 0.0352039709687233, 0.0035203970037400723, 0.7533649802207947, 0.9459279179573059, 0.968442976474762, 0.052303459495306015, 0.07179543375968933, 0.03248662129044533, 0.053602926433086395, 0.5613688230514526, 0.06627270579338074, 0.0240400992333889, 0.02241576835513115, 0.04613100364804268, 0.06984623521566391, 0.07243259996175766, 0.35502034425735474, 0.012072100304067135, 0.024093899875879288, 0.3402320146560669, 0.0076959640718996525, 0.11745147407054901, 0.025854414328932762, 0.022685488685965538, 0.022433986887335777, 0.01271622721105814, 0.002543245442211628, 0.002543245442211628, 0.005086490884423256, 0.8214682340621948, 0.06866762787103653, 0.045778416097164154, 0.005086490884423256, 0.005086490884423256, 0.03306218981742859, 0.972887396812439, 0.9775556325912476, 0.010182870551943779, 0.010182870551943779, 0.006508396472781897, 0.9762594699859619, 0.08466081321239471, 0.1961507648229599, 0.01252026204019785, 0.013712667860090733, 0.030406350269913673, 0.15918618440628052, 0.01252026204019785, 0.0250405240803957, 0.011924058198928833, 0.4537104368209839, 0.01878862828016281, 0.9394314289093018, 0.01878862828016281, 0.12072625756263733, 0.0019848612137138844, 0.362178772687912, 0.11115222424268723, 0.010391331277787685, 0.014594567008316517, 0.3140750825405121, 0.029889672994613647, 0.015178349800407887, 0.01984861120581627, 0.0168174859136343, 0.9585967063903809, 0.1286955177783966, 0.03838839381933212, 0.12617824971675873, 0.03964702785015106, 0.02171146869659424, 0.14867636561393738, 0.04295094683766365, 0.059627871960401535, 0.12177301943302155, 0.2721799910068512, 0.01233476772904396, 0.01233476772904396, 0.9497770667076111, 0.03617861121892929, 0.003946757409721613, 0.1414254754781723, 0.5735954642295837, 0.028942888602614403, 0.032231852412223816, 0.03091626800596714, 0.011182479560375214, 0.05920136347413063, 0.08288191258907318, 0.02570965513586998, 0.0313657782971859, 0.005656124092638493, 0.018510950729250908, 0.14397406578063965, 0.06118897721171379, 0.03959286957979202, 0.0025709655601531267, 0.008741282857954502, 0.6622807383537292, 0.002807171316817403, 0.005614342633634806, 0.008421513251960278, 0.002807171316817403, 0.790218710899353, 0.15299083292484283, 0.012632270343601704, 0.0014035856584087014, 0.023860955610871315, 0.002807171316817403, 0.037291381508111954, 0.0054008206352591515, 0.03703419864177704, 0.32687824964523315, 0.030347470194101334, 0.49198904633522034, 0.010801641270518303, 0.047578658908605576, 0.010287277400493622, 0.002314637415111065, 0.9650341868400574, 0.9742252230644226, 0.977888822555542, 0.01657438650727272, 0.5281915068626404, 0.0716916099190712, 0.03489624708890915, 0.022551996633410454, 0.05293784663081169, 0.11038608849048615, 0.03632058575749397, 0.08166196942329407, 0.023501554504036903, 0.0377449207007885, 0.9369585514068604, 0.14589263498783112, 0.5028925538063049, 0.01411864161491394, 0.01837664470076561, 0.1482083946466446, 0.014790957793593407, 0.05766979604959488, 0.035782165825366974, 0.023605771362781525, 0.038546133786439896, 0.04422323405742645, 0.13651520013809204, 0.01345924474298954, 0.01345924474298954, 0.05383697897195816, 0.02691848948597908, 0.6748849749565125, 0.009613745845854282, 0.01345924474298954, 0.011536495760083199, 0.9034503698348999, 0.004503366071730852, 0.054040390998125076, 0.004503366071730852, 0.013510097749531269, 0.013510097749531269, 0.004503366071730852, 0.004503366071730852, 0.8826597332954407, 0.027020195499062538, 0.9327457547187805, 0.01566752791404724, 0.09882595390081406, 0.0783376470208168, 0.06749089062213898, 0.04218180850148201, 0.003615583525970578, 0.6242907643318176, 0.016872722655534744, 0.027719473466277122, 0.024103891104459763, 0.008880621753633022, 0.022201554849743843, 0.07992559671401978, 0.013320932164788246, 0.865860641002655, 0.004440310876816511, 0.976641058921814, 0.0087985685095191, 0.05750540271401405, 0.9296706914901733, 0.8958410024642944, 0.010043060407042503, 0.001004306017421186, 0.001004306017421186, 0.008034448139369488, 0.024103345349431038, 0.046198077499866486, 0.008034448139369488, 0.004017224069684744, 0.15907037258148193, 0.1871470808982849, 0.13833822309970856, 0.02249855548143387, 0.16753056645393372, 0.014224293641746044, 0.09891927242279053, 0.024450909346342087, 0.08608951419591904, 0.10170834511518478, 0.9749503135681152, 0.9728959798812866, 0.012473025359213352, 0.019736304879188538, 0.049340762197971344, 0.0053826286457479, 0.7589506506919861, 0.036781296133995056, 0.006279733497649431, 0.03139866515994072, 0.05920891463756561, 0.020633408799767494, 0.011662362143397331, 0.09003151208162308, 0.06505926698446274, 0.123171366751194, 0.09979509562253952, 0.09040703624486923, 0.0468464270234108, 0.2362036257982254, 0.08064344525337219, 0.10589733719825745, 0.061961207538843155, 0.04116984084248543, 0.020140642300248146, 0.11077353358268738, 0.11817818135023117, 0.08619009703397751, 0.01747496798634529, 0.47656312584877014, 0.029914777725934982, 0.07641596347093582, 0.022806314751505852, 0.00793185643851757, 0.00793185643851757, 0.026439521461725235, 0.021151617169380188, 0.8989437222480774, 0.010575808584690094, 0.00793185643851757, 0.0026439521461725235, 0.00793185643851757, 0.00793185643851757, 0.9817667603492737, 0.019676124677062035, 0.2451816201210022, 0.08554836362600327, 0.07391379028558731, 0.1189122274518013, 0.04037882760167122, 0.2937730848789215, 0.02275586500763893, 0.05457985773682594, 0.045169536024332047, 0.0064792633056640625, 0.933013916015625, 0.019437789916992188, 0.02591705322265625, 0.012958526611328125, 0.0064792633056640625, 0.9761723279953003, 0.003401297377422452, 0.003401297377422452, 0.010203892365098, 0.003401297377422452, 0.0583752803504467, 0.013791961595416069, 0.2663773000240326, 0.12893880903720856, 0.03897031024098396, 0.035442136228084564, 0.18250294029712677, 0.04795112460851669, 0.1074490025639534, 0.12043910473585129, 0.0069500054232776165, 0.0069500054232776165, 0.9243507385253906, 0.041700031608343124, 0.0069500054232776165, 0.0069500054232776165, 0.1256648451089859, 0.0052800350822508335, 0.037664249539375305, 0.053152356296777725, 0.004576030652970076, 0.2766738533973694, 0.0024640164338052273, 0.41430675983428955, 0.029920199885964394, 0.05068833753466606, 0.02341063879430294, 0.001908077159896493, 0.07529566437005997, 0.31908920407295227, 0.007852471433579922, 0.15249939262866974, 0.04821564257144928, 0.034638941287994385, 0.2871656119823456, 0.04990355670452118, 0.9607811570167542, 0.0053803883492946625, 0.9846110939979553, 0.014319919981062412, 0.003579979995265603, 0.707642674446106, 0.029833165928721428, 0.008353286422789097, 0.051313046365976334, 0.007159959990531206, 0.004773306660354137, 0.1503591537475586, 0.025059860199689865, 0.9806231260299683, 0.008171859197318554, 0.008171859197318554, 0.9452488422393799, 0.05515468120574951, 0.042684927582740784, 0.49543291330337524, 0.07290010154247284, 0.018704630434513092, 0.030215172097086906, 0.12565675377845764, 0.015347389504313469, 0.05995073914527893, 0.0844106450676918, 0.9811434149742126, 0.9714817404747009, 0.9505749940872192, 0.019803645089268684, 0.9859605431556702, 0.9706854820251465, 0.014707355760037899, 0.18792665004730225, 0.0859459787607193, 0.05740421637892723, 0.04714201018214226, 0.058045607060194016, 0.034634947776794434, 0.3928501009941101, 0.06413879245519638, 0.048104092478752136, 0.02437273971736431, 0.010463693179190159, 0.010463693179190159, 0.08370954543352127, 0.015695540234446526, 0.8737183809280396, 0.00359416869468987, 0.9201071858406067, 0.00359416869468987, 0.00359416869468987, 0.00718833738937974, 0.00718833738937974, 0.025159180164337158, 0.017970843240618706, 0.010782506316900253, 0.9785245656967163, 0.9603416323661804, 0.3186102509498596, 0.22887304425239563, 0.007657863199710846, 0.024483591318130493, 0.15779513120651245, 0.04799646884202957, 0.08876650035381317, 0.0703229159116745, 0.025454306975007057, 0.030092166736721992, 0.5198010802268982, 0.27903300523757935, 0.003439544001594186, 0.006879088003188372, 0.1287679374217987, 0.0019347436027601361, 0.029236124828457832, 0.022786980494856834, 0.005374287720769644, 0.0025796580594033003, 0.21773584187030792, 0.2177962362766266, 0.008455760776996613, 0.03877570480108261, 0.1602366715669632, 0.043969955295324326, 0.13583575189113617, 0.12007180601358414, 0.026998035609722137, 0.030199145898222923, 0.002028718823567033, 0.002028718823567033, 0.5955738425254822, 0.26895013451576233, 0.002028718823567033, 0.08810435980558395, 0.002028718823567033, 0.006955607328563929, 0.020866822451353073, 0.011592678725719452, 0.0022748056799173355, 0.0060661486349999905, 0.6043400764465332, 0.18008878827095032, 0.0068244170397520065, 0.13345526158809662, 0.0018956714775413275, 0.029193339869379997, 0.02123151905834675, 0.01516537182033062, 0.9646148085594177, 0.011908824555575848, 0.11436856538057327, 0.07165095210075378, 0.17482832074165344, 0.022791827097535133, 0.01842453144490719, 0.1808333545923233, 0.011464152485132217, 0.19106920063495636, 0.04763082414865494, 0.1667761206626892, 0.08439026772975922, 0.11593679338693619, 0.1553366631269455, 0.23666545748710632, 0.11021316051483154, 0.08492270112037659, 0.08785106986761093, 0.044857289642095566, 0.03580596670508385, 0.043925534933805466, 0.9669437408447266, 0.901824414730072, 0.001135798986069858, 0.004543195944279432, 0.003407397074624896, 0.001135798986069858, 0.003407397074624896, 0.08291333168745041, 0.001135798986069858, 0.001135798986069858, 0.007441910915076733, 0.007193847093731165, 0.05482207611203194, 0.5678178071975708, 0.013891567476093769, 0.013643503189086914, 0.06772138923406601, 0.01959703303873539, 0.04837242141366005, 0.19919514656066895, 0.0023105554282665253, 0.006931666284799576, 0.041589997708797455, 0.0023105554282665253, 0.020794998854398727, 0.9242221713066101, 0.09521688520908356, 0.31812557578086853, 0.054462216794490814, 0.040478676557540894, 0.03118697926402092, 0.16789451241493225, 0.021619291976094246, 0.2030373513698578, 0.029439035803079605, 0.03836274519562721, 0.980956494808197, 0.0032918003853410482, 0.0065836007706820965, 0.0032918003853410482, 0.0032918003853410482, 0.04054860398173332, 0.008978620171546936, 0.10397820919752121, 0.15698103606700897, 0.028384024277329445, 0.043444935232400894, 0.03649374470114708, 0.07298748940229416, 0.47441866993904114, 0.033887047320604324, 0.9476771354675293, 0.016339261084794998, 0.016339261084794998, 0.9731718897819519, 0.009338806383311749, 0.009338806383311749, 0.009338806383311749, 0.961897075176239, 0.009338806383311749, 0.004478263668715954, 0.001492754672653973, 0.2642175555229187, 0.001492754672653973, 0.7210004925727844, 0.001492754672653973, 0.004478263668715954, 0.001492754672653973, 0.001492754672653973, 0.277523934841156, 0.05806791037321091, 0.12126205861568451, 0.02784077823162079, 0.03676750510931015, 0.10888838022947311, 0.040214456617832184, 0.053471971303224564, 0.10225962102413177, 0.17367342114448547, 0.9263334274291992, 0.027790002524852753, 0.027790002524852753, 0.96763676404953, 0.9693008661270142, 0.9003684520721436, 0.0032373550347983837, 0.987393319606781, 0.9717223048210144, 0.9763203859329224, 0.030815118923783302, 0.014511770568788052, 0.05482224375009537, 0.3694230914115906, 0.23550274968147278, 0.03260669484734535, 0.10131365805864334, 0.03610026836395264, 0.021857235580682755, 0.10310523957014084, 0.03218397498130798, 0.7235845923423767, 0.014427299611270428, 0.01775667630136013, 0.147602379322052, 0.004439169075340033, 0.024415429681539536, 0.013317507691681385, 0.009988131001591682, 0.014427299611270428, 0.9541844725608826, 0.0019362173043191433, 0.0019362173043191433, 0.009681086987257004, 0.8577442765235901, 0.0019362173043191433, 0.10261952131986618, 0.017425956204533577, 0.0019362173043191433, 0.0038724346086382866, 0.9768922924995422, 0.9808632135391235, 0.01140538603067398, 0.1437191665172577, 0.2673984169960022, 0.026470648124814034, 0.04351954162120819, 0.2651551365852356, 0.018095754086971283, 0.12412790209054947, 0.056231435388326645, 0.03454643860459328, 0.020787684246897697, 0.0344088152050972, 0.014487922191619873, 0.014487922191619873, 0.046180251985788345, 0.7452225089073181, 0.041652776300907135, 0.010865941643714905, 0.009960446506738663, 0.008149456232786179, 0.07243961095809937, 0.023121226578950882, 0.9479702711105347, 0.011560613289475441, 0.005090724676847458, 0.019635653123259544, 0.0276353619992733, 0.06545217335224152, 0.00799970980733633, 0.010181449353694916, 0.6785208582878113, 0.01163594238460064, 0.15054000914096832, 0.02327188476920128, 0.006024599075317383, 0.006024599075317383, 0.012049198150634766, 0.006024599075317383, 0.9579112529754639, 0.020908819511532784, 0.5872264504432678, 0.04626632481813431, 0.004448684863746166, 0.19841136038303375, 0.020019082352519035, 0.031140794977545738, 0.028471585363149643, 0.021353688091039658, 0.04226250946521759, 0.009235662408173084, 0.027706988155841827, 0.027706988155841827, 0.9328019618988037, 0.008312059566378593, 0.09143266081809998, 0.8644542098045349, 0.033248238265514374, 0.010009191930294037, 0.010009191930294037, 0.020018383860588074, 0.9108364582061768, 0.04003676772117615, 0.005893073044717312, 0.023572292178869247, 0.9487847685813904, 0.011786146089434624, 0.005893073044717312, 0.027539914473891258, 0.0038249881472438574, 0.012239961884915829, 0.16829948127269745, 0.01529995258897543, 0.07496976852416992, 0.07649976015090942, 0.009179971180856228, 0.005354983266443014, 0.60511314868927, 0.9803528189659119, 0.005079548340290785, 0.005079548340290785, 0.005079548340290785, 0.0853121355175972, 0.17025335133075714, 0.017433349043130875, 0.035052161663770676, 0.03616492822766304, 0.024666335433721542, 0.035794004797935486, 0.5574962496757507, 0.02615002542734146, 0.011869514361023903, 0.0024671154096722603, 0.009868461638689041, 0.04687519371509552, 0.019736923277378082, 0.004934230819344521, 0.007401346229016781, 0.004934230819344521, 0.8092138767242432, 0.09128326922655106, 0.0024671154096722603, 0.028544001281261444, 0.9419520497322083, 0.007933993823826313, 0.015867987647652626, 0.9203432202339172, 0.04760396108031273, 0.013485213741660118, 0.9709353446960449, 0.025561831891536713, 0.006278344430029392, 0.021525751799345016, 0.11211329698562622, 0.011659782379865646, 0.18072663247585297, 0.006726797670125961, 0.4628036916255951, 0.13094832003116608, 0.04125769063830376, 0.9497418999671936, 0.02999185025691986, 0.012914586812257767, 0.012914586812257767, 0.025829173624515533, 0.9169356822967529, 0.025829173624515533, 0.09307112544775009, 0.056474294513463974, 0.08508298546075821, 0.42374297976493835, 0.06390511989593506, 0.04848615452647209, 0.05443081632256508, 0.05015809088945389, 0.10143080353736877, 0.023407109081745148, 0.9739824533462524, 0.9672979116439819, 0.0124812638387084, 0.0062406319193542, 0.0062406319193542, 0.02489505708217621, 0.040706783533096313, 0.14028701186180115, 0.14466047286987305, 0.02556789666414261, 0.03902468457818031, 0.33255088329315186, 0.02775462530553341, 0.15441663563251495, 0.07031171768903732, 0.05818413943052292, 0.04752271622419357, 0.32415255904197693, 0.124534472823143, 0.033685553818941116, 0.0799606591463089, 0.01735316403210163, 0.0703200101852417, 0.1747792512178421, 0.0695260763168335, 0.003858989104628563, 0.002042994135990739, 0.46716466546058655, 0.3171181082725525, 0.00612898264080286, 0.008171976543962955, 0.09216173738241196, 0.0059019834734499454, 0.04993985965847969, 0.04744286462664604, 0.008355154655873775, 0.977553129196167, 0.008355154655873775, 0.016251441091299057, 0.055254895240068436, 0.8580760359764099, 0.003250288078561425, 0.0130011523142457, 0.02275201678276062, 0.0260023046284914, 0.015732740983366966, 0.06293096393346786, 0.007866370491683483, 0.023599112406373024, 0.8810334801673889, 0.007866370491683483, 0.035333242267370224, 0.005888873711228371, 0.023555494844913483, 0.041222117841243744, 0.871553361415863, 0.011777747422456741, 0.011777747422456741, 0.014040391892194748, 0.017160478979349136, 0.1255834996700287, 0.042121175676584244, 0.05694158747792244, 0.009360261261463165, 0.00546015240252018, 0.008580239489674568, 0.7082597613334656, 0.011700326576828957, 0.057604897767305374, 0.018965305760502815, 0.042361754924058914, 0.2587789297103882, 0.09571275115013123, 0.011698225513100624, 0.3674306273460388, 0.03509467467665672, 0.039348576217889786, 0.07284803688526154, 0.010038240812718868, 0.005019120406359434, 0.005019120406359434, 0.9134799242019653, 0.005019120406359434, 0.06022944301366806, 0.005019120406359434, 0.00930202566087246, 0.01860405132174492, 0.95810866355896, 0.00930202566087246, 0.00930202566087246, 0.9571819305419922, 0.01904839649796486, 0.01904839649796486, 0.017827196046710014, 0.9626685976982117, 0.008913598023355007, 0.02073480375111103, 0.07410772144794464, 0.0583646334707737, 0.035901930183172226, 0.1608867198228836, 0.5043548941612244, 0.05375690013170242, 0.007295579183846712, 0.0756436362862587, 0.00902347918599844, 0.9720008373260498, 0.08442001789808273, 0.016884004697203636, 0.008442002348601818, 0.0021105005871504545, 0.0021105005871504545, 0.008442002348601818, 0.004221001174300909, 0.8589736819267273, 0.01266300305724144, 0.0021105005871504545, 0.057241737842559814, 0.01813247613608837, 0.007821852341294289, 0.026309866458177567, 0.11057254672050476, 0.45366743206977844, 0.24532172083854675, 0.03484279662370682, 0.0337761789560318, 0.011732778511941433, 0.0029882402159273624, 0.005727460142225027, 0.009587270207703114, 0.004980400204658508, 0.4199722707271576, 0.2923494875431061, 0.21938663721084595, 0.001245100051164627, 0.035734374076128006, 0.007968640886247158, 0.02031632512807846, 0.9751836061477661, 0.4093570113182068, 0.014063798822462559, 0.016072912141680717, 0.010547849349677563, 0.06429164856672287, 0.013059241697192192, 0.4525529444217682, 0.007031899411231279, 0.00853873509913683, 0.004520506598055363, 0.07889609038829803, 0.07120706886053085, 0.04546554386615753, 0.09260261803865433, 0.46936488151550293, 0.0508144311606884, 0.016380969434976578, 0.09260261803865433, 0.06084359809756279, 0.022398466244339943, 0.02570410631597042, 0.021420089527964592, 0.009424839168787003, 0.7634119987487793, 0.054835427552461624, 0.01285205315798521, 0.009424839168787003, 0.028274517506361008, 0.052265018224716187, 0.023133696988224983, 0.012679965235292912, 0.05754014477133751, 0.16032470762729645, 0.1880861520767212, 0.009606034494936466, 0.09202580899000168, 0.024975689128041267, 0.16051682829856873, 0.1577310860157013, 0.13640569150447845, 0.0017389319837093353, 0.015650387853384018, 0.005216795951128006, 0.020867183804512024, 0.1286809742450714, 0.006955727934837341, 0.024345047771930695, 0.0017389319837093353, 0.0034778639674186707, 0.7929530143737793, 0.9547097682952881, 0.011642802506685257, 0.023285605013370514, 0.057486288249492645, 0.05403338000178337, 0.05002053827047348, 0.051047079265117645, 0.03564896434545517, 0.15612754225730896, 0.1639665812253952, 0.16592633724212646, 0.1455821543931961, 0.12010528147220612, 0.010359633713960648, 0.010359633713960648, 0.953086256980896, 0.010359633713960648, 0.022696591913700104, 0.9419085383415222, 0.011348295956850052, 0.011348295956850052, 0.12989288568496704, 0.03350802883505821, 0.5633291006088257, 0.09461089968681335, 0.014881506562232971, 0.024145489558577538, 0.02660931646823883, 0.017739543691277504, 0.07470319420099258, 0.020597580820322037, 0.019191740080714226, 0.004264831077307463, 0.9361304044723511, 0.0127944927662611, 0.0021324155386537313, 0.00639724638313055, 0.01705932430922985, 0.0021324155386537313, 0.08610102534294128, 0.05205347388982773, 0.43181464076042175, 0.05336299538612366, 0.014732114970684052, 0.14830328524112701, 0.015386875718832016, 0.06482130289077759, 0.10476170480251312, 0.02880946919322014, 0.9571086764335632, 0.011963858269155025, 0.011963858269155025, 0.014066077768802643, 0.0014066078001633286, 0.012659469619393349, 0.030945371836423874, 0.025318939238786697, 0.8256787657737732, 0.02250572480261326, 0.05907752737402916, 0.004219823516905308, 0.0028132156003266573, 0.035510070621967316, 0.0044387588277459145, 0.0044387588277459145, 0.07989766448736191, 0.861119270324707, 0.008877517655491829, 0.008877517655491829, 0.9788049459457397, 0.006603356916457415, 0.9376766681671143, 0.01981007121503353, 0.01981007121503353, 0.006603356916457415, 0.006603356916457415, 0.030301107093691826, 0.042421549558639526, 0.0060602216981351376, 0.9150934815406799, 0.0656486228108406, 0.055045582354068756, 0.05797833949327469, 0.08820828795433044, 0.07715405523777008, 0.10648161917924881, 0.08865948021411896, 0.11212153732776642, 0.09813454002141953, 0.2508634626865387, 0.07717673480510712, 0.670335054397583, 0.014700329862535, 0.02278551086783409, 0.08158683031797409, 0.013965313322842121, 0.038220856338739395, 0.0073501649312675, 0.055126238614320755, 0.01837541162967682, 0.1172066181898117, 0.3516198396682739, 0.10588699579238892, 0.01714114472270012, 0.16190296411514282, 0.046442799270153046, 0.06604191660881042, 0.042885202914476395, 0.04133279621601105, 0.049612294882535934, 0.963226318359375, 0.016649914905428886, 0.0026015492621809244, 0.426914244890213, 0.08344469219446182, 0.010341159068048, 0.011186662130057812, 0.3160882592201233, 0.005853486247360706, 0.08546089380979538, 0.041364636272192, 0.009050133638083935, 0.0011312667047604918, 0.6583971977233887, 0.13688327372074127, 0.005656333640217781, 0.022625334560871124, 0.11538920551538467, 0.015837734565138817, 0.0033938002306967974, 0.030544200912117958, 0.009472141042351723, 0.004736070521175861, 0.9045894742012024, 0.06156891584396362, 0.004736070521175861, 0.004736070521175861, 0.004736070521175861, 0.004421474412083626, 0.9727243781089783, 0.004421474412083626, 0.008842948824167252, 0.004421474412083626, 0.004421474412083626, 0.064934141933918, 0.33957576751708984, 0.1421811580657959, 0.06998889893293381, 0.09539227932691574, 0.0401788130402565, 0.05313972011208534, 0.05378776416182518, 0.05586151033639908, 0.08476433157920837, 0.9639211297035217, 0.019231505692005157, 0.021416904404759407, 0.10577328503131866, 0.04895292595028877, 0.019887125119566917, 0.036714691668748856, 0.3254058361053467, 0.04742314666509628, 0.06643611192703247, 0.3085782527923584, 0.032067667692899704, 0.010689222253859043, 0.032067667692899704, 0.9085838794708252, 0.005174563731998205, 0.005174563731998205, 0.0362219475209713, 0.06726932525634766, 0.005174563731998205, 0.04139650985598564, 0.005174563731998205, 0.8124064803123474, 0.025872819125652313, 0.07970056682825089, 0.007655965629965067, 0.10639829188585281, 0.03023124858736992, 0.002748295199126005, 0.5508369207382202, 0.006281817797571421, 0.17451675236225128, 0.019434373825788498, 0.02198636159300804, 0.026230864226818085, 0.9530547261238098, 0.13689228892326355, 0.02840004302561283, 0.10890088230371475, 0.041067689657211304, 0.027242247015237808, 0.16672255098819733, 0.09657377004623413, 0.12756182253360748, 0.16958299279212952, 0.09718672186136246, 0.9658023118972778, 0.9611797332763672, 0.018135467544198036, 0.0585351400077343, 0.016724325716495514, 0.03344865143299103, 0.016724325716495514, 0.016724325716495514, 0.8529406189918518, 0.008362162858247757, 0.04604697972536087, 0.009867209941148758, 0.10011106729507446, 0.07626530528068542, 0.013978547416627407, 0.06023109331727028, 0.3404187262058258, 0.07914324104785919, 0.09990549832582474, 0.1741151362657547, 0.21946708858013153, 0.013025924563407898, 0.08670791983604431, 0.0894709974527359, 0.04078824818134308, 0.24591365456581116, 0.13420648872852325, 0.10604944080114365, 0.03763044625520706, 0.02684129774570465, 0.9516379237174988, 0.011442071758210659, 0.0056042796932160854, 0.35306963324546814, 0.2444867044687271, 0.01424421090632677, 0.04086454212665558, 0.03012300468981266, 0.052540123462677, 0.06211410090327263, 0.18564176559448242, 0.022730059921741486, 0.9546625018119812, 0.0031043721828609705, 0.31745579838752747, 0.00580382626503706, 0.01349727064371109, 0.5293629765510559, 0.0036442631389945745, 0.06789126992225647, 0.03023388609290123, 0.005398908164352179, 0.023485250771045685, 0.020048918202519417, 0.05535774305462837, 0.02629898674786091, 0.15763157606124878, 0.13311833143234253, 0.016639791429042816, 0.5070672035217285, 0.02784121222794056, 0.020292427390813828, 0.03571467474102974, 0.019231673330068588, 0.00727684935554862, 0.023649759590625763, 0.535628080368042, 0.2406558096408844, 0.006757074501365423, 0.13462170958518982, 0.015333360992372036, 0.005457636900246143, 0.011435048654675484, 0.9379355907440186, 0.014999664388597012, 0.9749781489372253, 0.031188877299427986, 0.1502821147441864, 0.03016781248152256, 0.025155315175652504, 0.035551607608795166, 0.23363810777664185, 0.05058910325169563, 0.1838844269514084, 0.07184580713510513, 0.18778303265571594, 0.004688366316258907, 0.01437086146324873, 0.02252454124391079, 0.15267765522003174, 0.129643514752388, 0.035468507558107376, 0.023034146055579185, 0.1742849200963974, 0.4021802842617035, 0.04107416421175003, 0.04808489978313446, 0.02105678617954254, 0.11911226809024811, 0.026399552822113037, 0.011314094066619873, 0.05908471345901489, 0.03048519790172577, 0.035513684153556824, 0.21245354413986206, 0.436221182346344, 0.02733711525797844, 0.12671557068824768, 0.0984136164188385, 0.0791168287396431, 0.004180970601737499, 0.06400100886821747, 0.02090485207736492, 0.475665807723999, 0.02830195426940918, 0.07557908445596695, 0.038721565157175064, 0.10325751453638077, 0.038721565157175064, 0.011293790303170681, 0.09196372330188751, 0.041948363184928894, 0.004840195644646883, 0.012907189317047596, 0.0016133986646309495, 0.6518130302429199, 0.9740536212921143, 0.7159518003463745, 0.018993325531482697, 0.04893195629119873, 0.013520671986043453, 0.009335702285170555, 0.10108312219381332, 0.015452196821570396, 0.021890612319111824, 0.024465978145599365, 0.030582472681999207, 0.0023444388061761856, 0.07736647874116898, 0.0023444388061761856, 0.004688877612352371, 0.014066632837057114, 0.0023444388061761856, 0.039855457842350006, 0.7971091866493225, 0.060955408960580826, 0.006228928454220295, 0.010259411297738552, 0.029679011553525925, 0.10369333624839783, 0.052029870450496674, 0.004763298202306032, 0.47779545187950134, 0.2139820009469986, 0.09086906909942627, 0.010259411297738552, 0.9549008011817932, 0.01565411128103733, 0.9261988401412964, 0.018158892169594765, 0.0038766174111515284, 0.10344447195529938, 0.373787522315979, 0.01040565688163042, 0.018158892169594765, 0.03692987933754921, 0.017342761158943176, 0.3374697268009186, 0.08018477261066437, 0.9470762610435486, 0.045431703329086304, 0.01289241760969162, 0.0019460252951830626, 0.07127317786216736, 0.2091977298259735, 0.0029190380591899157, 0.005838076118379831, 0.025298329070210457, 0.008270607329905033, 0.5857536196708679, 0.07662475109100342, 0.9726715683937073, 0.003400949528440833, 0.003400949528440833, 0.013603798113763332, 0.003400949528440833, 0.09850814193487167, 0.7109119296073914, 0.0020666043274104595, 0.0020666043274104595, 0.15706193447113037, 0.0013777363346889615, 0.019288308918476105, 0.004133208654820919, 0.002755472669377923, 0.0020666043274104595, 0.00490756006911397, 0.002453780034556985, 0.0012268900172784925, 0.003680670168250799, 0.3361678719520569, 0.0012268900172784925, 0.6060836911201477, 0.003680670168250799, 0.03803358972072601, 0.0012268900172784925, 0.0364086776971817, 0.9102169275283813, 0.0038906713016331196, 0.011672013439238071, 0.8676196932792664, 0.08559476584196091, 0.015562685206532478, 0.011672013439238071, 0.0725395604968071, 0.5453900098800659, 0.033583130687475204, 0.0949283167719841, 0.1620945781469345, 0.01746322773396969, 0.02977704256772995, 0.017239339649677277, 0.014552689157426357, 0.012537701986730099, 0.983046293258667, 0.9407440423965454, 0.03484237194061279, 0.9436436891555786, 0.0028457483276724815, 0.9305596947669983, 0.0028457483276724815, 0.045531973242759705, 0.005691496655344963, 0.0028457483276724815, 0.005691496655344963, 0.0033902523573487997, 0.0033902523573487997, 0.9730024337768555, 0.0033902523573487997, 0.013561009429395199, 0.0033902523573487997, 0.0033902523573487997, 0.0033118599094450474, 0.9538156986236572, 0.0033118599094450474, 0.0033118599094450474, 0.02318301983177662, 0.0033118599094450474, 0.006623719818890095, 0.9761987328529358, 0.966835618019104, 0.012327454052865505, 0.006163727026432753, 0.9245590567588806, 0.03081863559782505, 0.012327454052865505, 0.012327454052865505, 0.03130202367901802, 0.005443830043077469, 0.03810681030154228, 0.08165745437145233, 0.782550573348999, 0.0013609575107693672, 0.00952670257538557, 0.044911596924066544, 0.0013609575107693672, 0.005443830043077469, 0.015954650938510895, 0.03190930187702179, 0.015954650938510895, 0.9094151258468628, 0.0030501610599458218, 0.0030501610599458218, 0.0030501610599458218, 0.0061003221198916435, 0.13115692138671875, 0.024401288479566574, 0.8296438455581665, 0.008034750819206238, 0.007532578427344561, 0.005021718796342611, 0.015065156854689121, 0.6729103326797485, 0.0025108593981713057, 0.2671554386615753, 0.0035152032505720854, 0.017073845490813255, 0.0010043438524007797, 0.005021743476390839, 0.010043486952781677, 0.9591529965400696, 0.005021743476390839, 0.005021743476390839, 0.010043486952781677, 0.9655318260192871, 0.9687427878379822, 0.08913277834653854, 0.04070968180894852, 0.12212904542684555, 0.3578166961669922, 0.04285229742527008, 0.0818478912115097, 0.06792089343070984, 0.08334771543741226, 0.0606360025703907, 0.05377963185310364, 0.011060723103582859, 0.007373815402388573, 0.011060723103582859, 0.05899052321910858, 0.8664233088493347, 0.014747630804777145, 0.007373815402388573, 0.018434537574648857, 0.010879352688789368, 0.9791417121887207, 0.04969588294625282, 0.01915651373565197, 0.13812124729156494, 0.3612668514251709, 0.03838243708014488, 0.024015050381422043, 0.10001643747091293, 0.07732012867927551, 0.059343550354242325, 0.13277685642242432, 0.0023412746377289295, 0.02341274730861187, 0.9528988003730774, 0.0023412746377289295, 0.016388922929763794, 0.0023412746377289295, 0.5226292014122009, 0.013004627078771591, 0.014480329118669033, 0.23634295165538788, 0.013742477633059025, 0.056215036660432816, 0.04879041016101837, 0.05058892071247101, 0.013419668190181255, 0.030805286020040512, 0.033917542546987534, 0.033917542546987534, 0.9157736301422119, 0.9528658390045166, 0.9318336844444275, 0.014262760058045387, 0.004754253197461367, 0.028525520116090775, 0.014262760058045387, 0.04980183020234108, 0.07262767106294632, 0.0749102532863617, 0.06474237889051437, 0.01307298056781292, 0.15687575936317444, 0.05685708671808243, 0.17887157201766968, 0.08653067797422409, 0.2456890195608139, 0.10876046866178513, 0.1325821429491043, 0.23282717168331146, 0.035247448831796646, 0.06909362226724625, 0.0781480073928833, 0.0754532516002655, 0.06402747333049774, 0.1429300159215927, 0.060793761163949966, 0.03680054470896721, 0.004600068088620901, 0.004600068088620901, 0.027600407600402832, 0.018400272354483604, 0.8694128394126892, 0.018400272354483604, 0.023000339046120644, 0.00939095113426447, 0.06630580872297287, 0.041832420974969864, 0.03130317106842995, 0.18611158430576324, 0.3921433687210083, 0.0774042010307312, 0.01280584279447794, 0.1596461683511734, 0.02276594191789627, 0.5440015196800232, 0.008977401070296764, 0.09353872388601303, 0.07457035034894943, 0.013031710870563984, 0.04952050372958183, 0.03236208111047745, 0.038733139634132385, 0.07160200923681259, 0.07370156794786453, 0.044389598071575165, 0.01059417612850666, 0.17024840414524078, 0.07659589499235153, 0.009746641851961613, 0.03866874426603317, 0.0284983329474926, 0.019705167040228844, 0.5798192620277405, 0.021718060597777367, 0.005093615036457777, 0.0033957434352487326, 0.08149784058332443, 0.13073612749576569, 0.050936151295900345, 0.0016978717176243663, 0.6825444102287292, 0.008489358238875866, 0.01358297374099493, 0.022072331979870796, 0.48447272181510925, 0.011452825739979744, 0.03363131359219551, 0.05126503109931946, 0.0070898449048399925, 0.2117863893508911, 0.006908053997904062, 0.03708534315228462, 0.06180890277028084, 0.09453126043081284, 0.03348637744784355, 0.03348637744784355, 0.9376185536384583, 0.9631587266921997, 0.019072450697422028, 0.9666882157325745, 0.9607496857643127, 0.025966206565499306, 0.01652846485376358, 0.018364960327744484, 0.020201455801725388, 0.12488172948360443, 0.034893423318862915, 0.031220432370901108, 0.018364960327744484, 0.03856641799211502, 0.6684845685958862, 0.02754743956029415, 0.0028354909736663103, 0.0028354909736663103, 0.01984843611717224, 0.0028354909736663103, 0.028354909271001816, 0.9385474920272827, 0.0028354909736663103, 0.015488479286432266, 0.9757742285728455, 0.9612165689468384, 0.9497260451316833, 0.9446523189544678, 0.024745291098952293, 0.024745291098952293, 0.9403210878372192, 0.0023470791056752205, 0.001564719364978373, 0.6790882349014282, 0.04615922272205353, 0.003129438729956746, 0.1032714769244194, 0.003911798354238272, 0.09779496490955353, 0.045376863330602646, 0.0172119140625, 0.9725187420845032, 0.9507479667663574, 0.01846708171069622, 0.01846708171069622, 0.9418211579322815, 0.01846708171069622, 0.9734087586402893, 0.8398411870002747, 0.011949718929827213, 0.008215432055294514, 0.012323147617280483, 0.006348288152366877, 0.013070004992187023, 0.013816862367093563, 0.041824016720056534, 0.0022405723575502634, 0.05003944784402847, 0.019851379096508026, 0.9661004543304443, 0.013745383359491825, 0.004581794608384371, 0.02749076671898365, 0.009163589216768742, 0.036654356867074966, 0.009163589216768742, 0.004581794608384371, 0.004581794608384371, 0.8888681530952454, 0.952881932258606, 0.7006567120552063, 0.002769394079223275, 0.031017214059829712, 0.018831878900527954, 0.004431030713021755, 0.04486418515443802, 0.024370668455958366, 0.15231667459011078, 0.00553878815844655, 0.014954728074371815, 0.00862213782966137, 0.01724427565932274, 0.00862213782966137, 0.025866413488984108, 0.9311909079551697, 0.9671053886413574, 0.004421718884259462, 0.004421718884259462, 0.004421718884259462, 0.9595129489898682, 0.01768687553703785, 0.004421718884259462, 0.34819358587265015, 0.04321029409766197, 0.03281904011964798, 0.10192087292671204, 0.06433917582035065, 0.11975919455289841, 0.06399279832839966, 0.0978509709239006, 0.065724678337574, 0.06226092576980591, 0.2388421595096588, 0.0022746871691197157, 0.0022746871691197157, 0.7506467700004578, 0.0022746871691197157, 0.22261402010917664, 0.03577110916376114, 0.04524892568588257, 0.028204144909977913, 0.04956744611263275, 0.03366917371749878, 0.3578639626502991, 0.01494284626096487, 0.1946391761302948, 0.017465168610215187, 0.9719599485397339, 0.9794300198554993, 0.0036826725117862225, 0.1436242312192917, 0.01473069004714489, 0.007365345023572445, 0.8101879358291626, 0.007365345023572445, 0.007365345023572445, 0.9753866791725159, 0.004182567819952965, 0.00836513563990593, 0.13802474737167358, 0.00836513563990593, 0.8239659070968628, 0.004182567819952965, 0.012547703459858894, 0.9760980606079102, 0.003401038469746709, 0.003401038469746709, 0.013604153878986835, 0.003401038469746709, 0.0012472789967432618, 0.9254810214042664, 0.0012472789967432618, 0.0012472789967432618, 0.03991292789578438, 0.0012472789967432618, 0.008730952627956867, 0.017461905255913734, 0.0012472789967432618, 0.0037418371066451073, 0.9611403346061707, 0.9774588942527771, 0.9785345196723938, 0.9565927386283875, 0.0581524483859539, 0.019384149461984634, 0.045229680836200714, 0.012922766618430614, 0.8529025912284851, 0.9496840834617615, 0.014734303578734398, 0.04004708305001259, 0.6237521767616272, 0.015112105756998062, 0.005667039658874273, 0.1530100703239441, 0.00982286874204874, 0.05667039752006531, 0.04004708305001259, 0.040424883365631104, 0.9789637327194214, 0.9889770150184631, 0.0052051423117518425, 0.01813979633152485, 0.9614092111587524, 0.003508792258799076, 0.016374364495277405, 0.08070222288370132, 0.03157912939786911, 0.06432785838842392, 0.003508792258799076, 0.09824617952108383, 0.0023391947615891695, 0.6947408318519592, 0.003508792258799076, 0.9821716547012329, 0.9724412560462952, 0.01246719527989626, 0.9848859310150146, 0.013256611302495003, 0.9677326083183289, 0.003146008588373661, 0.009438025765120983, 0.9500945806503296, 0.003146008588373661, 0.006292017176747322, 0.012584034353494644, 0.006292017176747322, 0.006292017176747322, 0.972756028175354, 0.01945512183010578, 0.9750964045524597, 0.016812007874250412, 0.01609218679368496, 0.9655312299728394, 0.9528312683105469, 0.13029319047927856, 0.063480444252491, 0.3533911108970642, 0.09463750571012497, 0.06831228733062744, 0.03415614366531372, 0.07197782397270203, 0.034822605550289154, 0.11446473002433777, 0.034655988216400146, 0.919175922870636, 0.04377027973532677, 0.00867136288434267, 0.3570840358734131, 0.030011046677827835, 0.05832846835255623, 0.2590569853782654, 0.013278025202453136, 0.10940821468830109, 0.020188016816973686, 0.024726934731006622, 0.11909575760364532, 0.09042303264141083, 0.6033898591995239, 0.0064640953205525875, 0.005349596031010151, 0.24028602242469788, 0.005423896014690399, 0.016717487946152687, 0.016717487946152687, 0.007355694659054279, 0.007875794544816017, 0.036021679639816284, 0.8274391889572144, 0.0021189223043620586, 0.004237844608724117, 0.10806503891944885, 0.0021189223043620586, 0.009535150602459908, 0.0021189223043620586, 0.0021189223043620586, 0.004237844608724117, 0.01805928163230419, 0.7204702496528625, 0.010265275835990906, 0.015017718076705933, 0.1642444133758545, 0.004372247029095888, 0.034407682716846466, 0.0026613676454871893, 0.003992051351815462, 0.026423579081892967, 0.9737566709518433, 0.01094108633697033, 0.9638146758079529, 0.008517478592693806, 0.0026084778364747763, 0.27218666672706604, 0.09081761538982391, 0.005855766590684652, 0.079798124730587, 0.07303737848997116, 0.03194054216146469, 0.23258039355278015, 0.20266275107860565, 0.006605897564440966, 0.977672815322876, 0.006605897564440966, 0.006605897564440966, 0.03983510285615921, 0.029876327142119408, 0.009958775714039803, 0.9062486290931702, 0.9694199562072754, 0.9836559295654297, 0.007566583808511496, 0.008238471113145351, 0.9803780913352966, 0.9678521156311035, 0.20680242776870728, 0.07509136945009232, 0.05521424114704132, 0.07870539277791977, 0.042163603007793427, 0.4286632835865021, 0.022487254813313484, 0.027908289805054665, 0.029916079714894295, 0.03312854468822479, 0.17551082372665405, 0.5143427848815918, 0.008470798842608929, 0.0070245652459561825, 0.15288759768009186, 0.013945828191936016, 0.046899303793907166, 0.043490324169397354, 0.009193915873765945, 0.02820156328380108, 0.9516566395759583, 0.027584251016378403, 0.015274545177817345, 0.9622963070869446, 0.007637272588908672, 0.048968590795993805, 0.10335373878479004, 0.12619110941886902, 0.024447698146104813, 0.01017434149980545, 0.24140271544456482, 0.018738355487585068, 0.3076457381248474, 0.049554165452718735, 0.06961005926132202, 0.020368510857224464, 0.007451894227415323, 0.06309270858764648, 0.2341550886631012, 0.025170844048261642, 0.012751019559800625, 0.3482518792152405, 0.05183206498622894, 0.039909034967422485, 0.19689561426639557, 0.013850362040102482, 0.007254951633512974, 0.1859905868768692, 0.1431204080581665, 0.01319082174450159, 0.0046167876571416855, 0.5619289875030518, 0.00791449286043644, 0.014509903267025948, 0.04814649745821953, 0.016413994133472443, 0.0032827986869961023, 0.00984839629381895, 0.12474635243415833, 0.006565597373992205, 0.8272652626037598, 0.0032827986869961023, 0.0032827986869961023, 0.0032827986869961023, 0.006565597373992205, 0.009550380520522594, 0.0026860444340854883, 0.02954648993909359, 0.010445728898048401, 0.008058133535087109, 0.013728671707212925, 0.20891457796096802, 0.005073639564216137, 0.035217028111219406, 0.6771816611289978, 0.015249309130012989, 0.03558172285556793, 0.010166206397116184, 0.030498618260025978, 0.005083103198558092, 0.005083103198558092, 0.884459912776947, 0.015249309130012989, 0.011916732415556908, 0.0801384449005127, 0.008764157071709633, 0.005107170902192593, 0.6305779814720154, 0.21872562170028687, 0.01298860739916563, 0.0020176477264612913, 0.026922987774014473, 0.002774265594780445, 0.9765690565109253, 0.008276009000837803, 0.0041380045004189014, 0.0041380045004189014, 0.0041380045004189014, 0.06258092820644379, 0.00924407597631216, 0.1221538633108139, 0.3214884400367737, 0.01335255429148674, 0.13616670668125153, 0.019808735698461533, 0.14387010037899017, 0.1310311108827591, 0.04027776047587395, 0.06774391233921051, 0.03180000185966492, 0.07585155963897705, 0.03549348562955856, 0.008017564192414284, 0.4218679964542389, 0.023602265864610672, 0.22052805125713348, 0.05765439197421074, 0.05738413706421852, 0.09019362181425095, 0.01170983724296093, 0.10622494667768478, 0.030250411480665207, 0.007945960387587547, 0.34669482707977295, 0.01644953340291977, 0.201715886592865, 0.09632734954357147, 0.09242407232522964, 0.012107075192034245, 0.03632122650742531, 0.02421415038406849, 0.9201377630233765, 0.19742479920387268, 0.22203709185123444, 0.024546483531594276, 0.03612873703241348, 0.29666367173194885, 0.01776823215186596, 0.05521313473582268, 0.06324174255132675, 0.0597539059817791, 0.027244621887803078, 0.006643733009696007, 0.9367663264274597, 0.01993119902908802, 0.026574932038784027, 0.006643733009696007, 0.9804120063781738, 0.012691417708992958, 0.0017030646558851004, 0.0017030646558851004, 0.9758560657501221, 0.0017030646558851004, 0.0017030646558851004, 0.011921452358365059, 0.0017030646558851004, 0.0017030646558851004, 0.14806264638900757, 0.05757991969585419, 0.06720574200153351, 0.11620993167161942, 0.04445379972457886, 0.3388289511203766, 0.029752543196082115, 0.08715745061635971, 0.08418219536542892, 0.0264272578060627, 0.14771826565265656, 0.003907890524715185, 0.03282628208398819, 0.016022350639104843, 0.0023447342682629824, 0.15318931639194489, 0.004298679530620575, 0.4165811240673065, 0.18445242941379547, 0.03829732909798622, 0.045492567121982574, 0.13216374814510345, 0.0019608864095062017, 0.012549673207104206, 0.0031374183017760515, 0.6486612558364868, 0.016471445560455322, 0.13098721206188202, 0.005490481853485107, 0.0027452409267425537, 0.022080153226852417, 0.9715268015861511, 0.008273321203887463, 0.008886159397661686, 0.0297226719558239, 0.008886159397661686, 0.5129458904266357, 0.282518595457077, 0.013788868673145771, 0.004289870150387287, 0.10908526927232742, 0.021142931655049324, 0.9708428382873535, 0.09357298910617828, 0.003043023869395256, 0.09357298910617828, 0.001521511934697628, 0.002282267902046442, 0.03195175155997276, 0.004564535804092884, 0.7615167498588562, 0.000760755967348814, 0.005325291771441698, 0.039999138563871384, 0.008571243844926357, 0.09714076668024063, 0.005714162718504667, 0.0028570813592523336, 0.03428497537970543, 0.0028570813592523336, 0.8056969046592712, 0.0028570813592523336, 0.968172550201416, 0.9269487261772156, 0.2960509657859802, 0.0032259600702673197, 0.21457242965698242, 0.03465602919459343, 0.023411253467202187, 0.006544090341776609, 0.3127337694168091, 0.06829818338155746, 0.023042572662234306, 0.01742018386721611, 0.06385068595409393, 0.13370075821876526, 0.021426403895020485, 0.07970622181892395, 0.047138091176748276, 0.022711988538503647, 0.09727587550878525, 0.4589535892009735, 0.059136874973773956, 0.016712594777345657, 0.971871554851532, 0.020252350717782974, 0.34855660796165466, 0.07224854826927185, 0.028558088466525078, 0.23915977776050568, 0.12140031903982162, 0.05580773949623108, 0.012287942692637444, 0.08442271500825882, 0.01729414239525795, 0.03501589223742485, 0.9454291462898254, 0.984886646270752, 0.017621546983718872, 0.017621546983718872, 0.011747697368264198, 0.005873848684132099, 0.013705647550523281, 0.0019579497165977955, 0.0019579497165977955, 0.9261101484298706, 0.003915899433195591, 0.9715385437011719, 0.9553367495536804, 0.01425875723361969, 0.01425875723361969, 0.01638653129339218, 0.01638653129339218, 0.9668053984642029, 0.9452676177024841, 0.035447534173727036, 0.9660614132881165, 0.01207576785236597, 0.39031168818473816, 0.03743806108832359, 0.030534587800502777, 0.0878865122795105, 0.04726223275065422, 0.1525401771068573, 0.09425894916057587, 0.05230707675218582, 0.07952269166707993, 0.028012165799736977, 0.9739465713500977, 0.1787204146385193, 0.27644437551498413, 0.030080199241638184, 0.016727525740861893, 0.06162772700190544, 0.10080534964799881, 0.02685208059847355, 0.22787585854530334, 0.02905307151377201, 0.05164990574121475, 0.9473745226860046, 0.23858875036239624, 0.29201433062553406, 0.003038259921595454, 0.02171161025762558, 0.36148467659950256, 0.006417897529900074, 0.025603314861655235, 0.045642174780368805, 0.0034820507280528545, 0.0019799896981567144, 0.2047199308872223, 0.19579361379146576, 0.005706988740712404, 0.020779291167855263, 0.514506995677948, 0.0026339946780353785, 0.03190060332417488, 0.017559964209794998, 0.0029266609344631433, 0.003365660086274147, 0.964113712310791, 0.01506427675485611, 0.024007132276892662, 0.05796007812023163, 0.459565132856369, 0.08539680391550064, 0.048700183629989624, 0.07853762060403824, 0.03703957796096802, 0.052472732961177826, 0.06344742327928543, 0.09294190257787704, 0.06518938392400742, 0.050609391182661057, 0.01261729933321476, 0.015701528638601303, 0.0025234599597752094, 0.8319006562232971, 0.0033646132797002792, 0.003925382159650326, 0.01261729933321476, 0.0014019221998751163, 0.010821555741131306, 0.02164311148226261, 0.9631184339523315, 0.021222710609436035, 0.9550219774246216, 0.9575124979019165, 0.9646611213684082, 0.013035961426794529, 0.013035961426794529, 0.14872893691062927, 0.5025283098220825, 0.01078205555677414, 0.011521999724209309, 0.20485904812812805, 0.02547524869441986, 0.04175403714179993, 0.03033774346113205, 0.01078205555677414, 0.013319008983671665, 0.00958263874053955, 0.028747916221618652, 0.00958263874053955, 0.00958263874053955, 0.9199333190917969, 0.009265025146305561, 0.737341582775116, 0.02239047735929489, 0.018530050292611122, 0.039376355707645416, 0.006176683120429516, 0.07875271141529083, 0.006176683120429516, 0.03011133149266243, 0.05250180885195732, 0.017581261694431305, 0.35240086913108826, 0.07006649672985077, 0.040161117911338806, 0.20054703950881958, 0.005946603137999773, 0.1786566525697708, 0.016891799867153168, 0.05377797782421112, 0.06403371691703796, 0.011924795806407928, 0.009414312429726124, 0.06715542823076248, 0.12866227328777313, 0.05679968371987343, 0.03263628110289574, 0.46161511540412903, 0.021652918308973312, 0.12677940726280212, 0.0834735706448555, 0.9334546327590942, 0.066917285323143, 0.05866977944970131, 0.13908298313617706, 0.20187652111053467, 0.03617657348513603, 0.07197825610637665, 0.04011288285255432, 0.030178384855389595, 0.25604766607284546, 0.09934499114751816, 0.22891990840435028, 0.34320497512817383, 0.030079619958996773, 0.041534360498189926, 0.23906302452087402, 0.0034976303577423096, 0.06085876747965813, 0.029992179945111275, 0.010318009182810783, 0.0125914691016078, 0.008967694826424122, 0.03552138805389404, 0.5811532139778137, 0.061725690960884094, 0.016596058383584023, 0.007162509486079216, 0.02055582031607628, 0.007511900272220373, 0.23438292741775513, 0.02643723040819168, 0.02463383786380291, 0.9360858798027039, 0.6529707908630371, 0.023230422288179398, 0.013714345172047615, 0.06521311402320862, 0.02211088314652443, 0.038344189524650574, 0.012594806961715221, 0.07724814862012863, 0.01679307594895363, 0.07752803713083267, 0.05022818222641945, 0.8507398366928101, 0.0031392613891512156, 0.0784815326333046, 0.0031392613891512156, 0.006278522778302431, 0.0031392613891512156, 0.0031392613891512156, 0.9802060127258301, 0.09218337386846542, 0.884960412979126, 0.0061455583199858665, 0.0061455583199858665, 0.01272466592490673, 0.9670746326446533, 0.9439136981964111, 0.0036728158593177795, 0.014691263437271118, 0.0036728158593177795, 0.022036895155906677, 0.011018447577953339, 0.0036728158593177795, 0.9449251890182495, 0.9702438116073608, 0.04101891443133354, 0.008378331549465656, 0.059521060436964035, 0.024785896763205528, 0.0029673257376998663, 0.1728030890226364, 0.0038400685880333185, 0.3524135649204254, 0.25763368606567383, 0.07662682235240936, 0.06706921011209488, 0.06097200885415077, 0.8597053289413452, 0.5871261954307556, 0.05131474509835243, 0.014453653246164322, 0.03472297638654709, 0.06063692271709442, 0.04207808896899223, 0.027538912370800972, 0.17361488938331604, 0.0029078354127705097, 0.005644621793180704, 0.974470853805542, 0.010827453806996346, 0.9679510593414307, 0.06063703075051308, 0.6378928422927856, 0.01663687452673912, 0.020358281210064888, 0.10813968628644943, 0.0070050000213086605, 0.03502500057220459, 0.05866687744855881, 0.038965314626693726, 0.01685578189790249, 0.008439346216619015, 0.11111805588006973, 0.003516394179314375, 0.00703278835862875, 0.845341145992279, 0.00492295203730464, 0.01125246100127697, 0.0021098365541547537, 0.0014065576251596212, 0.004219673108309507, 0.004811745136976242, 0.019246980547904968, 0.9671607613563538, 0.004811745136976242, 0.004811745136976242, 0.010196247138082981, 0.020392494276165962, 0.9584472179412842, 0.9626115560531616, 0.01688792183995247, 0.01688792183995247, 0.1318425089120865, 0.46990546584129333, 0.01548691000789404, 0.011003857478499413, 0.1454954445362091, 0.04442298039793968, 0.07539679855108261, 0.06683824211359024, 0.016302010044455528, 0.023637915030121803, 0.9603651165962219, 0.6110060214996338, 0.010539306327700615, 0.06939516961574554, 0.028880435973405838, 0.06186709553003311, 0.0186148788779974, 0.014645528979599476, 0.09676998853683472, 0.024089843034744263, 0.06460458040237427, 0.00940345786511898, 0.9826613068580627, 0.001226305030286312, 0.015941964462399483, 0.001226305030286312, 0.001226305030286312, 0.7198410034179688, 0.1900772750377655, 0.06867308169603348, 0.001226305030286312, 0.001226305030286312, 0.02578933723270893, 0.9542055130004883, 0.06160150468349457, 0.012665729969739914, 0.10478013008832932, 0.5808964371681213, 0.020725740119814873, 0.050087202340364456, 0.03799718990921974, 0.05296577885746956, 0.05181434750556946, 0.02648288942873478, 0.15457704663276672, 0.0892077162861824, 0.029239272698760033, 0.047118064016103745, 0.023279674351215363, 0.09814710915088654, 0.039296090602874756, 0.4156818687915802, 0.02812184765934944, 0.07523991167545319, 0.9396630525588989, 0.8879942893981934, 0.026706594973802567, 0.0300449188798666, 0.010014972649514675, 0.003338324371725321, 0.003338324371725321, 0.013353297486901283, 0.023368271067738533, 0.9865592122077942, 0.9704145789146423, 0.013962799683213234, 0.006981399841606617, 0.0034906999208033085, 0.010843687690794468, 0.9759318828582764, 0.010843053460121155, 0.9758747816085815, 0.01495631318539381, 0.004985437728464603, 0.9223059415817261, 0.004985437728464603, 0.02991262637078762, 0.01495631318539381, 0.003414577804505825, 0.003414577804505825, 0.003414577804505825, 0.9799838662147522, 0.003414577804505825, 0.00682915560901165, 0.9414123296737671, 0.9817818999290466, 0.9340736269950867, 0.022969024255871773, 0.022969024255871773, 0.006647716276347637, 0.9439757466316223, 0.019943149760365486, 0.019943149760365486, 0.006647716276347637, 0.006647716276347637, 0.006521314848214388, 0.9390693306922913, 0.01956394501030445, 0.01956394501030445, 0.006521314848214388, 0.006521314848214388, 0.6073790192604065, 0.009759155102074146, 0.08346646279096603, 0.04083435982465744, 0.07190956920385361, 0.025681987404823303, 0.038522981107234955, 0.05675719305872917, 0.051877617835998535, 0.013868273235857487, 0.0785883367061615, 0.020224131643772125, 0.31171542406082153, 0.054297398775815964, 0.01681680604815483, 0.11035341769456863, 0.038689643144607544, 0.11848703026771545, 0.12947840988636017, 0.12145470082759857, 0.008544613607227802, 0.03702665865421295, 0.005696408916264772, 0.039874862879514694, 0.03133025020360947, 0.005696408916264772, 0.03702665865421295, 0.8117382526397705, 0.011392817832529545, 0.011392817832529545, 0.008186263032257557, 0.019442375749349594, 0.6907159686088562, 0.05832712724804878, 0.03479161858558655, 0.09209546446800232, 0.014325961470603943, 0.019442375749349594, 0.04093131795525551, 0.022512225434184074, 0.007057673297822475, 0.01411534659564495, 0.952785849571228, 0.01411534659564495, 0.007057673297822475, 0.965410590171814, 0.01609017699956894, 0.14514325559139252, 0.2215656191110611, 0.002488170051947236, 0.018839001655578613, 0.0885077640414238, 0.0045024030841887, 0.02464473247528076, 0.49028798937797546, 0.002488170051947236, 0.0015402957797050476, 0.04013871029019356, 0.01003467757254839, 0.9332250356674194, 0.971206784248352, 0.014282452873885632, 0.06139462813735008, 0.04184858500957489, 0.022553129121661186, 0.3177485167980194, 0.26462337374687195, 0.032200854271650314, 0.12441809475421906, 0.08783190697431564, 0.020297816023230553, 0.02743964083492756, 0.9503369927406311, 0.005976962391287088, 0.005976962391287088, 0.005976962391287088, 0.011953924782574177, 0.011953924782574177, 0.005976962391287088, 0.968982458114624, 0.008499846793711185, 0.008499846793711185, 0.008499846793711185, 0.0050374711863696575, 0.011334310285747051, 0.7027272582054138, 0.027706092223525047, 0.011334310285747051, 0.01637178100645542, 0.022668620571494102, 0.13349299132823944, 0.03148419409990311, 0.039040401577949524, 0.24760468304157257, 0.2357475608587265, 0.011740880087018013, 0.08962592482566833, 0.23493383824825287, 0.005928562954068184, 0.12519730627536774, 0.02510920725762844, 0.007788504473865032, 0.016274485737085342, 0.9007519483566284, 0.010064267553389072, 0.0010064267553389072, 0.0010064267553389072, 0.007044987287372351, 0.025160670280456543, 0.0432763509452343, 0.0060385605320334435, 0.004025707021355629, 0.006232683081179857, 0.009349023923277855, 0.05609414726495743, 0.0031163415405899286, 0.021814391016960144, 0.01869804784655571, 0.06544317305088043, 0.006232683081179857, 0.01869804784655571, 0.7946670651435852, 0.021166175603866577, 0.9524779319763184, 0.027782855555415154, 0.17187237739562988, 0.04690399765968323, 0.184238463640213, 0.20880722999572754, 0.11287465691566467, 0.13711656630039215, 0.027183616533875465, 0.06155809015035629, 0.021627046167850494, 0.014149196445941925, 0.014149196445941925, 0.9621453285217285, 0.016758102923631668, 0.9552119374275208, 0.04068871960043907, 0.020344359800219536, 0.9358405470848083, 0.27967920899391174, 0.05369318649172783, 0.007119538262486458, 0.13823769986629486, 0.07493314146995544, 0.007119538262486458, 0.07623838633298874, 0.33432164788246155, 0.004924347158521414, 0.023731794208288193, 0.005379603710025549, 0.01613881066441536, 0.011431657709181309, 0.16676771640777588, 0.01613881066441536, 0.019501063972711563, 0.029587820172309875, 0.7235566973686218, 0.00605205399915576, 0.00605205399915576, 0.11185908317565918, 0.16460615396499634, 0.03535377234220505, 0.0710226371884346, 0.11715269088745117, 0.08016042411327362, 0.1055571585893631, 0.17311374843120575, 0.06175881624221802, 0.07934117317199707, 0.027125749737024307, 0.9494011998176575, 0.9648112058639526, 0.09179665893316269, 0.007447347976267338, 0.04759826883673668, 0.06573094427585602, 0.024446729570627213, 0.21354460716247559, 0.021370651200413704, 0.17306989431381226, 0.22034436464309692, 0.1345379650592804, 0.04027805104851723, 0.06981529295444489, 0.10942204296588898, 0.5430824160575867, 0.05415160208940506, 0.04072558507323265, 0.03915921598672867, 0.035131413489580154, 0.04520092532038689, 0.022824229672551155, 0.011624722741544247, 0.005812361370772123, 0.017437083646655083, 0.005812361370772123, 0.11043485999107361, 0.005812361370772123, 0.8369799852371216, 0.004903214052319527, 0.004903214052319527, 0.009806428104639053, 0.004903214052319527, 0.9659331440925598, 0.025103474035859108, 0.19138024747371674, 0.0661328062415123, 0.03131185844540596, 0.5833183526992798, 0.011067123152315617, 0.007288105320185423, 0.010527263395488262, 0.0361705981194973, 0.038060106337070465, 0.012501134537160397, 0.20939400792121887, 0.004687925800681114, 0.004687925800681114, 0.6698524951934814, 0.008854970335960388, 0.012501134537160397, 0.005729686934500933, 0.03281547874212265, 0.039066046476364136, 0.04575992748141289, 0.9266385436058044, 0.0035528920125216246, 0.007105784025043249, 0.007105784025043249, 0.0035528920125216246, 0.0035528920125216246, 0.014211568050086498, 0.03197602927684784, 0.0035528920125216246, 0.9308577179908752, 0.1341007798910141, 0.057643767446279526, 0.027843596413731575, 0.09000254422426224, 0.06562057882547379, 0.07946712523698807, 0.03777698799967766, 0.21055778861045837, 0.09602277725934982, 0.2007748931646347, 0.47375354170799255, 0.03414911404252052, 0.07293114811182022, 0.06426309794187546, 0.025929413735866547, 0.09871111065149307, 0.05753789097070694, 0.08436399698257446, 0.02988981455564499, 0.05835986137390137, 0.010397917591035366, 0.00891250092536211, 0.01435902900993824, 0.18840037286281586, 0.025994794443249702, 0.6736365556716919, 0.04084896296262741, 0.011264410801231861, 0.006808160338550806, 0.019310418516397476, 0.9463045597076416, 0.0056160506792366505, 0.0028080253396183252, 0.0056160506792366505, 0.028080254793167114, 0.0056160506792366505, 0.0028080253396183252, 0.0028080253396183252, 0.10321817547082901, 0.18933162093162537, 0.06340544670820236, 0.03465181589126587, 0.04276181384921074, 0.062225814908742905, 0.057212360203266144, 0.34858250617980957, 0.056475088000297546, 0.04231945052742958, 0.1329655647277832, 0.02607729658484459, 0.13411180675029755, 0.07966470718383789, 0.012178957462310791, 0.27151909470558167, 0.022495251148939133, 0.14227887988090515, 0.10803451389074326, 0.07078123092651367, 0.9658600687980652, 0.6987872123718262, 0.06933842599391937, 0.0036019959952682257, 0.012787085957825184, 0.12246786057949066, 0.006843792274594307, 0.015668682754039764, 0.06411553174257278, 0.001260698540136218, 0.005042794160544872, 0.018781449645757675, 0.9578539133071899, 0.7276945114135742, 0.001051581697538495, 0.004907381255179644, 0.12513822317123413, 0.010515816509723663, 0.018227415159344673, 0.008062126114964485, 0.02523796074092388, 0.0028042178601026535, 0.07641493529081345, 0.02829567715525627, 0.9479051828384399, 0.9557495713233948, 0.01917433924973011, 0.01917433924973011, 0.01917433924973011, 0.9395426511764526, 0.039889562875032425, 0.0012867600889876485, 0.06562476605176926, 0.778489887714386, 0.010294080711901188, 0.0038602803833782673, 0.0630512461066246, 0.0038602803833782673, 0.007720560766756535, 0.028308723121881485, 0.18360693752765656, 0.017768412828445435, 0.010661047883331776, 0.018952973186969757, 0.027837179601192474, 0.17294588685035706, 0.0035536824725568295, 0.5591127276420593, 0.0035536824725568295, 0.0017768412362784147, 0.9751810431480408, 0.1510879546403885, 0.09153503179550171, 0.041466474533081055, 0.07366915792226791, 0.11623846739530563, 0.03440834954380989, 0.09440239518880844, 0.34783312678337097, 0.02360059879720211, 0.025806263089179993, 0.004196159075945616, 0.02517695538699627, 0.008392318151891232, 0.9567242860794067, 0.9569887518882751, 0.21317051351070404, 0.06805259734392166, 0.08411875367164612, 0.12186770141124725, 0.03239355608820915, 0.28004753589630127, 0.03187108039855957, 0.06635454297065735, 0.042320616543293, 0.05969296768307686, 0.07821499556303024, 0.17071926593780518, 0.020556505769491196, 0.061168137937784195, 0.1682959347963333, 0.05740780010819435, 0.142474964261055, 0.1360406130552292, 0.08322877436876297, 0.08189176768064499, 0.9671977758407593, 0.011271954514086246, 0.007591316010802984, 0.0809740424156189, 0.3694440424442291, 0.020703589543700218, 0.035196103155612946, 0.1325029730796814, 0.06142064929008484, 0.20427541434764862, 0.07614320516586304, 0.06023859232664108, 0.07228631526231766, 0.012047719210386276, 0.006023859605193138, 0.03614315763115883, 0.8011733293533325, 0.012047719210386276, 0.006023859605193138, 0.0050621479749679565, 0.017717517912387848, 0.0025310739874839783, 0.9618080854415894, 0.0025310739874839783, 0.007593221962451935, 0.9625566601753235, 0.025141404941678047, 0.007183258421719074, 0.9618396759033203, 0.01502874493598938, 0.16928094625473022, 0.027382243424654007, 0.2886870801448822, 0.07060707360506058, 0.011735247448086739, 0.06180563569068909, 0.21015872061252594, 0.0526130273938179, 0.07667028158903122, 0.031000612303614616, 0.0430724173784256, 0.44735610485076904, 0.1478108912706375, 0.05066340044140816, 0.12299094349145889, 0.030619798228144646, 0.05313686281442642, 0.021493563428521156, 0.040769536048173904, 0.04221949726343155, 0.039985015988349915, 0.4994112253189087, 0.09458302706480026, 0.05010170489549637, 0.1289476603269577, 0.02392677590250969, 0.02617492899298668, 0.026656676083803177, 0.08093352615833282, 0.029547158628702164, 0.8023743033409119, 0.018928883597254753, 0.009464441798627377, 0.012619255110621452, 0.015774069353938103, 0.012619255110621452, 0.006309627555310726, 0.1062120646238327, 0.009464441798627377, 0.006309627555310726, 0.017725830897688866, 0.005908610764890909, 0.049238421022892, 0.011817221529781818, 0.02560397982597351, 0.007878147065639496, 0.16347156465053558, 0.01575629413127899, 0.6361603736877441, 0.06696425378322601, 0.0063508059829473495, 0.0063508059829473495, 0.0063508059829473495, 0.0063508059829473495, 0.971673309803009, 0.012303507886826992, 0.012303507886826992, 0.9719771146774292, 0.15683569014072418, 0.00347237684763968, 0.00347237684763968, 0.010417129844427109, 0.034145038574934006, 0.04108979180455208, 0.6794283986091614, 0.007523483131080866, 0.04514089971780777, 0.0190980713814497, 0.0061959815211594105, 0.012391963042318821, 0.01858794502913952, 0.0061959815211594105, 0.04337187111377716, 0.8860253691673279, 0.0061959815211594105, 0.0061959815211594105, 0.012391963042318821, 0.00640766229480505, 0.003203831147402525, 0.09291110187768936, 0.003203831147402525, 0.0128153245896101, 0.00640766229480505, 0.7945501208305359, 0.028834480792284012, 0.0512612983584404, 0.9633452296257019, 0.008037697523832321, 0.003409932367503643, 0.6871013641357422, 0.1534469574689865, 0.00852483045309782, 0.027279458940029144, 0.02922799065709114, 0.0036534988321363926, 0.03239435702562332, 0.04700835421681404, 0.07140585035085678, 0.13666565716266632, 0.10012463480234146, 0.07688141614198685, 0.10481797903776169, 0.05363819748163223, 0.31210726499557495, 0.04916834831237793, 0.033747363835573196, 0.06134868785738945, 0.0841895192861557, 0.3875652849674225, 0.11124388873577118, 0.04172792285680771, 0.13105319440364838, 0.06447193026542664, 0.034757982939481735, 0.05199941247701645, 0.05163257196545601, 0.04126937314867973, 0.03922053799033165, 0.9216826558113098, 0.9212421178817749, 0.012449217960238457, 0.012449217960238457, 0.04979687184095383, 0.05891680717468262, 0.8984813094139099, 0.014729201793670654, 0.014729201793670654, 0.017484523355960846, 0.9616487622261047, 0.07413163036108017, 0.08386141061782837, 0.011737508699297905, 0.34378543496131897, 0.3403877317905426, 0.007104281336069107, 0.07057949155569077, 0.035212524235248566, 0.009266453795135021, 0.02378389798104763, 0.10869325697422028, 0.017801959067583084, 0.02257809415459633, 0.07786548137664795, 0.2115972638130188, 0.033143483102321625, 0.025617452338337898, 0.4797844886779785, 0.009118076413869858, 0.013604748994112015, 0.9772937297821045, 0.009581310674548149, 0.04115039110183716, 0.05205879360437393, 0.1039402112364769, 0.1481059342622757, 0.05702522024512291, 0.013746359385550022, 0.30463704466819763, 0.024920819327235222, 0.06660332530736923, 0.18774865567684174, 0.9591329097747803, 0.005048067774623632, 0.03028840757906437, 0.9835885763168335, 0.006345732603222132, 0.006345732603222132, 0.027154013514518738, 0.3036765158176422, 0.08831281960010529, 0.053186990320682526, 0.024538259953260422, 0.1383858174085617, 0.026406655088067055, 0.2521088123321533, 0.04558884724974632, 0.04073102027177811, 0.00802469439804554, 0.0026748981326818466, 0.08827164024114609, 0.010699592530727386, 0.8613172173500061, 0.0026748981326818466, 0.01604938879609108, 0.005349796265363693, 0.0026748981326818466, 0.012570756487548351, 0.012570756487548351, 0.9553775191307068, 0.9677234888076782, 0.04287425056099892, 0.9346586465835571, 0.008574849925935268, 0.002432536333799362, 0.5933071970939636, 0.021545320749282837, 0.028611259534955025, 0.007065938785672188, 0.0022008661180734634, 0.23271264135837555, 0.005907588172703981, 0.019112784415483475, 0.08710796386003494, 0.03055635094642639, 0.3093830645084381, 0.02164408192038536, 0.03628566861152649, 0.15532812476158142, 0.017400145530700684, 0.32572221755981445, 0.017187947407364845, 0.031829532235860825, 0.054958995431661606, 0.0036720719654113054, 0.2544439733028412, 0.03381366282701492, 0.03105960786342621, 0.08812972158193588, 0.00413108104839921, 0.2977438271045685, 0.01147522497922182, 0.009486185386776924, 0.26607221364974976, 0.06467679142951965, 0.026124348863959312, 0.08040212094783783, 0.1308753788471222, 0.02447572536766529, 0.2472936064004898, 0.08116302639245987, 0.08560162782669067, 0.10880918800830841, 0.15040524303913116, 0.004541885573416948, 0.005367682781070471, 0.836945652961731, 0.04459305852651596, 0.007432176265865564, 0.003716088132932782, 0.009496670216321945, 0.005780581850558519, 0.0627605989575386, 0.019406238570809364, 0.015124647878110409, 0.0089372918009758, 0.6049858927726746, 0.07493575662374496, 0.04674891009926796, 0.008249808102846146, 0.08524801582098007, 0.013749679550528526, 0.07081084698438644, 0.0714983344078064, 0.9727274179458618, 0.0034011448733508587, 0.0034011448733508587, 0.010203434154391289, 0.0034011448733508587, 0.9749928712844849, 0.012828853912651539, 0.0799640342593193, 0.005711717065423727, 0.02570272609591484, 0.06282888352870941, 0.008567575365304947, 0.014279291965067387, 0.028558583930134773, 0.768225908279419, 0.0028558585327118635, 0.9641249179840088, 0.0033718105405569077, 0.0008429526351392269, 0.007586573716253042, 0.7932184338569641, 0.005900668445974588, 0.0033718105405569077, 0.05900668352842331, 0.01348724216222763, 0.0994684100151062, 0.01348724216222763, 0.30164405703544617, 0.11716167628765106, 0.04830557107925415, 0.13818463683128357, 0.04594343900680542, 0.0975559875369072, 0.10216214507818222, 0.048541780561208725, 0.049250420182943344, 0.05114012584090233, 0.9695919156074524, 0.017010383307933807, 0.9595564603805542, 0.013327172957360744, 0.013766000047326088, 0.0992601066827774, 0.1043921709060669, 0.03610556200146675, 0.007366017904132605, 0.11489780247211456, 0.014611281454563141, 0.14949393272399902, 0.38846686482429504, 0.07172811031341553, 0.9737824201583862, 0.008933783508837223, 0.007045396137982607, 0.00922202318906784, 0.08225356787443161, 0.11289817839860916, 0.009737539105117321, 0.010883131995797157, 0.00910746306180954, 0.007961870171129704, 0.7388501167297363, 0.011971445754170418, 0.005386859644204378, 0.005386859644204378, 0.09696347266435623, 0.8080289363861084, 0.07541603595018387, 0.957513689994812, 0.9654332399368286, 0.9763674736022949, 0.9614076018333435, 0.9788714051246643, 0.012390777468681335, 0.9377048015594482, 0.022870847955346107, 0.9385243058204651, 0.03476015850901604, 0.028408940881490707, 0.8380637764930725, 0.0019195231143385172, 0.0007678092224523425, 0.11018062382936478, 0.002687332220375538, 0.006526378449052572, 0.004606855567544699, 0.00307123688980937, 0.0038390462286770344, 0.07125750929117203, 0.09459183365106583, 0.2501539885997772, 0.025968845933675766, 0.023208871483802795, 0.11968250572681427, 0.08003924041986465, 0.1016172245144844, 0.11679707467556, 0.11654616892337799, 0.027727121487259865, 0.007534543983638287, 0.24321508407592773, 0.47859424352645874, 0.021699486300349236, 0.05424871668219566, 0.03224784880876541, 0.042193446308374405, 0.05274181067943573, 0.03948101028800011, 0.029188208281993866, 0.02093101665377617, 0.2212158888578415, 0.4049863815307617, 0.024003461003303528, 0.081611767411232, 0.02995631843805313, 0.08967692404985428, 0.06068074703216553, 0.03821351006627083, 0.004671163856983185, 0.985615611076355, 0.022942624986171722, 0.03392923250794411, 0.058487534523010254, 0.20712988078594208, 0.006785846780985594, 0.44204944372177124, 0.04200762137770653, 0.057194992899894714, 0.03134414926171303, 0.09855633974075317, 0.1337071806192398, 0.015679016709327698, 0.029180392622947693, 0.029398158192634583, 0.01371914055198431, 0.020905356854200363, 0.06750687956809998, 0.09189645946025848, 0.5897052884101868, 0.008057272993028164, 0.024276629090309143, 0.9467885494232178, 0.02741798758506775, 0.9596295356750488, 0.06894759088754654, 0.07750025391578674, 0.27118510007858276, 0.1701321303844452, 0.052763331681489944, 0.1014477014541626, 0.040000129491090775, 0.0767107754945755, 0.07644762098789215, 0.0646054744720459, 0.1561966985464096, 0.032838739454746246, 0.0797998383641243, 0.1936294585466385, 0.03828350454568863, 0.05461779981851578, 0.06635807454586029, 0.10549232363700867, 0.052405864000320435, 0.22034282982349396, 0.971276044845581, 0.011539402417838573, 0.9693098068237305, 0.9701435565948486, 0.0177477840334177, 0.10421664267778397, 0.07718222588300705, 0.063149094581604, 0.4280105233192444, 0.04540130868554115, 0.08873891830444336, 0.13764850795269012, 0.016715936362743378, 0.021256066858768463, 0.017147982493042946, 0.03472466394305229, 0.027008071541786194, 0.08445381373167038, 0.6914923787117004, 0.006001793779432774, 0.05615964159369469, 0.04715695232152939, 0.005573094356805086, 0.030437668785452843, 0.02300470694899559, 0.05857500806450844, 0.07036733627319336, 0.3866337239742279, 0.01855841837823391, 0.10207130014896393, 0.028030944988131523, 0.045236144214868546, 0.1542668491601944, 0.11309036612510681, 0.07816966623067856, 0.025746358558535576, 0.03319108858704567, 0.05599057674407959, 0.21636247634887695, 0.11136075854301453, 0.26847559213638306, 0.13291946053504944, 0.021248500794172287, 0.05645586922764778, 0.010484294034540653, 0.010484294034540653, 0.005242147017270327, 0.005242147017270327, 0.9540707468986511, 0.005242147017270327, 0.9173343181610107, 0.9447474479675293, 0.11791004240512848, 0.30029812455177307, 0.03904646262526512, 0.008605634793639183, 0.09877213835716248, 0.13807548582553864, 0.013743327930569649, 0.2318383753299713, 0.043156616389751434, 0.008477192372083664, 0.005877346731722355, 0.0029386733658611774, 0.0029386733658611774, 0.005877346731722355, 0.9433141350746155, 0.0146933663636446, 0.0029386733658611774, 0.01175469346344471, 0.008816019631922245, 0.022179020568728447, 0.01414661854505539, 0.07217173278331757, 0.049033619463443756, 0.007193196099251509, 0.12060591578483582, 0.03141028806567192, 0.4200826585292816, 0.17443500459194183, 0.08871608227491379, 0.01105501502752304, 0.9728413820266724, 0.005241989623755217, 0.0034946599043905735, 0.016308411955833435, 0.23181243240833282, 0.03028705157339573, 0.005824433173984289, 0.06756342202425003, 0.5323531627655029, 0.09086115658283234, 0.01747329905629158, 0.9371579885482788, 0.0005355188623070717, 0.0026775943115353584, 0.01927867904305458, 0.003748632036149502, 0.018743159249424934, 0.010710377246141434, 0.00321311317384243, 0.001606556586921215, 0.0021420754492282867, 0.8792676329612732, 0.0005720674525946379, 0.012585483491420746, 0.04404919221997261, 0.005720674525946379, 0.03375197947025299, 0.012013415805995464, 0.004576539620757103, 0.0028603372629731894, 0.004576539620757103, 0.9549780488014221, 0.9785828590393066, 0.9472554922103882, 0.9766399264335632, 0.9659805297851562, 0.9749956130981445, 0.008705317974090576, 0.7478370666503906, 0.004952563438564539, 0.004952563438564539, 0.005571634043008089, 0.0024762817192822695, 0.14981505274772644, 0.0012381408596411347, 0.05757354944944382, 0.017333971336483955, 0.008047915995121002, 0.9741789102554321, 0.010588901117444038, 0.13165928423404694, 0.5809064507484436, 0.003211202099919319, 0.0041745626367628574, 0.1518898606300354, 0.022157294675707817, 0.03628658503293991, 0.006101284176111221, 0.00706464471295476, 0.05651715770363808, 0.9824780225753784, 0.00901355966925621, 0.0038966273423284292, 0.0017318343743681908, 0.10953852534294128, 0.06537675112485886, 0.01818426139652729, 0.0783655047416687, 0.6182648539543152, 0.0077932546846568584, 0.08096325397491455, 0.016452426090836525, 0.03463716059923172, 0.8745883107185364, 0.005772859789431095, 0.0028864298947155476, 0.005772859789431095, 0.0028864298947155476, 0.0028864298947155476, 0.0577285997569561, 0.01154571957886219, 0.01460269931703806, 0.009735132567584515, 0.02920539863407612, 0.024337831884622574, 0.01947026513516903, 0.8566917181015015, 0.01947026513516903, 0.004867566283792257, 0.01460269931703806, 0.015843292698264122, 0.08080078661441803, 0.06733398884534836, 0.07367131114006042, 0.027725761756300926, 0.5751115083694458, 0.01425896305590868, 0.02218060940504074, 0.01742762140929699, 0.10535789281129837, 0.8885164260864258, 0.010316590778529644, 0.015474886633455753, 0.016764460131525993, 0.007737443316727877, 0.05029338225722313, 0.002579147694632411, 0.005158295389264822, 0.0038687216583639383, 0.0012895738473162055, 0.9841501116752625, 0.059035759419202805, 0.018671775236725807, 0.09720306098461151, 0.012081736698746681, 0.024163473397493362, 0.22927841544151306, 0.02306513302028179, 0.046954020857810974, 0.4692656397819519, 0.02059386856853962, 0.0034979714546352625, 0.0023319809697568417, 0.10960309952497482, 0.0034979714546352625, 0.0034979714546352625, 0.7567278146743774, 0.023319808766245842, 0.02565179020166397, 0.01399188581854105, 0.05829952284693718, 0.002094196854159236, 0.002094196854159236, 0.0062825907953083515, 0.9863667488098145, 0.9627463221549988, 0.9897816181182861, 0.9590101838111877, 0.022542323917150497, 0.07360204309225082, 0.017925221472978592, 0.027702614665031433, 0.5583978295326233, 0.26616236567497253, 0.008962610736489296, 0.008962610736489296, 0.010592176578938961, 0.005160291213542223, 0.0715174600481987, 0.05700667202472687, 0.1098674014210701, 0.4208128750324249, 0.0505286380648613, 0.08265966922044754, 0.037054333835840225, 0.09354276210069656, 0.04638269916176796, 0.030576305463910103, 0.024861646816134453, 0.008287215605378151, 0.9364553689956665, 0.016574431210756302, 0.008287215605378151, 0.20301395654678345, 0.016767047345638275, 0.04897425323724747, 0.05983061343431473, 0.01592266373336315, 0.41097360849380493, 0.03872102126479149, 0.08564463257789612, 0.02074771374464035, 0.09939602017402649, 0.34556835889816284, 0.007108834572136402, 0.07872376590967178, 0.02632901817560196, 0.019220182672142982, 0.19812585413455963, 0.1296704113483429, 0.07082505524158478, 0.05594916269183159, 0.06832379847764969, 0.2831780016422272, 0.02121940441429615, 0.01907220296561718, 0.020335262641310692, 0.009346642531454563, 0.4370186924934387, 0.01919850893318653, 0.1425994634628296, 0.011367538943886757, 0.036628734320402145, 0.9728503823280334, 0.0008251487161032856, 0.0008251487161032856, 0.001650297432206571, 0.001650297432206571, 0.010314359329640865, 0.0012377231614664197, 0.00866406224668026, 0.0012377231614664197, 0.0004125743580516428, 0.9635488986968994, 0.006982238031923771, 0.020946715027093887, 0.31186261773109436, 0.04894648864865303, 0.11072118580341339, 0.06675653159618378, 0.049195580184459686, 0.0717383623123169, 0.03549554571509361, 0.07559928297996521, 0.03026462160050869, 0.1992732286453247, 0.7488431930541992, 0.004464993253350258, 0.005740705877542496, 0.01594640500843525, 0.005102849565446377, 0.08228345215320587, 0.005740705877542496, 0.12693338096141815, 0.0031892810948193073, 0.0012757123913615942, 0.007014543283730745, 0.02263784408569336, 0.04049304500222206, 0.14762425422668457, 0.02582627348601818, 0.018811728805303574, 0.21171167492866516, 0.11733417958021164, 0.39408978819847107, 0.014666772447526455, 0.9675753712654114, 0.012317916378378868, 0.9731153845787048, 0.9535740613937378, 0.028046296909451485, 0.01520279236137867, 0.01520279236137867, 0.9425731301307678, 0.9663459062576294], "Term": ["abductive_reasoning", "abductive_reasoning", "academic_institution", "academic_institution", "academic_institution", "academic_institution", "academic_institution", "academic_institution", "academic_institution", "academic_institution", "academic_institution", "academic_licensee", "academic_licensee", "academic_licensee", "academic_licensee", "academic_licensee", "accurate_metaknowledge", "action", "action", "action", "action", "action", "action", "action", "action", "action", "action", "activity", "activity", "activity", "activity", "activity", "activity", "activity", "activity", "activity", "activity", "actor", "actor", "actor", "actor", "actor", "actor", "actor", "actor", "actor", "actor", "ad", "ad", "ad", "ad", "ad", "ad", "ad", "ad", "ad", "ad", "advert", "advert", "advert", "advert", "advertiser", "advertiser", "advertiser", "advertiser", "advertiser", "advertiser", "advertiser", "advertiser", "advertiser", "advertiser", "advertising", "advertising", "advertising", "advertising", "advertising", "advertising", "advertising", "advertising", "advertising", "advertising", "aesthetic", "aesthetic", "aesthetic", "aesthetic", "aesthetic", "aesthetic", "aesthetic", "aesthetic", "aesthetic", "aesthetic", "affective", "affective", "affective", "affective", "affective", "affective", "affective", "affective", "affective", "affective", "affective_appraisal", "affine", "affine", "affine", "affine", "affine", "affine", "affine", "affordance", "affordance", "affordance", "affordance", "affordance", "affordance", "affordance", "affordance", "affordance", "affordance", "agility", "agility", "agility", "agility", "agility", "agility", "agility", "agility", "agility", "agility", "aid", "aid", "aid", "aid", "aid", "aid", "aid", "aid", "aid", "aid", "album", "album", "album", "album", "album", "album", "album", "algorithm", "algorithm", "algorithm", "algorithm", "algorithm", "algorithm", "algorithm", "algorithm", "algorithm", "algorithm", "algorithmic_trading", "algorithmic_trading", "algorithmic_trading", "algorithmic_trading", "alignment", "alignment", "alignment", "alignment", "alignment", "alignment", "alignment", "alignment", "alignment", "alignment", "alliance", "alliance", "alliance", "alliance", "alliance", "alliance", "alliance", "alliance", "alliance", "alliance", "alliance_rivalry", "ambient_awareness", "ambiguity_averse", "ambulance_diversion", "analyst", "analyst", "analyst", "analyst", "analyst", "analyst", "analyst", "analyst", "analyst", "analyst", "analytic", "analytic", "analytic", "analytic", "analytic", "analytic", "analytic", "analytic", "analytic", "analytic", "analytical_innovator", "analytical_innovator", "announcement", "announcement", "announcement", "announcement", "announcement", "announcement", "announcement", "announcement", "announcement", "announcement", "app", "app", "app", "app", "app", "app", "app", "app", "app", "app", "applicant", "applicant", "applicant", "applicant", "applicant", "applicant", "applicant", "applicant", "applicant", "applicant", "applicant_reaction", "applicant_reaction", "applicant_reaction", "appointment", "appointment", "appointment", "appointment", "appointment", "appointment", "appointment", "appointment", "appointment", "appointment", "appointment_slot", "approximation", "approximation", "approximation", "approximation", "approximation", "approximation", "approximation", "approximation", "approximation", "approximation", "archetypal_problem", "arousal", "arousal", "arousal", "arousal", "arousal", "arousal", "arousal", "arousal", "arousal", "arousal", "arrival", "arrival", "arrival", "arrival", "arrival", "arrival", "arrival", "arrival", "arrival", "arrival", "artifact", "artifact", "artifact", "artifact", "artifact", "artifact", "artifact", "artifact", "artifact", "artifact", "ascend_auction", "assembler", "assembler", "assembler", "assess", "assess", "assess", "assess", "assess", "assess", "assess", "assess", "assess", "assess", "asset", "asset", "asset", "asset", "asset", "asset", "asset", "asset", "asset", "asset", "assume", "assume", "assume", "assume", "assume", "assume", "assume", "assume", "assume", "assume", "attack", "attack", "attack", "attack", "attack", "attack", "attack", "attack", "attack", "attack", "attainment_discrepancy", "attestation", "attestation", "attestation", "attitude", "attitude", "attitude", "attitude", "attitude", "attitude", "attitude", "attitude", "attitude", "attitude", "attribute", "attribute", "attribute", "attribute", "attribute", "attribute", "attribute", "attribute", "attribute", "attribute", "auction", "auction", "auction", "auction", "auction", "auction", "auction", "auction", "auction", "auction", "auctioneer", "auctioneer", "auctioneer", "audit", "audit", "audit", "audit", "audit", "audit", "audit", "audit", "audit", "audit", "audit_litigation", "auditor", "auditor", "auditor", "auditor", "auditor", "auditor", "auditor", "auditor", "auditor", "auditor", "auditor_independence", "augment_clinical", "augment_clinical", "augment_clinical", "augment_triage", "augment_triage", "authorize_ebscohost", "authorize_ebscohost", "authorize_ebscohost", "authorize_ebscohost", "authorize_ebscohost", "bank", "bank", "bank", "bank", "bank", "bank", "bank", "bank", "bank", "bank", "banknote", "battery_swap", "batting_average", "battle_channel", "bed", "bed", "bed", "bed", "bed", "bed", "bed", "bed", "bed", "bed", "bettor", "bias", "bias", "bias", "bias", "bias", "bias", "bias", "bias", "bias", "bias", "bid", "bid", "bid", "bid", "bid", "bid", "bid", "bid", "bid", "bid", "bid_increment", "bidder", "bidder", "bidder", "bidder", "bidder", "bidder", "bidder", "bidder", "bidder", "bidder", "bidding", "bidding", "bidding", "bidding", "bidding", "bidding", "bidding", "bidding", "bidding", "bidding", "big", "big", "big", "big", "big", "big", "big", "big", "big", "big", "biofeedback", "biofeedback", "biofeedback", "biometric", "biometric", "biometric", "biometric", "biometric", "biometric", "biometric", "biometric", "biometric", "bitcoin", "bitcoin", "bitcoin", "bitcoin", "bitcoin", "bitcoin", "bitcoin", "bitcoin", "bliese", "blog", "blog", "blog", "blog", "blog", "blog", "blog", "blog", "blog", "blog", "board", "board", "board", "board", "board", "board", "board", "board", "board", "board", "board_independence", "board_independence", "boat", "boat", "boat", "boat", "boat", "boat", "boat", "boat", "boat", "bop", "bop", "bop", "bop", "bop", "bop", "bop", "borrower", "borrower", "borrower", "borrower", "borrower", "borrower", "borrower", "borrower", "borrower", "borrower", "bound", "bound", "bound", "bound", "bound", "bound", "bound", "bound", "bound", "bound", "boundary_spanner", "boundary_spanner", "boundary_spanner", "boundary_spanner", "brain", "brain", "brain", "brain", "brain", "brain", "brain", "brain", "brain", "brain", "brain_activation", "brand", "brand", "brand", "brand", "brand", "brand", "brand", "brand", "brand", "brand", "breach", "breach", "breach", "breach", "breach", "breach", "breach", "breach", "breach", "breach", "breach_occurrence", "breadth_sampling", "buzz", "buzz", "buzz", "buzz", "buzz", "buzz", "buzz", "buzz", "bvit", "bvit", "bvit", "can_graphically", "can_graphically", "can_graphically", "can_graphically", "can_graphically", "can_graphically", "can_graphically", "can_graphically", "can_graphically", "capability", "capability", "capability", "capability", "capability", "capability", "capability", "capability", "capability", "capability", "capacity", "capacity", "capacity", "capacity", "capacity", "capacity", "capacity", "capacity", "capacity", "capacity", "capital", "capital", "capital", "capital", "capital", "capital", "capital", "capital", "capital", "capital", "capital_adequacy", "capitalize_agility", "care", "care", "care", "care", "care", "care", "care", "care", "care", "care", "career", "career", "career", "career", "career", "career", "career", "career", "career", "career", "carefree", "carefree", "carefree", "catch_fish", "catch_fish", "category", "category", "category", "category", "category", "category", "category", "category", "category", "category", "ceo", "ceo", "ceo", "ceo", "ceo", "ceo", "ceo", "ceo", "ceo", "ceo", "chain", "chain", "chain", "chain", "chain", "chain", "chain", "chain", "chain", "chain", "chance_constrain", "chance_constrain", "channel", "channel", "channel", "channel", "channel", "channel", "channel", "channel", "channel", "channel", "chromatography", "chromatography_step", "chromatography_step", "chronological_age", "chronological_age", "citation", "citation", "citation", "citation", "citation", "citation", "citation", "citation", "citation", "citation", "clan", "clan", "clan", "clan", "clan", "clan", "clan", "clan", "clan", "clan", "classification", "classification", "classification", "classification", "classification", "classification", "classification", "classification", "classification", "classification", "classifier", "classifier", "classifier", "classifier", "classifier", "classifier", "classifier", "classifier", "classifier", "classifier", "clearance", "clearance", "clearance", "clearance", "clearance", "clearance", "clearance", "clearance", "clearance", "clearance", "click", "click", "click", "click", "click", "click", "click", "click", "click", "click", "click_conversion", "click_conversion", "click_conversion", "client", "client", "client", "client", "client", "client", "client", "client", "client", "client", "clinic", "clinic", "clinic", "clinic", "clinic", "clinic", "clinic", "clinic", "clinic", "clinic", "clinical_hit", "clinical_hit", "clinical_hit", "co_authorship", "co_authorship", "co_authorship", "codification", "codification", "codification", "codification", "codification", "codification", "codification", "codification", "coeff_stat", "coeff_stat", "cognitive", "cognitive", "cognitive", "cognitive", "cognitive", "cognitive", "cognitive", "cognitive", "cognitive", "cognitive", "cognitive_entrenchment", "cohesion", "cohesion", "cohesion", "cohesion", "cohesion", "cohesion", "cohesion", "cohesion", "cohesion", "cohesion", "collaboration", "collaboration", "collaboration", "collaboration", "collaboration", "collaboration", "collaboration", "collaboration", "collaboration", "collaboration", "collective", "collective", "collective", "collective", "collective", "collective", "collective", "collective", "collective", "collective", "color_photo", "combinatorial_auction", "combinatorial_auction", "combinatorial_auction", "combinatorial_auction", "combinatorial_auction", "common_fate", "common_fate", "common_fate", "common_fate", "common_fate", "common_fate", "communication", "communication", "communication", "communication", "communication", "communication", "communication", "communication", "communication", "communication", "community", "community", "community", "community", "community", "community", "community", "community", "community", "community", "compatriot", "competency", "competency", "competency", "competency", "competency", "competency", "competency", "competency", "competency", "competency", "competition", "competition", "competition", "competition", "competition", "competition", "competition", "competition", "competition", "competition", "competitive", "competitive", "competitive", "competitive", "competitive", "competitive", "competitive", "competitive", "competitive", "competitive", "compliance", "compliance", "compliance", "compliance", "compliance", "compliance", "compliance", "compliance", "compliance", "compliance", "computer", "computer", "computer", "computer", "computer", "computer", "computer", "computer", "computer", "computer", "computer_anxiety", "computer_anxiety", "computer_anxiety", "computer_anxiety", "computer_anxiety", "computer_anxiety", "computer_anxiety", "concept", "concept", "concept", "concept", "concept", "concept", "concept", "concept", "concept", "concept", "conceptual", "conceptual", "conceptual", "conceptual", "conceptual", "conceptual", "conceptual", "conceptual", "conceptual", "conceptual", "conflict", "conflict", "conflict", "conflict", "conflict", "conflict", "conflict", "conflict", "conflict", "conflict", "confucian_culture", "connective_action", "connective_action", "constraint", "constraint", "constraint", "constraint", "constraint", "constraint", "constraint", "constraint", "constraint", "constraint", "construct", "construct", "construct", "construct", "construct", "construct", "construct", "construct", "construct", "construct", "consumer", "consumer", "consumer", "consumer", "consumer", "consumer", "consumer", "consumer", "consumer", "consumer", "content", "content", "content", "content", "content", "content", "content", "content", "content", "content", "contextual_ambidexterity", "contextual_ambidexterity", "contextual_ambidexterity", "continuous_combinatorial", "continuous_combinatorial", "continuous_combinatorial", "contract", "contract", "contract", "contract", "contract", "contract", "contract", "contract", "contract", "contract", "contribution", "contribution", "contribution", "contribution", "contribution", "contribution", "contribution", "contribution", "contribution", "contribution", "controllee", "controller", "controller", "controller", "controller", "controller", "controller", "controller", "conversion", "conversion", "conversion", "conversion", "conversion", "conversion", "conversion", "conversion", "conversion", "conversion", "convex", "convex", "convex", "convex", "convex", "convex", "convex", "convex", "convex", "convex", "convex_optimization", "convex_optimization", "convex_optimization", "convex_optimization", "coordination", "coordination", "coordination", "coordination", "coordination", "coordination", "coordination", "coordination", "coordination", "coordination", "corporate", "corporate", "corporate", "corporate", "corporate", "corporate", "corporate", "corporate", "corporate", "corporate", "corruption", "corruption", "corruption", "corruption", "corruption", "corruption", "corruption", "corruption", "corruption", "corruption", "cortex", "cortex", "cortical_activity", "countable_state", "country", "country", "country", "country", "country", "country", "country", "country", "country", "country", "course_pack", "course_pack", "course_pack", "course_pack", "course_pack", "coworker", "coworker", "coworker", "coworker", "coworker", "coworker", "coworker", "coworker", "coworker", "coworker", "crawler", "crawler", "crawler", "crawler", "crawler", "crawler", "crawler", "crawler", "credibility_assessment", "credibility_assessment", "credibility_assessment", "credibility_assessment", "credibility_assessment", "credibility_assessment", "credibility_assessment", "credit_grade", "credit_grade", "critical_realism", "critical_realism", "critical_realism", "critical_realism", "ctr", "cue_multiplicity", "cue_multiplicity", "cultural", "cultural", "cultural", "cultural", "cultural", "cultural", "cultural", "cultural", "cultural", "cultural", "cultural_signifier", "cultural_signifier", "culture", "culture", "culture", "culture", "culture", "culture", "culture", "culture", "culture", "culture", "cybercriminal", "cybercriminal", "cybercriminal", "cybercriminal", "darknet", "darknet", "darknet", "day", "day", "day", "day", "day", "day", "day", "day", "day", "day", "debris", "debris", "debris", "debris", "debris", "debris", "deceiver", "deceiver", "deceiver", "deception", "deception", "deception", "deception", "deception", "deception", "deception", "deception", "deception", "deception", "deception_detection", "deception_detection", "deception_detection", "deception_detection", "deception_detection", "deception_detection", "deception_detection", "deceptive", "deceptive", "deceptive", "deceptive", "deceptive", "deceptive", "deceptive", "deceptive", "deceptive", "deceptive", "default_repayment", "defect_discovery", "delay", "delay", "delay", "delay", "delay", "delay", "delay", "delay", "delay", "delay", "demand", "demand", "demand", "demand", "demand", "demand", "demand", "demand", "demand", "demand", "deprivation", "deprivation", "deprivation", "deprivation", "deprivation", "deprivation", "deprivation", "deprivation", "deprivation", "deprivation", "desk_salesperson", "desksale", "desksale", "desksale", "detect_deception", "detect_deception", "detection", "detection", "detection", "detection", "detection", "detection", "detection", "detection", "detection", "detection", "digit_naic", "digit_naic", "digit_naic", "digital", "digital", "digital", "digital", "digital", "digital", "digital", "digital", "digital", "digital", "digitally_mature", "digitally_mature", "dimension", "dimension", "dimension", "dimension", "dimension", "dimension", "dimension", "dimension", "dimension", "dimension", "dimensional_surface", "dimensional_surface", "dimensional_surface", "director", "director", "director", "director", "director", "director", "director", "director", "director", "director", "disaster", "disaster", "disaster", "disaster", "disaster", "disaster", "disaster", "disaster", "disaster", "disaster", "discharge", "discharge", "discharge", "discharge", "discharge", "discharge", "discharge", "discharge", "discharge", "discharge", "disclosure", "disclosure", "disclosure", "disclosure", "disclosure", "disclosure", "disclosure", "disclosure", "disclosure", "disclosure", "discover_unobserved", "discretionary_accrual", "discriminatory_auction", "discriminatory_auction", "display", "display", "display", "display", "display", "display", "display", "display", "display", "display", "distance_humane", "distribution", "distribution", "distribution", "distribution", "distribution", "distribution", "distribution", "distribution", "distribution", "distribution", "district", "district", "district", "district", "district", "district", "district", "district", "district", "district", "divest_stock", "division_labor", "division_labor", "division_labor", "division_labor", "division_labor", "division_labor", "division_labor", "division_labor", "division_labor", "doctoral_recipient", "domestic", "domestic", "domestic", "domestic", "domestic", "domestic", "domestic", "domestic", "domestic", "domestic", "downside_risk", "downside_risk", "downside_risk", "downside_risk", "downside_risk", "downside_risk", "dsmm", "dsmm", "dual_emphasis", "dual_emphasis", "due_image", "due_image", "due_image", "due_image", "due_image", "due_image", "due_image", "due_image", "due_image", "dynamic", "dynamic", "dynamic", "dynamic", "dynamic", "dynamic", "dynamic", "dynamic", "dynamic", "dynamic", "eam", "earn_own", "earn_own", "earning", "earning", "earning", "earning", "earning", "earning", "earning", "earning", "earning", "earning", "economic", "economic", "economic", "economic", "economic", "economic", "economic", "economic", "economic", "economic", "economy", "economy", "economy", "economy", "economy", "economy", "economy", "economy", "economy", "economy", "ed", "ed", "ed", "ed", "ed", "ed", "ed", "ed", "ed", "ed", "effectuation", "efficiency", "efficiency", "efficiency", "efficiency", "efficiency", "efficiency", "efficiency", "efficiency", "efficiency", "efficiency", "electronic_mechanical", "electronic_mechanical", "electronic_mechanical", "electronic_mechanical", "electronic_mechanical", "electronic_mechanical", "electronic_reserve", "electronic_reserve", "electronic_reserve", "electronic_reserve", "electronic_reserve", "emerge", "emerge", "emerge", "emerge", "emerge", "emerge", "emerge", "emerge", "emerge", "emerge", "emergency_department", "emergency_department", "emergency_department", "emergency_department", "emergency_department", "emergency_department", "emotion", "emotion", "emotion", "emotion", "emotion", "emotion", "emotion", "emotion", "emotion", "emotion", "employee", "employee", "employee", "employee", "employee", "employee", "employee", "employee", "employee", "employee", "employer_commit", "empower_leadership", "empower_leadership", "enact", "enact", "enact", "enact", "enact", "enact", "enact", "enact", "enact", "enact", "end_concession", "end_concession", "end_concession", "engage_onshore", "engineering", "engineering", "engineering", "engineering", "engineering", "engineering", "engineering", "engineering", "engineering", "engineering", "enthusiastic_leaver", "enthusiastic_stayer", "enticement", "enticement", "entitativity", "entravision", "entravision", "entry", "entry", "entry", "entry", "entry", "entry", "entry", "entry", "entry", "entry", "environmental_dynamism", "environmental_dynamism", "environmental_dynamism", "environmental_dynamism", "environmental_dynamism", "epistemic", "epistemic", "epistemic", "epistemic", "epistemic", "epistemic", "epistemic", "epistemic", "epistemic", "epistemic_virtue", "equally_skilled", "equation", "equation", "equation", "equation", "equation", "equation", "equation", "equation", "equation", "equation", "equilibrium", "equilibrium", "equilibrium", "equilibrium", "equilibrium", "equilibrium", "equilibrium", "equilibrium", "equilibrium", "equilibrium", "estimate", "estimate", "estimate", "estimate", "estimate", "estimate", "estimate", "estimate", "estimate", "estimate", "ethic", "ethic", "ethic", "ethic", "ethic", "ethic", "ethic", "ethic", "ethic", "ethic", "ethical", "ethical", "ethical", "ethical", "ethical", "ethical", "ethical", "ethical", "ethical", "ethical", "ethnographic_story", "ethnographic_story", "evaluation", "evaluation", "evaluation", "evaluation", "evaluation", "evaluation", "evaluation", "evaluation", "evaluation", "evaluation", "event", "event", "event", "event", "event", "event", "event", "event", "event", "event", "evolvable", "ewom", "ewom", "ewom", "ewom", "ewom", "ewom", "ewom", "ewom", "ewom", "executive", "executive", "executive", "executive", "executive", "executive", "executive", "executive", "executive", "executive", "expatriate", "expatriate", "expatriate", "expatriate", "expatriate", "expatriate", "experiment", "experiment", "experiment", "experiment", "experiment", "experiment", "experiment", "experiment", "experiment", "experiment", "expert_blog", "expert_blog", "expert_blog", "expert_blog", "expert_blog", "expertise", "expertise", "expertise", "expertise", "expertise", "expertise", "expertise", "expertise", "expertise", "expertise", "exponential_smooth", "exponential_smooth", "exponential_smooth", "faithful_representation", "fast_frugal", "fast_frugal", "fast_frugal", "fast_frugal", "fast_frugal", "fear_appeal", "fear_appeal", "fear_appeal", "fear_appeal", "fear_appeal", "fear_appeal", "fear_appeal", "fear_appeal", "fear_appeal", "feature", "feature", "feature", "feature", "feature", "feature", "feature", "feature", "feature", "feature", "feedstock", "feedstock", "feedstock", "feeling_violation", "female_director", "fielder", "filler_interface", "filler_interface", "fimix", "fimix_pls", "financial", "financial", "financial", "financial", "financial", "financial", "financial", "financial", "financial", "financial", "finite", "finite", "finite", "finite", "finite", "finite", "finite", "finite", "finite", "finite", "finitely_many", "fintech", "fintech", "fintech", "fintech", "fintech", "fintech", "fintech", "fintech", "fintech", "fintech_lender", "fiscal_quarter", "fiscal_quarter", "fix", "fix", "fix", "fix", "fix", "fix", "fix", "fix", "fix", "fix", "flight", "flight", "flight", "flight", "flight", "flight", "flight", "flight", "flight", "flight", "forage", "forage", "forage", "foreign", "foreign", "foreign", "foreign", "foreign", "foreign", "foreign", "foreign", "foreign", "foreign", "forgiveness", "forgiveness", "forgiveness", "forgiveness", "forgiveness", "formulation", "formulation", "formulation", "formulation", "formulation", "formulation", "formulation", "formulation", "formulation", "formulation", "forward_citation", "forward_citation", "forward_citation", "forward_citation", "franchisee", "franchisee", "franchisee", "franchisee", "franchising", "franchising", "franchising", "franchising", "franchising", "franchisor", "franchisor", "franchisor", "franchisor", "franchisor", "fraud", "fraud", "fraud", "fraud", "fraud", "fraud", "fraud", "fraud", "fraud", "fraud", "functional_affordance", "functional_affordance", "functional_affordance", "functional_affordance", "game", "game", "game", "game", "game", "game", "game", "game", "game", "game", "gamification", "gamification", "gamification", "gamification", "gamification", "gamification", "gamification", "gamification", "gamification", "gamification", "gamifie", "gamifie", "gamified", "gamified", "gamified", "gamified", "gaming_element", "gaming_element", "gender", "gender", "gender", "gender", "gender", "gender", "gender", "gender", "gender", "gender", "generativity", "generativity", "geographic_dispersion", "geographic_dispersion", "geographic_dispersion", "geographic_dispersion", "geographic_dispersion", "get", "get", "get", "get", "get", "get", "get", "get", "get", "get", "gifting", "glaucoma", "glaucoma", "glaucoma", "glaucoma", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "goal", "goal", "goal", "goal", "goal", "goal", "goal", "goal", "goal", "goal", "governance", "governance", "governance", "governance", "governance", "governance", "governance", "governance", "governance", "governance", "gradient_estimator", "gradient_estimator", "gradient_estimator", "grammar", "grammar", "grammar", "grammar", "grammar", "grammar", "grammar", "grand_challenge", "grand_challenge", "grand_challenge", "grand_challenge", "grand_challenge", "grand_challenge", "gratification", "gratification", "gratification", "gratification", "gratification", "gratification", "gratification", "green", "green", "green", "green", "green", "green", "green", "green", "green", "green", "growth", "growth", "growth", "growth", "growth", "growth", "growth", "growth", "growth", "growth", "habituation", "habituation", "habituation", "habituation", "habituation", "habituation", "habituation", "hacker_forum", "hacker_forum", "hacker_forum", "hacker_forum", "hacker_forum", "harm_crisis", "harm_crisis", "harm_crisis", "hate_crime", "hate_crime", "hate_crime", "health", "health", "health", "health", "health", "health", "health", "health", "health", "health", "health_literacy", "helpfulness", "helpfulness", "helpfulness", "helpfulness", "helpfulness", "helpfulness", "helpfulness", "helpfulness", "helpfulness", "helpfulness", "hit", "hit", "hit", "hit", "hit", "hit", "hit", "hit", "hit", "hit", "hospital", "hospital", "hospital", "hospital", "hospital", "hospital", "hospital", "hospital", "hospital", "hospital", "hot_patent", "hot_patent", "hotel", "hotel", "hotel", "hotel", "hotel", "hotel", "hotel", "hotel", "hotel", "hotel", "hour", "hour", "hour", "hour", "hour", "hour", "hour", "hour", "hour", "hour", "hr", "hr", "hr", "hr", "hr", "hr", "hr", "hr", "hr", "hr", "human", "human", "human", "human", "human", "human", "human", "human", "human", "human", "humanitarian", "humanitarian", "humanitarian", "humanitarian", "humanitarian", "humanitarian", "humanitarian", "humanitarian", "humanitarian", "humanitarian", "hybrid_seed", "hybrid_seed", "hybrid_seed", "hypothesis", "hypothesis", "hypothesis", "hypothesis", "hypothesis", "hypothesis", "hypothesis", "hypothesis", "hypothesis", "hypothesis", "ice_breaker", "ice_breaker", "ice_breaker", "ice_breaker", "icu", "icu", "icu", "icu", "idea", "idea", "idea", "idea", "idea", "idea", "idea", "idea", "idea", "idea", "ideation", "ideation", "ideation", "ideation", "ideation", "ideation", "ideation", "ideation", "identity", "identity", "identity", "identity", "identity", "identity", "identity", "identity", "identity", "identity", "idiographic", "idiographic", "idiographic", "illegal", "illegal", "illegal", "illegal", "illegal", "illegal", "illegal", "illegal", "illegal", "illegal", "imitator", "imitator", "imitator", "imitator", "imitator", "imitator", "imitator", "implementer_response", "include_photocopy", "include_photocopy", "include_photocopy", "include_photocopy", "include_photocopy", "include_photocopy", "income_household", "income_household", "income_household", "income_household", "indicator", "indicator", "indicator", "indicator", "indicator", "indicator", "indicator", "indicator", "indicator", "indicator", "inequality", "inequality", "inequality", "inequality", "inequality", "inequality", "inequality", "inequality", "inequality", "inequality", "inform", "inform", "inform", "inform", "inform", "inform", "inform", "inform", "inform", "inform", "informed_trader", "innovation", "innovation", "innovation", "innovation", "innovation", "innovation", "innovation", "innovation", "innovation", "innovation", "innovator", "innovator", "innovator", "innovator", "innovator", "innovator", "innovator", "innovator", "innovator", "innovator", "inpatient", "inpatient", "inpatient", "inpatient", "inpatient", "inpatient", "inpatient", "inscription", "inscription", "inscription", "inscription", "inscription", "inscription", "instance", "instance", "instance", "instance", "instance", "instance", "instance", "instance", "instance", "instance", "insular_cortex", "integration", "integration", "integration", "integration", "integration", "integration", "integration", "integration", "integration", "integration", "intellectual_alignment", "intellectual_alignment", "intellectual_alignment", "intellectual_alignment", "intellectual_capital", "intellectual_capital", "intellectual_capital", "intellectual_capital", "intellectual_capital", "intellectual_capital", "intellectual_capital", "intellectual_capital", "intellectual_capital", "intention", "intention", "intention", "intention", "intention", "intention", "intention", "intention", "intention", "intention", "intention_comply", "intention_comply", "interaction", "interaction", "interaction", "interaction", "interaction", "interaction", "interaction", "interaction", "interaction", "interaction", "interbank", "interbank_payment", "interbank_payment", "interconnection", "interconnection", "interconnection", "interconnection", "interconnection", "interconnection", "interconnection", "international", "international", "international", "international", "international", "international", "international", "international", "international", "international", "internet", "internet", "internet", "internet", "internet", "internet", "internet", "internet", "internet", "internet", "interpersonal_injustice", "interview", "interview", "interview", "interview", "interview", "interview", "interview", "interview", "interview", "interview", "intrinsic_hedonic", "intrinsic_hedonic", "inventory", "inventory", "inventory", "inventory", "inventory", "inventory", "inventory", "inventory", "inventory", "inventory", "investment", "investment", "investment", "investment", "investment", "investment", "investment", "investment", "investment", "investment", "investor", "investor", "investor", "investor", "investor", "investor", "investor", "investor", "investor", "investor", "invs", "isp_violation", "isp_violation", "item", "item", "item", "item", "item", "item", "item", "item", "item", "item", "job", "job", "job", "job", "job", "job", "job", "job", "job", "job", "journal", "journal", "journal", "journal", "journal", "journal", "journal", "journal", "journal", "journal", "judgment", "judgment", "judgment", "judgment", "judgment", "judgment", "judgment", "judgment", "judgment", "judgment", "kernel", "kernel", "kernel", "kernel", "kernel", "kernel", "kernel", "kernel", "kernel", "kernel", "kernel_smooth", "keyword", "keyword", "keyword", "keyword", "keyword", "keyword", "keyword", "keyword", "keyword", "keyword", "kms", "kms", "kms", "kms", "kms", "kms", "kms", "kms", "kms", "labor", "labor", "labor", "labor", "labor", "labor", "labor", "labor", "labor", "labor", "last_fm", "last_fm", "launcher", "leader", "leader", "leader", "leader", "leader", "leader", "leader", "leader", "leader", "leader", "leader_behaviour", "leader_behaviour", "leadership", "leadership", "leadership", "leadership", "leadership", "leadership", "leadership", "leadership", "leadership", "leadership", "learning_please", "learning_please", "learning_please", "learning_please", "learning_please", "lemma", "lemma", "lemma", "lemma", "lemma", "lemma", "lemma", "lemma", "lemma", "lemma", "lender", "lender", "lender", "lender", "lender", "lender", "lender", "lender", "lender", "lender", "lending_club", "lending_club", "length_stay", "length_stay", "length_stay", "length_stay", "length_stay", "length_stay", "let", "let", "let", "let", "let", "let", "let", "let", "let", "let", "lexicographic_strategy", "liability_foreignness", "liability_foreignness", "liability_outsidership", "librarian", "librarian", "librarian", "librarian", "librarian", "librarian", "librarian", "license_private", "license_private", "license_private", "license_private", "license_private", "license_private", "license_private", "licensee", "licensee", "licensee", "licensee", "licensee", "licensee", "licensee", "linguistic_cue", "liquefaction", "liquidation", "liquidation", "liquidation", "liquidation", "liquidation", "liquidation", "liquidity", "liquidity", "liquidity", "liquidity", "liquidity", "liquidity", "liquidity", "liquidity", "liquidity", "liquidity", "loaf", "loaf", "loaf", "loaf", "loafing", "loafing", "loafing", "loafing", "loafing", "loafing", "loafing", "loan", "loan", "loan", "loan", "loan", "loan", "loan", "loan", "loan", "loan", "loan_officer", "loan_officer", "loan_officer", "loan_officer", "loan_officer", "loan_officer", "locational_target", "lock_trial", "look", "look", "look", "look", "look", "look", "look", "look", "look", "look", "malicious", "malicious", "malicious", "malicious", "malicious", "malicious", "malicious", "malicious", "malware_propagation", "malware_propagation", "manager", "manager", "manager", "manager", "manager", "manager", "manager", "manager", "manager", "manager", "markdown", "markdown", "markdown", "markdown", "markdown", "markdown", "marketing", "marketing", "marketing", "marketing", "marketing", "marketing", "marketing", "marketing", "marketing", "marketing", "marxist", "marxist", "marxist", "massive_fine", "materiality", "materiality", "materiality", "materiality", "materiality", "measurement", "measurement", "measurement", "measurement", "measurement", "measurement", "measurement", "measurement", "measurement", "measurement", "mechanism", "mechanism", "mechanism", "mechanism", "mechanism", "mechanism", "mechanism", "mechanism", "mechanism", "mechanism", "median_split", "median_split", "median_split", "median_split", "median_split", "median_split", "median_split", "median_split", "medical", "medical", "medical", "medical", "medical", "medical", "medical", "medical", "medical", "medical", "medium", "medium", "medium", "medium", "medium", "medium", "medium", "medium", "medium", "medium", "member", "member", "member", "member", "member", "member", "member", "member", "member", "member", "merger", "merger", "merger", "merger", "merger", "merger", "merger", "merger", "merger", "merger", "message", "message", "message", "message", "message", "message", "message", "message", "message", "message", "metaknowledge", "metaknowledge", "metaknowledge", "micromarket", "micromarket", "mindful_mindless", "mindset_metric", "mindset_metric", "minority", "minority", "minority", "minority", "minority", "minority", "minority", "minority", "minority", "minority", "misinformation", "misinformation", "misinformation", "misinformation", "misinformation", "misinformation", "misinformation", "mission_orient", "mission_orient", "misstatement", "mock_crime", "modular_integral", "modular_upgrade", "modular_upgrade", "modular_upgrade", "moral", "moral", "moral", "moral", "moral", "moral", "moral", "moral", "moral", "moral", "moral_courage", "mosquito_net", "moss", "moss", "moss", "moss", "motorway", "movie", "movie", "movie", "movie", "movie", "movie", "movie", "movie", "movie", "movie", "mssp", "mssp", "multitasking", "multitasking", "multitasking", "multitasking", "multitasking", "multitasking", "multitasking", "multitasking", "multitasking", "multiteam", "music", "music", "music", "music", "music", "music", "music", "music", "music", "music", "nanotechnology", "nanotechnology", "nanotechnology", "nanotechnology", "nanotechnology", "nanotechnology_patent", "navigability", "navigability", "navigability", "navigability", "navigability", "navigability", "negative", "negative", "negative", "negative", "negative", "negative", "negative", "negative", "negative", "negative", "net_neutrality", "net_neutrality", "net_neutrality", "net_neutrality", "net_neutrality", "network", "network", "network", "network", "network", "network", "network", "network", "network", "network", "neural_correlate", "neurodiverse", "neuroscience", "neuroscience", "neuroscience", "neuroscience", "neuroscience", "neuroscience", "neuroscience", "neutrality_regulation", "neutralization", "neutralization", "neutralization", "neutralization", "neutralization", "neutralization", "neutralization", "newsletter_content", "newsletter_content", "newsletter_content", "newsletter_content", "newsletter_content", "newsvendor", "newsvendor", "newsvendor", "newsvendor", "newsvendor", "newsvendor", "newsvendor", "newsvendor", "newsvendor", "newsvendor", "niche_construction", "nmsvs", "non_urgent", "nonnetworked_prediction", "nonverbal", "nonverbal", "nonverbal", "nonverbal", "nonverbal", "nxn", "object", "object", "object", "object", "object", "object", "object", "object", "object", "object", "ocba", "ocrs", "ocrs", "oculometric", "oculometric", "offshore", "offshore", "offshore", "offshore", "offshore", "offshore", "offshore", "offshore", "offshore", "offshore", "offshore_advanced", "og", "og", "online_firestorm", "onsite_offshore", "onsite_offshore", "ontological", "ontological", "ontological", "ontological", "ontological", "ontological", "ontological", "ontological", "ontologically_clear", "ontologically_clear", "ontologically_unclear", "ontologically_unclear", "opaque_channel", "opaque_channel", "opaque_selling", "open", "open", "open", "open", "open", "open", "open", "open", "open", "open", "openness_divergent", "openness_divergent", "operation", "operation", "operation", "operation", "operation", "operation", "operation", "operation", "operation", "operation", "optimal", "optimal", "optimal", "optimal", "optimal", "optimal", "optimal", "optimal", "optimal", "optimal", "optimality", "optimality", "optimality", "optimality", "optimality", "optimality", "optimality", "optimality", "optimality", "optimality", "optimization", "optimization", "optimization", "optimization", "optimization", "optimization", "optimization", "optimization", "optimization", "optimization", "organic_listing", "organic_listing", "organic_sponsor", "organizational", "organizational", "organizational", "organizational", "organizational", "organizational", "organizational", "organizational", "organizational", "organizational", "organize_vision", "organize_vision", "organize_vision", "organize_vision", "oscm", "oscm", "oscm", "oscm", "outsidership", "overdraft", "overdraft", "overdraft_fee", "overdraft_fee", "overprediction", "page", "page", "page", "page", "page", "page", "page", "page", "page", "page", "parameter", "parameter", "parameter", "parameter", "parameter", "parameter", "parameter", "parameter", "parameter", "parameter", "parent_seed", "parent_seed", "partially_observable", "partially_observable", "partially_observable", "participant", "participant", "participant", "participant", "participant", "participant", "participant", "participant", "participant", "participant", "partner", "partner", "partner", "partner", "partner", "partner", "partner", "partner", "partner", "partner", "partnership", "partnership", "partnership", "partnership", "partnership", "partnership", "partnership", "partnership", "partnership", "partnership", "password", "password", "password", "password", "password", "password", "password", "password", "password", "password", "patent", "patent", "patent", "patent", "patent", "patent", "patent", "patent", "patent", "patent", "pathology", "pathology", "pathology", "pathology", "pathology", "pathology", "pathology", "pathology", "patient", "patient", "patient", "patient", "patient", "patient", "patient", "patient", "patient", "patient", "paywall", "paywall", "paywall", "paywall", "paywall", "people", "people", "people", "people", "people", "people", "people", "people", "people", "people", "perceive", "perceive", "perceive", "perceive", "perceive", "perceive", "perceive", "perceive", "perceive", "perceive", "perception", "perception", "perception", "perception", "perception", "perception", "perception", "perception", "perception", "perception", "perceptual_congruence", "perceptual_congruence", "perceptual_congruence", "perceptual_congruence", "period", "period", "period", "period", "period", "period", "period", "period", "period", "period", "permission_back", "permission_back", "permission_back", "permission_back", "permission_back", "perpetual_software", "perpetual_software", "persistent_linking", "persistent_linking", "persistent_linking", "persistent_linking", "persistent_linking", "persistent_linking", "persistent_linking", "persistent_linking", "personal", "personal", "personal", "personal", "personal", "personal", "personal", "personal", "personal", "personal", "personality", "personality", "personality", "personality", "personality", "personality", "personality", "personality", "personality", "personality", "personalization", "personalization", "personalization", "personalization", "personalization", "personalization", "personalization", "personalization", "personalization", "personalization", "physical_countermeasure", "physical_countermeasure", "physician", "physician", "physician", "physician", "physician", "physician", "physician", "physician", "physician", "physician", "pickup_location", "piracy", "piracy", "piracy", "piracy", "piracy", "piracy", "piracy", "piracy", "piracy", "piracy", "pirate", "pirate", "pirate", "pirate", "pirate", "pirate", "pirate", "pirate", "pirate", "pitching", "plagiarism_detection", "platform", "platform", "platform", "platform", "platform", "platform", "platform", "platform", "platform", "platform", "player", "player", "player", "player", "player", "player", "player", "player", "player", "player", "pluck", "policy", "policy", "policy", "policy", "policy", "policy", "policy", "policy", "policy", "policy", "politeness_courtesy", "politeness_courtesy", "political_conservatism", "politician", "politician", "politician", "politician", "politician", "politician", "politician", "politician", "politician", "polycentric_governance", "polygon", "polygon", "polygon", "polygraph", "polygraph", "polygraph", "polymorphic_warning", "polymorphic_warning", "polytope", "polytope", "post", "post", "post", "post", "post", "post", "post", "post", "post", "post", "predictable_variability", "preference", "preference", "preference", "preference", "preference", "preference", "preference", "preference", "preference", "preference", "prevention_mitigation", "price", "price", "price", "price", "price", "price", "price", "price", "price", "price", "pricing", "pricing", "pricing", "pricing", "pricing", "pricing", "pricing", "pricing", "pricing", "pricing", "primal_dual", "primal_dual", "principle", "principle", "principle", "principle", "principle", "principle", "principle", "principle", "principle", "principle", "privacy", "privacy", "privacy", "privacy", "privacy", "privacy", "privacy", "privacy", "privacy", "privacy", "privacy_seal", "privacy_seal", "privacy_seal", "prn", "prn", "proactive_stance", "probabilistic_expander", "probabilistic_expander", "probabilistic_expander", "probability", "probability", "probability", "probability", "probability", "probability", "probability", "probability", "probability", "probability", "procedural_rationality", "procedural_rationality", "procedural_rationality", "procedural_rationality", "procedural_rationality", "procurement", "procurement", "procurement", "procurement", "procurement", "procurement", "procurement", "procurement", "procurement", "procurement", "production", "production", "production", "production", "production", "production", "production", "production", "production", "production", "productivity", "productivity", "productivity", "productivity", "productivity", "productivity", "productivity", "productivity", "productivity", "productivity", "productivity_tenured", "professional", "professional", "professional", "professional", "professional", "professional", "professional", "professional", "professional", "professional", "profit", "profit", "profit", "profit", "profit", "profit", "profit", "profit", "profit", "profit", "project", "project", "project", "project", "project", "project", "project", "project", "project", "project", "prolific_scholar", "prolific_scholar", "promotion", "promotion", "promotion", "promotion", "promotion", "promotion", "promotion", "promotion", "promotion", "promotion", "proof_theorem", "proof_theorem", "proof_theorem", "proof_theorem", "proof_theorem", "proof_theorem", "proof_theorem", "proof_theorem", "proportion_pragmatist", "proposer", "proposer", "proposer", "proposer", "proposer_responder", "proposer_responder", "proprietary_software", "proprietary_software", "proprietary_software", "proprietary_software", "proprietary_software", "proprietary_software", "proprietary_software", "prot", "protein_impurity", "psychology", "psychology", "psychology", "psychology", "psychology", "psychology", "psychology", "psychology", "psychology", "psychology", "pupil_dilation", "pupil_dilation", "pupil_dilation", "purchase", "purchase", "purchase", "purchase", "purchase", "purchase", "purchase", "purchase", "purchase", "purchase", "purchase_funnel", "purchase_funnel", "purity_yield", "quantity", "quantity", "quantity", "quantity", "quantity", "quantity", "quantity", "quantity", "quantity", "quantity", "queue", "queue", "queue", "queue", "queue", "queue", "queue", "queue", "queue", "queue", "queue_length", "queue_length", "queue_length", "queue_length", "queue_length", "racial_hate", "racial_hate", "racial_hate", "railcar", "railcar", "railcar", "random", "random", "random", "random", "random", "random", "random", "random", "random", "random", "ransomware", "rating", "rating", "rating", "rating", "rating", "rating", "rating", "rating", "rating", "rating", "raw_signal", "raw_signal", "readmission", "readmission", "readmission", "readmission", "readmission", "readmission", "readmission", "readmission", "readmission", "readoption", "readoption", "really", "really", "really", "really", "really", "really", "really", "really", "really", "really", "recommendation", "recommendation", "recommendation", "recommendation", "recommendation", "recommendation", "recommendation", "recommendation", "recommendation", "recommendation", "red_queen", "reformulation", "reformulation", "reformulation", "reformulation", "reformulation", "reformulation", "reformulation", "reformulation", "regulatory_burden", "reinvention", "reinvention", "reinvention", "reinvention", "reluctant_leaver", "reluctant_leaver", "reluctant_stayer", "reluctant_stayer", "remanufacturing", "remanufacturing", "remanufacturing", "remanufacturing", "remanufacturing", "remanufacturing", "remittance", "remittance", "remittance", "remittance", "remittance", "remittance", "remunerative", "reporting_aggressiveness", "reprint_copy", "reprint_copy", "reprint_copy", "reprint_quantity", "reprint_quantity", "reprint_quantity", "reprint_quantity", "reprint_quantity", "reprint_quantity", "reproduce_transmit", "reproduce_transmit", "reproduce_transmit", "reproduce_transmit", "reproduce_transmit", "reproduce_transmit", "reputation", "reputation", "reputation", "reputation", "reputation", "reputation", "reputation", "reputation", "reputation", "reputation", "researcher", "researcher", "researcher", "researcher", "researcher", "researcher", "researcher", "researcher", "researcher", "researcher", "reset", "reset", "reset", "reset", "reset", "reset", "reset", "reset", "reset", "reset", "resistance", "resistance", "resistance", "resistance", "resistance", "resistance", "resistance", "resistance", "resistance", "resistance", "restatement", "restatement", "restatement", "restatement", "restatement", "resupply", "resupply", "retailer", "retailer", "retailer", "retailer", "retailer", "retailer", "retailer", "retailer", "retailer", "retailer", "retraction", "retraction", "retraction", "retransmission_intention", "retransmission_intention", "return", "return", "return", "return", "return", "return", "return", "return", "return", "return", "retweet", "retweet", "retweet", "retweet", "retweet", "retweet", "retweet", "retweete", "retweete", "retweete", "retweete", "reuse", "reuse", "reuse", "reuse", "reuse", "reuse", "reuse", "reuse", "reuse", "reuse", "revenue", "revenue", "revenue", "revenue", "revenue", "revenue", "revenue", "revenue", "revenue", "revenue", "right_restriction", "right_restriction", "right_restriction", "right_restriction", "right_restriction", "right_restriction", "right_restriction", "right_restriction", "right_restriction", "rigidity", "rigidity", "rigidity", "rigidity", "rigidity", "rigidity", "rigidity", "rigidity", "rigidity", "rigidity", "rigidity_detection", "rigidity_detection", "risk", "risk", "risk", "risk", "risk", "risk", "risk", "risk", "risk", "risk", "round_trip", "round_trip", "round_trip", "safe_computing", "safe_computing", "salary_premium", "salary_premium", "salary_premium", "sale", "sale", "sale", "sale", "sale", "sale", "sale", "sale", "sale", "sale", "salesperson", "salesperson", "salesperson", "salesperson", "salesperson", "salesperson", "salesperson", "salesperson", "salesperson", "salesperson", "sample", "sample", "sample", "sample", "sample", "sample", "sample", "sample", "sample", "sample", "sanitization", "sanitization", "sapse", "satisfaction", "satisfaction", "satisfaction", "satisfaction", "satisfaction", "satisfaction", "satisfaction", "satisfaction", "satisfaction", "satisfaction", "say", "say", "say", "say", "say", "say", "say", "say", "say", "say", "scam", "scam", "scam", "scam", "scam", "scam", "scam", "scent", "scent", "scent", "scent", "scent", "schedule", "schedule", "schedule", "schedule", "schedule", "schedule", "schedule", "schedule", "schedule", "schedule", "scheduling", "scheduling", "scheduling", "scheduling", "scheduling", "scheduling", "scheduling", "scheduling", "scheduling", "scheduling", "scientometric", "scientometric", "scm", "scm", "scm", "scm", "scm", "scm", "scm", "scm", "scm", "score", "score", "score", "score", "score", "score", "score", "score", "score", "score", "search", "search", "search", "search", "search", "search", "search", "search", "search", "search", "security", "security", "security", "security", "security", "security", "security", "security", "security", "security", "seeding", "seeding", "seeding", "seeding", "seeding", "seeding", "seeding", "seeding", "selection", "selection", "selection", "selection", "selection", "selection", "selection", "selection", "selection", "selection", "self", "self", "self", "self", "self", "self", "self", "self", "self", "self", "sell_solicitation", "seller", "seller", "seller", "seller", "seller", "seller", "seller", "seller", "seller", "seller", "semantic_theme", "semantic_theme", "sentiment", "sentiment", "sentiment", "sentiment", "sentiment", "sentiment", "sentiment", "sentiment", "sentiment", "sentiment", "sequentiality", "sequentiality", "serviceability", "shakedown_phase", "shakedown_phase", "shakedown_phase", "shakedown_phase", "shareholder", "shareholder", "shareholder", "shareholder", "shareholder", "shareholder", "shareholder", "shareholder", "shareholder", "shareholder", "shopping", "shopping", "shopping", "shopping", "shopping", "shopping", "shopping", "shopping", "shopping", "shopping", "shopping_assistance", "signal", "signal", "signal", "signal", "signal", "signal", "signal", "signal", "signal", "signal", "signifier", "signifier", "signifier", "signifier", "silk_road", "site", "site", "site", "site", "site", "site", "site", "site", "site", "site", "size", "size", "size", "size", "size", "size", "size", "size", "size", "size", "size_overbooking", "skill", "skill", "skill", "skill", "skill", "skill", "skill", "skill", "skill", "skill", "slant", "slant", "slant", "slant", "slant", "slant", "slant", "slant", "smart_connected", "smart_connected", "smart_connected", "smart_connected", "smart_connected", "smart_connected", "sociomaterial", "sociomaterial", "sociomaterial", "sociomaterial_routine", "sociomaterial_routine", "software", "software", "software", "software", "software", "software", "software", "software", "software", "software", "solution", "solution", "solution", "solution", "solution", "solution", "solution", "solution", "solution", "solution", "solve", "solve", "solve", "solve", "solve", "solve", "solve", "solve", "solve", "solve", "song", "song", "song", "song", "song", "song", "song", "song", "song", "song", "specialization", "specialization", "specialization", "specialization", "specialization", "specialization", "specialization", "specialization", "specialization", "specialization", "spectacle", "spectacle", "spectacle", "spectacle", "spectacle", "speculator", "speculator", "speculator", "spillover", "spillover", "spillover", "spillover", "spillover", "spillover", "spillover", "spillover", "spillover", "spillover", "springboard", "springboard", "springboard", "springboard", "springboard", "springboard", "springboard", "springboard", "springboard", "st", "st", "st", "st", "st", "st", "st", "st", "st", "stack_classifier", "stakeholder", "stakeholder", "stakeholder", "stakeholder", "stakeholder", "stakeholder", "stakeholder", "stakeholder", "stakeholder", "stakeholder", "standard", "standard", "standard", "standard", "standard", "standard", "standard", "standard", "standard", "standard", "state", "state", "state", "state", "state", "state", "state", "state", "state", "state", "stent", "stent", "stigmatize", "stigmatize", "stigmatize", "stigmatize", "stochastic_frontier", "stochastic_frontier", "stochastic_frontier", "stochastic_frontier", "stochastic_kriging", "stochastic_kriging", "stock", "stock", "stock", "stock", "stock", "stock", "stock", "stock", "stock", "stock", "store", "store", "store", "store", "store", "store", "store", "store", "store", "store", "store_liquidation", "store_liquidation", "strategic", "strategic", "strategic", "strategic", "strategic", "strategic", "strategic", "strategic", "strategic", "strategic", "strategic_posture", "strategic_posture", "strategic_posture", "stretch_goal", "stretch_goal", "stretch_goal", "subject", "subject", "subject", "subject", "subject", "subject", "subject", "subject", "subject", "subject", "subjective_norm", "subjective_norm", "subjective_norm", "subjective_norm", "subjective_norm", "subjective_norm", "subjective_norm", "subjective_norm", "subjective_norm", "subjective_vitality", "subjective_vitality", "subjective_vitality", "subsidy_scheme", "summary_pile", "summary_pile", "summary_pile", "supplier", "supplier", "supplier", "supplier", "supplier", "supplier", "supplier", "supplier", "supplier", "supplier", "supply", "supply", "supply", "supply", "supply", "supply", "supply", "supply", "supply", "supply", "supply_chain", "supply_chain", "supply_chain", "supply_chain", "supply_chain", "supply_chain", "supply_chain", "supply_chain", "supply_chain", "supply_chain", "survey", "survey", "survey", "survey", "survey", "survey", "survey", "survey", "survey", "survey", "sustainability", "sustainability", "sustainability", "sustainability", "sustainability", "sustainability", "sustainability", "sustainability", "sustainability", "sustainability", "sustainable", "sustainable", "sustainable", "sustainable", "sustainable", "sustainable", "sustainable", "sustainable", "sustainable", "sustainable", "syllabus_mean", "syllabus_mean", "syllabus_mean", "syllabus_mean", "syllabus_mean", "symbolization", "symbolization", "tablet", "tablet", "tablet", "tablet", "tablet", "tablet", "tablet", "tablet", "tablet", "tablet_commerce", "talent", "talent", "talent", "talent", "talent", "talent", "talent", "talent", "talent", "talent", "target", "target", "target", "target", "target", "target", "target", "target", "target", "target", "target_couponing", "target_couponing", "targeting", "targeting", "task", "task", "task", "task", "task", "task", "task", "task", "task", "task", "taskforce", "taskforce", "team", "team", "team", "team", "team", "team", "team", "team", "team", "team", "technological_turbulence", "technological_turbulence", "technological_turbulence", "technological_turbulence", "technological_turbulence", "telemedicine_consultation", "temperance", "temporal_dissociation", "temporal_landmark", "ternary_relationship", "ternary_relationship", "tfp", "tfp", "tfp_growth", "tfp_growth", "theorem", "theorem", "theorem", "theorem", "theorem", "theorem", "theorem", "theorem", "theorem", "theorem", "theoretical", "theoretical", "theoretical", "theoretical", "theoretical", "theoretical", "theoretical", "theoretical", "theoretical", "theoretical", "thing", "thing", "thing", "thing", "thing", "thing", "thing", "thing", "thing", "thing", "think", "think", "think", "think", "think", "think", "think", "think", "think", "think", "thinklet", "thinklet", "threat", "threat", "threat", "threat", "threat", "threat", "threat", "threat", "threat", "threat", "tie", "tie", "tie", "tie", "tie", "tie", "tie", "tie", "tie", "tie", "tmp", "tmp", "tms", "tms", "tool", "tool", "tool", "tool", "tool", "tool", "tool", "tool", "tool", "tool", "top", "top", "top", "top", "top", "top", "top", "top", "top", "top", "topical_crawler", "topicality", "topicality", "topicality_status", "trade", "trade", "trade", "trade", "trade", "trade", "trade", "trade", "trade", "trade", "trading", "trading", "trading", "trading", "trading", "trading", "trading", "trading", "trading", "trading", "training", "training", "training", "training", "training", "training", "training", "training", "training", "training", "transaction", "transaction", "transaction", "transaction", "transaction", "transaction", "transaction", "transaction", "transaction", "transaction", "transactive_memory", "transactive_memory", "transactive_memory", "transactive_memory", "transactive_memory", "transactive_memory", "translucence", "transmission_velocity", "treatment", "treatment", "treatment", "treatment", "treatment", "treatment", "treatment", "treatment", "treatment", "treatment", "triage", "triage", "triage", "triage", "triage", "triage", "triage", "triage", "triage", "trust", "trust", "trust", "trust", "trust", "trust", "trust", "trust", "trust", "trust", "truth_teller", "truth_teller", "turnover", "turnover", "turnover", "turnover", "turnover", "turnover", "turnover", "turnover", "turnover", "turnover", "tweet", "tweet", "tweet", "tweet", "tweet", "tweet", "tweet", "tweet", "tweet", "tweet", "twitter", "twitter", "twitter", "twitter", "twitter", "twitter", "twitter", "twitter", "twitter", "twitter", "umetric", "uncertainty_quantification", "unconcerned_pragmatist", "uncover_unobserved", "understaffe_retail", "uninformed_trader", "uninformed_trader", "valence", "valence", "valence", "valence", "valence", "valence", "valence", "valence", "valence", "valence", "valence_elasticity", "valence_elasticity", "vector", "vector", "vector", "vector", "vector", "vector", "vector", "vector", "vector", "vector", "vendor_silence", "vendor_silence", "venture", "venture", "venture", "venture", "venture", "venture", "venture", "venture", "venture", "venture", "vertex", "vertex", "vertex", "vertex", "vertex", "vertex", "vertex", "vertex", "vertex", "vertical_integration", "vertical_integration", "vertical_integration", "vertical_integration", "vertical_integration", "vertical_integration", "vertical_integration", "vertical_integration", "vertical_integration", "violation", "violation", "violation", "violation", "violation", "violation", "violation", "violation", "violation", "violation", "viral", "viral", "viral", "viral", "viral", "viral", "viral", "viral", "viral", "viral", "virality", "virtual", "virtual", "virtual", "virtual", "virtual", "virtual", "virtual", "virtual", "virtual", "virtual", "virtual_world", "virtual_world", "virtual_world", "virtual_world", "virtual_world", "virtual_world", "virtual_world", "virtual_world", "virtual_world", "virtual_world", "virtuality", "virtuality", "virtuality", "virtuality", "vocal_pitch", "voluntary_profiling", "voxel", "wait", "wait", "wait", "wait", "wait", "wait", "wait", "wait", "wait", "wait", "want", "want", "want", "want", "want", "want", "want", "want", "want", "want", "wardrobe", "wardrobe", "wardrobe", "wardrobe", "wardrobe", "web", "web", "web", "web", "web", "web", "web", "web", "web", "web", "web_delivery", "web_delivery", "web_delivery", "web_delivery", "web_delivery", "web_delivery", "web_delivery", "web_delivery", "web_delivery", "web_delivery", "website", "website", "website", "website", "website", "website", "website", "website", "website", "website", "wom", "wom", "wom", "wom", "wom", "wom", "wom", "wom", "wom", "wom", "wom_valence", "wom_valence", "wom_valence", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word_mouth", "word_mouth", "word_mouth", "word_mouth", "word_mouth", "word_mouth", "word_mouth", "word_mouth", "word_mouth", "word_mouth", "worker", "worker", "worker", "worker", "worker", "worker", "worker", "worker", "worker", "worker", "workgroup_norm", "wsq", "wsq", "xn", "xn", "yearly_quarterly", "yearly_quarterly", "yearly_quarterly", "yoplait"]}, "R": 30, "lambda.step": 0.01, "plot.opts": {"xlab": "PC1", "ylab": "PC2"}, "topic.order": [8, 6, 2, 5, 9, 1, 3, 10, 4, 7]};

function LDAvis_load_lib(url, callback){
  var s = document.createElement('script');
  s.src = url;
  s.async = true;
  s.onreadystatechange = s.onload = callback;
  s.onerror = function(){console.warn("failed to load library " + url);};
  document.getElementsByTagName("head")[0].appendChild(s);
}

if(typeof(LDAvis) !== "undefined"){
   // already loaded: just create the visualization
   !function(LDAvis){
       new LDAvis("#" + "ldavis_el3333140501354869024505943978", ldavis_el3333140501354869024505943978_data);
   }(LDAvis);
}else if(typeof define === "function" && define.amd){
   // require.js is available: use it to load d3/LDAvis
   require.config({paths: {d3: "https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min"}});
   require(["d3"], function(d3){
      window.d3 = d3;
      LDAvis_load_lib("https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js", function(){
        new LDAvis("#" + "ldavis_el3333140501354869024505943978", ldavis_el3333140501354869024505943978_data);
      });
    });
}else{
    // require.js not available: dynamically load d3 & LDAvis
    LDAvis_load_lib("https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js", function(){
         LDAvis_load_lib("https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js", function(){
                 new LDAvis("#" + "ldavis_el3333140501354869024505943978", ldavis_el3333140501354869024505943978_data);
            })
         });
}
</script></div></div>
</div>
<p>We need <strong>pandas</strong> to present the most important words in a dataframe.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div>
</div>
</div>
</div>
<p>The following code builds a dataframe from the ten most important words for each topic. Now our task would be to figure out the topics from these words.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">top_words_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">temp_words</span> <span class="o">=</span> <span class="n">lda_model</span><span class="o">.</span><span class="n">show_topic</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">just_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">name</span> <span class="k">for</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span><span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="n">temp_words</span><span class="p">]</span>
    <span class="n">top_words_df</span><span class="p">[</span><span class="s1">&#39;Topic &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">just_words</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">top_words_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Topic 1</th>
      <th>Topic 2</th>
      <th>Topic 3</th>
      <th>Topic 4</th>
      <th>Topic 5</th>
      <th>Topic 6</th>
      <th>Topic 7</th>
      <th>Topic 8</th>
      <th>Topic 9</th>
      <th>Topic 10</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>privacy</td>
      <td>project</td>
      <td>network</td>
      <td>team</td>
      <td>manager</td>
      <td>price</td>
      <td>capability</td>
      <td>consumer</td>
      <td>price</td>
      <td>sale</td>
    </tr>
    <tr>
      <th>1</th>
      <td>security</td>
      <td>innovation</td>
      <td>investment</td>
      <td>task</td>
      <td>marketing</td>
      <td>optimal</td>
      <td>organizational</td>
      <td>marketing</td>
      <td>patient</td>
      <td>participant</td>
    </tr>
    <tr>
      <th>2</th>
      <td>perceive</td>
      <td>idea</td>
      <td>innovation</td>
      <td>member</td>
      <td>analytic</td>
      <td>demand</td>
      <td>construct</td>
      <td>brand</td>
      <td>demand</td>
      <td>retailer</td>
    </tr>
    <tr>
      <th>3</th>
      <td>patient</td>
      <td>organizational</td>
      <td>country</td>
      <td>network</td>
      <td>people</td>
      <td>distribution</td>
      <td>patent</td>
      <td>medium</td>
      <td>period</td>
      <td>trust</td>
    </tr>
    <tr>
      <th>4</th>
      <td>website</td>
      <td>action</td>
      <td>capability</td>
      <td>organizational</td>
      <td>employee</td>
      <td>policy</td>
      <td>strategic</td>
      <td>price</td>
      <td>policy</td>
      <td>consumer</td>
    </tr>
    <tr>
      <th>5</th>
      <td>web</td>
      <td>digital</td>
      <td>strategic</td>
      <td>project</td>
      <td>financial</td>
      <td>inform</td>
      <td>item</td>
      <td>purchase</td>
      <td>capacity</td>
      <td>store</td>
    </tr>
    <tr>
      <th>6</th>
      <td>participant</td>
      <td>activity</td>
      <td>platform</td>
      <td>job</td>
      <td>risk</td>
      <td>contract</td>
      <td>feature</td>
      <td>search</td>
      <td>inventory</td>
      <td>app</td>
    </tr>
    <tr>
      <th>7</th>
      <td>consumer</td>
      <td>community</td>
      <td>standard</td>
      <td>employee</td>
      <td>big</td>
      <td>operation</td>
      <td>communication</td>
      <td>network</td>
      <td>risk</td>
      <td>game</td>
    </tr>
    <tr>
      <th>8</th>
      <td>intention</td>
      <td>software</td>
      <td>digital</td>
      <td>communication</td>
      <td>corporate</td>
      <td>solution</td>
      <td>manager</td>
      <td>content</td>
      <td>operation</td>
      <td>sample</td>
    </tr>
    <tr>
      <th>9</th>
      <td>attitude</td>
      <td>goal</td>
      <td>risk</td>
      <td>tie</td>
      <td>return</td>
      <td>supplier</td>
      <td>operation</td>
      <td>sale</td>
      <td>pricing</td>
      <td>task</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The following steps will build a figure with the evolution of each topic. These are calculated by evaluating the weight of each topic in the documents for a certain year and then summing up these weights.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">evolution</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">corpus</span><span class="p">),</span><span class="mi">10</span><span class="p">])</span> <span class="c1"># We pre-build the numpy array filled with zeroes.</span>
<span class="n">ind</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">bow</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">:</span>
    <span class="n">topics</span> <span class="o">=</span> <span class="n">lda_model</span><span class="o">.</span><span class="n">get_document_topics</span><span class="p">(</span><span class="n">bow</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">topic</span> <span class="ow">in</span> <span class="n">topics</span><span class="p">:</span>
        <span class="n">evolution</span><span class="p">[</span><span class="n">ind</span><span class="p">,</span><span class="n">topic</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">topic</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">ind</span><span class="o">+=</span><span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<p>We create a pandas dataframe from the NumPy array and add the years and column names.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">evolution_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">evolution</span><span class="p">)</span>
<span class="n">evolution_df</span><span class="p">[</span><span class="s1">&#39;Year&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">file_years</span>
<span class="n">evolution_df</span><span class="p">[</span><span class="s1">&#39;Date&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">evolution_df</span><span class="p">[</span><span class="s1">&#39;Year&#39;</span><span class="p">],</span><span class="nb">format</span> <span class="o">=</span> <span class="s2">&quot;%Y&quot;</span><span class="p">)</span> <span class="c1"># Change Year to datetime-object.</span>
<span class="n">evolution_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">&#39;Date&#39;</span><span class="p">,</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># Set Date as an index of the dataframe</span>
<span class="n">evolution_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Year&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">inplace</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;fivethirtyeight&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span><span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">[</span><span class="mi">15</span><span class="p">,</span><span class="mi">10</span><span class="p">])</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span><span class="n">column</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axs</span><span class="o">.</span><span class="n">flat</span><span class="p">,</span><span class="n">evolution_df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;Date&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">evolution_df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;Date&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()[</span><span class="n">column</span><span class="p">])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">column</span><span class="p">,{</span><span class="s1">&#39;fontsize&#39;</span><span class="p">:</span><span class="mi">14</span><span class="p">})</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/10_NLP_in_accounting_83_0.png" src="_images/10_NLP_in_accounting_83_0.png" />
</div>
</div>
<p>Next, we plot the marginal topic distribution, i.e., the relative importance of the topics. It is calculated by summing the topic weights of all documents.</p>
<p>First, we calculate the topic weights for every document.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">doc_tops</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">:</span>
    <span class="n">doc_tops</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">item</span> <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span><span class="n">item</span><span class="p">)</span> <span class="ow">in</span> <span class="n">lda_model</span><span class="o">.</span><span class="n">get_document_topics</span><span class="p">(</span><span class="n">doc</span><span class="p">)])</span>
<span class="n">doc_tops_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">doc_tops</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="n">top_words_df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Then, we sum (and plot) these weights.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">doc_tops_df</span> <span class="o">=</span> <span class="n">doc_tops_df</span><span class="o">/</span><span class="n">doc_tops_df</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">doc_tops_df</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">bar</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fca454fd2b0&gt;
</pre></div>
</div>
<img alt="_images/10_NLP_in_accounting_88_1.png" src="_images/10_NLP_in_accounting_88_1.png" />
</div>
</div>
<p>As our subsequent analysis, let’s search the most representative document for each topic. It is the document that has the largest weight for a certain topic.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">doc_topics</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">:</span>
    <span class="n">doc_topics</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">item</span> <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span><span class="n">item</span><span class="p">)</span> <span class="ow">in</span> <span class="n">lda_model</span><span class="o">.</span><span class="n">get_document_topics</span><span class="p">(</span><span class="n">doc</span><span class="p">)])</span>               
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">temp_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">doc_topics</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>With <strong>idxmax()</strong>, we can pick up the index that has the largest weight.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">temp_df</span><span class="o">.</span><span class="n">idxmax</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0    1172
1    1775
2     858
3    1705
4    1644
5     133
6     517
7    1203
8     303
dtype: int64
</pre></div>
</div>
</div>
</div>
<p>We can now use <strong>files</strong> to connect indices to documents. For example, the most representative document of Topic 1 (index 0) is “PRACTICING SAFE COMPUTING: A MULTIMETHOD EMPIRICAL EXAMINATION OF HOME COMPUTER USER SECURITY BEHAVIORAL INTENTIONS”</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">files</span><span class="p">[</span><span class="mi">1172</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;2010_2729.txt&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">raw_text</span><span class="p">[</span><span class="mi">1172</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">500</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;Anderson &amp; Agarwal/Practicing Safe Computing\n\nSPECIAL ISSUE\n\nPRACTICING SAFE COMPUTING: A MULTIMETHOD EMPIRICAL EXAMINATION OF HOME COMPUTER USER SECURITY BEHAVIORAL INTENTIONS1\n\nBy: Catherine L. Anderson Decision, Operations, and Information Technologies Department Robert H. Smith School of Business University of Maryland Van Munching Hall College Park, MD 20742-1815 U.S.A. Catherine_Anderson@rhsmith.umd.edu\nRitu Agarwal Center for Health Information and Decision Systems Robert H. Smith School &#39;
</pre></div>
</div>
</div>
</div>
<p>Let’s build a master table that has the document names and other information.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">master_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Article&#39;</span><span class="p">:</span><span class="n">files</span><span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<p>First, we add the most important topic of each article to the table.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">top_topic</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">:</span>
    <span class="n">test</span><span class="o">=</span><span class="n">lda_model</span><span class="o">.</span><span class="n">get_document_topics</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
    <span class="n">test2</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">test</span><span class="p">]</span>
    <span class="n">top_topic</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test2</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">test2</span><span class="p">))</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

<span class="n">master_df</span><span class="p">[</span><span class="s1">&#39;Top topic&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">top_topic</span>
</pre></div>
</div>
</div>
</div>
<p>With <strong>head()</strong>, we can check the first (ten) values of our dataframe.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">master_df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Article</th>
      <th>Top topic</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2019_1167.txt</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2017_27218.txt</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2012_40089.txt</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2012_2684.txt</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2015_16392.txt</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>2019_3891.txt</td>
      <td>1</td>
    </tr>
    <tr>
      <th>6</th>
      <td>2011_1162.txt</td>
      <td>1</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2015_19684.txt</td>
      <td>4</td>
    </tr>
    <tr>
      <th>8</th>
      <td>2010_2866.txt</td>
      <td>2</td>
    </tr>
    <tr>
      <th>9</th>
      <td>2018_30301.txt</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p><strong>value_counts()</strong> for the “Top topic” -column can be used to check that in how many documents each topic is the most important.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">master_df</span><span class="p">[</span><span class="s1">&#39;Top topic&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1    794
2    604
3    402
4    210
5     81
6     23
7      9
9      2
8      1
Name: Top topic, dtype: int64
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="summarisation">
<h2><span class="section-number">4.6. </span>Summarisation<a class="headerlink" href="#summarisation" title="Permalink to this headline">¶</a></h2>
<p>Let’s do something else. Gensim also includes efficient summarisation-functions (these are not related to LDA any more):</p>
<p>From the <strong>summarization</strong> -module, we can use <strong>summarize</strong> to automatically build a short summarisation of the document. Notice that we use the original documents for this and not the preprocessed ones. <strong>ratio = 0.01</strong> means that the length of the summarisation should be 1 % from the original document.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">summary</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">raw_text</span><span class="p">:</span>
    <span class="n">summary</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gensim</span><span class="o">.</span><span class="n">summarization</span><span class="o">.</span><span class="n">summarize</span><span class="p">(</span><span class="n">file</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span><span class="s1">&#39; &#39;</span><span class="p">),</span><span class="n">ratio</span><span class="o">=</span><span class="mf">0.01</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Below is an example summary for the first document.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">summary</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;*NLM Title Abbreviation:*     J Appl Psychol *Publisher:*     US : American Psychological Association *ISSN:*     0021-9010 (Print)     1939-1854 (Electronic) *ISBN:*     978-1-4338-9042-0 *Language:*     English *Keywords:*     ability, personality, interests, motivation, individual differences *Abstract:*     This article reviews 100 years of research on individual differences     and their measurement, with a focus on research published in the     Journal of Applied Psychology.\nWe focus on 3 major individual     differences domains: (a) knowledge, skill, and ability, including     both the cognitive and physical domains; (b) personality, including     integrity, emotional intelligence, stable motivational attributes     (e.g., achievement motivation, core self-evaluations), and     creativity; and (c) vocational interests.\n(PsycINFO Database Record (c) 2017 APA, all     rights reserved) *Document Type:*     Journal Article *Subjects:*     *Individual Differences; *Measurement; *Motivation; *Occupational     Interests; *Personality; Ability; Interests *PsycINFO Classification:*     Occupational &amp; Employment Testing (2228)     Occupational Interests &amp; Guidance (3610) *Population:*     Human *Tests &amp; Measures:*     Vocational Preference Inventory *Methodology:*     Literature Review *Supplemental Data:*     Tables and Figures Internet *Format Covered:*     Electronic *Publication Type:*     Journal; Peer Reviewed Journal *Publication History:*     First Posted: Feb 2, 2017; Accepted: Jun 27, 2016; Revised: Jun 2,     2016; First Submitted: Aug 12, 2015 *Release Date:*     20170202 *Correction Date:*     20170309 *Copyright:*     American Psychological Association.\n85)  The development of standardized measures of attributes on which individuals differ emerged very early in psychology’s history, and has been a major theme in research published in the /Journal of Applied Psychology/ (/JAP/).\nThus, ability, personality, interest patterns, and motivational traits (e.g., achievement motivation, core self-evaluations [CSEs]) fall under the individual differences umbrella, whereas variables that are transient, such as mood, or that are closely linked to the specifics of the work setting (e.g., turnover intentions or perceived organizational climate), do not.\nThe very first volume of the journal contained research on topics that are core topics of research today, including criterion-related validity (Terman et al., 1917 &lt;#c252&gt;), bias and group differences (Sunne, 1917 &lt;#c245&gt;), measurement issues (Miner, 1917 &lt;#c181&gt;; Yerkes, 1917 &lt;#c282&gt;), and relationships with learning and the development of knowledge and skill (Bingham, 1917 &lt;#c26&gt;).\nOver the years, studies have reflected the tension between viewing cognitive abilities as enduring capacities that are largely innate versus treating them as measures of developed capabilities, a distinction that has implications for the study of criterion related validity, group differences, aging effects, and the structure of human abilities (Kuncel &amp; Beatty, 2013 &lt;#c155&gt;).\nIn addition to the direct study of knowledge and skill measurement, these individual differences have been a part of research on job analysis, leadership, career development, performance appraisal, training, and skill acquisition, among others.\nIn a well-cited /JAP/ article, Ghiselli and Barthol (1953) &lt;#c94&gt; summarized this first era by reviewing 113 studies about the validity of personality inventories for predicting job performance.\nBy using only measures designed to assess the FFM, Hurtz and Donovan’s (2000) &lt;#c129&gt; meta-analysis addressed potential construct-related validity concerns of prior meta-analyses and found evidence for personality as a predictor of task and contextual performance.\nSeveral meta-analyses, large-scale studies, and simulations were published in /JAP/ that demonstrated the limited impact of socially desirable responding and faking on the construct-related and criterion-related validity of personality measures (Ellingson, Smith, &amp; Sackett, 2001 &lt;#c66&gt;; Ones, Viswesvaran, &amp; Reiss, 1996 &lt;#c200&gt;; and, later on, Hogan, Barrett, &amp; Hogan, 2007 &lt;#c112&gt;; Schmitt &amp; Oswald, 2006 &lt;#c229&gt;).\nIn /JAP/, research has also appeared measuring personality with conditional reasoning tests (Bing et al., 2007 &lt;#c25&gt;), SJTs (Motowidlo, Hooper, &amp; Jackson, 2006 &lt;#c185&gt;), structured interviews (Van Iddekinge, Raymark, &amp; Roth, 2005 &lt;#c268&gt;), and ideal point models (Stark, Chernyshenko, Drasgow, &amp; Williams, 2006 &lt;#c240&gt;), though not all of these approaches demonstrated incremental prediction over self-reports.\nSome of the more notable advancements are that (a) personality can interact with the situation to affect behavior (e.g., Tett &amp; Burnett’s [2003] &lt;#c253&gt; trait activation theory); (b) motivational forces mediate the effects of personality (e.g., Barrick, Stewart, &amp; Piotrowski, 2002 &lt;#c18&gt;), with both implicit and explicit motives being important (Frost, Ko, &amp; James, 2007 &lt;#c86&gt;; Lang, Zettler, Ewen, &amp; Hülsheger, 2012 &lt;#c157&gt;); (c) personality is stable, yet also prone to change, across life (Woods &amp; Hampson, 2010 &lt;#c279&gt;); (d) personality both affects and is affected by work (Wille &amp; De Fruyt, 2014 &lt;#c275&gt;); and (e) personality traits represent stable distributions of variable personality states (Judge, Simon, Hurst, &amp; Kelley, 2014 &lt;#c147&gt;; Minbashian, Wood, &amp; Beckmann, 2010 &lt;#c180&gt;).\n/JAP/ has published several articles that have contributed to the understanding of integrity tests, including Hogan and Hogan (1989) &lt;#c113&gt; on the measurement of employee reliability, and studies that examined faking (Cunningham, Wong, &amp; Barbee, 1994 &lt;#c51&gt;), subgroup differences (Ones &amp; Viswesvaran, 1998 &lt;#c199&gt;), and difficulties in predicting low base rate behaviors (Murphy, 1987 &lt;#c188&gt;).\nIt is worth noting that other individual difference measures have also been examined in /JAP/ as predictors of CWB, including biodata (Rosenbaum, 1976 &lt;#c217&gt;), conditional reasoning (Bing et al., 2007 &lt;#c25&gt;; LeBreton, Barksdale, Robin, &amp; James, 2007 &lt;#c161&gt;), and the Big Five personality dimensions (Berry, Ones, &amp; Sackett, 2007 &lt;#c22&gt;).\nFourth, many studies during this period focused on the vocational interests of particular groups or of individuals in specific jobs or occupations, including tuberculosis patients (Shultz &amp; Rush, 1942 &lt;#c235&gt;), industrial psychology students (Lawshe &amp; Deutsch, 1952 &lt;#c159&gt;), retired YMCA secretaries (Verburg, 1952 &lt;#c271&gt;), and female computer programmers (Perry &amp; Cannon, 1968 &lt;#c206&gt;).&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">master_df</span><span class="p">[</span><span class="s1">&#39;Summaries&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">summary</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">master_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Article</th>
      <th>Top topic</th>
      <th>Summaries</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2019_1167.txt</td>
      <td>1</td>
      <td>We add to the literature on the economics of c...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2017_27218.txt</td>
      <td>3</td>
      <td>*NLM Title Abbreviation:*     J Appl Psychol *...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2012_40089.txt</td>
      <td>3</td>
      <td>Service Quality in Software-as-a-Service: Deve...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2012_2684.txt</td>
      <td>2</td>
      <td>RESEARCH ARTICLE  UNDERSTANDING USER REVISIONS...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2015_16392.txt</td>
      <td>1</td>
      <td>*Document Type:*     Article *Subject Terms:* ...</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>2121</th>
      <td>2016_2942.txt</td>
      <td>1</td>
      <td>{rajiv.kohli@mason.wm.edu} Sharon Swee-Lin Tan...</td>
    </tr>
    <tr>
      <th>2122</th>
      <td>2016_81.txt</td>
      <td>2</td>
      <td>J Bus Ethics (2016) 138:349364 DOI 10.1007/s10...</td>
    </tr>
    <tr>
      <th>2123</th>
      <td>2018_13221.txt</td>
      <td>6</td>
      <td>Sinha Carlson School of Management, University...</td>
    </tr>
    <tr>
      <th>2124</th>
      <td>2016_23236.txt</td>
      <td>1</td>
      <td>*Document Type:*     Article *Subject Terms:* ...</td>
    </tr>
    <tr>
      <th>2125</th>
      <td>2016_38632.txt</td>
      <td>4</td>
      <td>To understand the trade-offs involved in decid...</td>
    </tr>
  </tbody>
</table>
<p>2126 rows × 3 columns</p>
</div></div></div>
</div>
<p>The same <strong>summarization</strong> -module also includes a function to search keywords from the documents. It works in the same way as <strong>summarize()</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">keywords</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">raw_text</span><span class="p">:</span>
    <span class="n">keywords</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gensim</span><span class="o">.</span><span class="n">summarization</span><span class="o">.</span><span class="n">keywords</span><span class="p">(</span><span class="n">file</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span><span class="s1">&#39; &#39;</span><span class="p">),</span><span class="n">ratio</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span><span class="s1">&#39; &#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Here are keywords for the second document</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">keywords</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;journal journals psychology psychological personality person personal research researchers researched performance performing perform performed performances testing tests test tested measurement measures measureable measure measured measuring ability abilities difference different differing differs study studies studied individual differences individuals differ job jobs interests interesting interested including include includes included traits trait&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">master_df</span><span class="p">[</span><span class="s1">&#39;Keywords&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">keywords</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">master_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Article</th>
      <th>Top topic</th>
      <th>Summaries</th>
      <th>Keywords</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2019_1167.txt</td>
      <td>1</td>
      <td>We add to the literature on the economics of c...</td>
      <td>bitcoin bitcoins user users transactions trans...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2017_27218.txt</td>
      <td>3</td>
      <td>*NLM Title Abbreviation:*     J Appl Psychol *...</td>
      <td>journal journals psychology psychological pers...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2012_40089.txt</td>
      <td>3</td>
      <td>Service Quality in Software-as-a-Service: Deve...</td>
      <td>service services saas research researchers cus...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2012_2684.txt</td>
      <td>2</td>
      <td>RESEARCH ARTICLE  UNDERSTANDING USER REVISIONS...</td>
      <td>use uses usefulness features feature user user...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2015_16392.txt</td>
      <td>1</td>
      <td>*Document Type:*     Article *Subject Terms:* ...</td>
      <td>automation automated automate work working hum...</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>2121</th>
      <td>2016_2942.txt</td>
      <td>1</td>
      <td>{rajiv.kohli@mason.wm.edu} Sharon Swee-Lin Tan...</td>
      <td>data ehr ehrs research researchers health pati...</td>
    </tr>
    <tr>
      <th>2122</th>
      <td>2016_81.txt</td>
      <td>2</td>
      <td>J Bus Ethics (2016) 138:349364 DOI 10.1007/s10...</td>
      <td>organizational relationship relationships altr...</td>
    </tr>
    <tr>
      <th>2123</th>
      <td>2018_13221.txt</td>
      <td>6</td>
      <td>Sinha Carlson School of Management, University...</td>
      <td>firms firm production product products modelin...</td>
    </tr>
    <tr>
      <th>2124</th>
      <td>2016_23236.txt</td>
      <td>1</td>
      <td>*Document Type:*     Article *Subject Terms:* ...</td>
      <td>reviewer reviews review reviewers reviewed rev...</td>
    </tr>
    <tr>
      <th>2125</th>
      <td>2016_38632.txt</td>
      <td>4</td>
      <td>To understand the trade-offs involved in decid...</td>
      <td>time timing hire hiring hired management manag...</td>
    </tr>
  </tbody>
</table>
<p>2126 rows × 4 columns</p>
</div></div></div>
</div>
</div>
<div class="section" id="similarities">
<h2><span class="section-number">4.7. </span>Similarities<a class="headerlink" href="#similarities" title="Permalink to this headline">¶</a></h2>
<p>As a next example, we analyse the similarities between the documents.</p>
<p>Gensim has a specific function for that: <strong>SparseMatrixSimilarity</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">gensim.similarities</span> <span class="kn">import</span> <span class="n">SparseMatrixSimilarity</span>
</pre></div>
</div>
</div>
</div>
<p>The parameters to the function are the word-frequency corpus and the length of the dictionary.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">index</span> <span class="o">=</span> <span class="n">SparseMatrixSimilarity</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span><span class="n">num_features</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">id2word</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sim_matrix</span> <span class="o">=</span> <span class="n">index</span><span class="p">[</span><span class="n">corpus</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>In the similarity matrix, the diagonal has values 1 (similarity of a document with itself). We replace those values with zero to find the most similar documents from the corpus.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sim_matrix</span><span class="p">)):</span>
    <span class="n">sim_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Seaborn</strong> (https://seaborn.pydata.org/) has a convenient heatmap function to plot similarities. Here is information about Seaborn from their web page: Seaborn is a Python data visualization library based on Matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">sim_matrix</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fca4c47e6d0&gt;
</pre></div>
</div>
<img alt="_images/10_NLP_in_accounting_127_1.png" src="_images/10_NLP_in_accounting_127_1.png" />
</div>
</div>
<p>We search the most similar article by locating the index with a largest value for every column.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">master_df</span><span class="p">[</span><span class="s1">&#39;Most_similar_article&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">sim_matrix</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">master_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Article</th>
      <th>Top topic</th>
      <th>Summaries</th>
      <th>Keywords</th>
      <th>Most_similar_article</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2019_1167.txt</td>
      <td>1</td>
      <td>We add to the literature on the economics of c...</td>
      <td>bitcoin bitcoins user users transactions trans...</td>
      <td>1306</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2017_27218.txt</td>
      <td>3</td>
      <td>*NLM Title Abbreviation:*     J Appl Psychol *...</td>
      <td>journal journals psychology psychological pers...</td>
      <td>2090</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2012_40089.txt</td>
      <td>3</td>
      <td>Service Quality in Software-as-a-Service: Deve...</td>
      <td>service services saas research researchers cus...</td>
      <td>28</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2012_2684.txt</td>
      <td>2</td>
      <td>RESEARCH ARTICLE  UNDERSTANDING USER REVISIONS...</td>
      <td>use uses usefulness features feature user user...</td>
      <td>1796</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2015_16392.txt</td>
      <td>1</td>
      <td>*Document Type:*     Article *Subject Terms:* ...</td>
      <td>automation automated automate work working hum...</td>
      <td>1280</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>2121</th>
      <td>2016_2942.txt</td>
      <td>1</td>
      <td>{rajiv.kohli@mason.wm.edu} Sharon Swee-Lin Tan...</td>
      <td>data ehr ehrs research researchers health pati...</td>
      <td>301</td>
    </tr>
    <tr>
      <th>2122</th>
      <td>2016_81.txt</td>
      <td>2</td>
      <td>J Bus Ethics (2016) 138:349364 DOI 10.1007/s10...</td>
      <td>organizational relationship relationships altr...</td>
      <td>2070</td>
    </tr>
    <tr>
      <th>2123</th>
      <td>2018_13221.txt</td>
      <td>6</td>
      <td>Sinha Carlson School of Management, University...</td>
      <td>firms firm production product products modelin...</td>
      <td>1942</td>
    </tr>
    <tr>
      <th>2124</th>
      <td>2016_23236.txt</td>
      <td>1</td>
      <td>*Document Type:*     Article *Subject Terms:* ...</td>
      <td>reviewer reviews review reviewers reviewed rev...</td>
      <td>1917</td>
    </tr>
    <tr>
      <th>2125</th>
      <td>2016_38632.txt</td>
      <td>4</td>
      <td>To understand the trade-offs involved in decid...</td>
      <td>time timing hire hiring hired management manag...</td>
      <td>412</td>
    </tr>
  </tbody>
</table>
<p>2126 rows × 5 columns</p>
</div></div></div>
</div>
</div>
<div class="section" id="cluster-model">
<h2><span class="section-number">4.8. </span>Cluster model<a class="headerlink" href="#cluster-model" title="Permalink to this headline">¶</a></h2>
<p>Next, we build a topic analysis using a different approach. We create a TF-IDF model from the corpus and apply K-means clustering for that model.</p>
<p>Explanation of TF-IDF from Wikipedia: “In information retrieval, TF–IDF or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modelling. The TF–IDF value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.”</p>
<p>Explanation of K-means clustering from Wikipedia: “k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centres or cluster centroid), serving as a prototype of the cluster…k-means clustering minimizes within-cluster variances (squared Euclidean distances)…”</p>
<p><img alt="kmeans" src="_images/kmeans.gif" /></p>
<p>From <strong>Gensim.models</strong> we pick up <strong>TfidModel</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">TfidfModel</span>
</pre></div>
</div>
</div>
</div>
<p>As parameters, we need the corpus and the dictionary.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tf_idf_model</span> <span class="o">=</span> <span class="n">TfidfModel</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span><span class="n">id2word</span><span class="o">=</span><span class="n">id2word</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We use the model to build up a TF-IDF -transformed corpus.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tform_corpus</span> <span class="o">=</span> <span class="n">tf_idf_model</span><span class="p">[</span><span class="n">corpus</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p><strong>corpus2csc</strong> converts a streamed corpus in bag-of-words format into a sparse matrix, with documents as columns.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">spar_matr</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">matutils</span><span class="o">.</span><span class="n">corpus2csc</span><span class="p">(</span><span class="n">tform_corpus</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">spar_matr</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;32237x2126 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;
	with 1897708 stored elements in Compressed Sparse Column format&gt;
</pre></div>
</div>
</div>
</div>
<p>Sparse matrix to normal array. Also, we need to transpose it for the K-means model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tfidf_matrix</span> <span class="o">=</span> <span class="n">spar_matr</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tfidf_matrix</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.00125196, 0.00243009, 0.00474072, ..., 0.        , 0.        ,
        0.        ],
       [0.        , 0.00594032, 0.        , ..., 0.        , 0.        ,
        0.        ],
       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
        0.        ],
       ...,
       [0.        , 0.0077254 , 0.        , ..., 0.        , 0.        ,
        0.        ],
       [0.00396927, 0.        , 0.        , ..., 0.        , 0.        ,
        0.        ],
       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
        0.        ]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tfidf_matrix</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(2126, 32237)
</pre></div>
</div>
</div>
</div>
<p>Scikit-learn has a function to form a K-means clustering model from a matrix. It is done below. We use ten clusters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>

<span class="n">kmodel</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">kmodel</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">tfidf_matrix</span><span class="p">)</span>

<span class="n">clusters</span> <span class="o">=</span> <span class="n">km</span><span class="o">.</span><span class="n">labels_</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">master_df</span><span class="p">[</span><span class="s1">&#39;Tf_idf_clusters&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">clusters</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">km</span><span class="o">.</span><span class="n">cluster_centers_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 1.33476883e-03,  4.71424426e-03,  2.62007897e-04, ...,
         2.71050543e-20,  3.38813179e-21,  0.00000000e+00],
       [ 6.67748803e-04,  9.98513608e-04,  9.39093177e-04, ...,
         1.00166900e-03,  6.77626358e-21,  5.77802403e-05],
       [ 1.96181867e-04,  3.44260097e-04,  1.83757076e-04, ...,
        -5.42101086e-20,  0.00000000e+00, -3.38813179e-21],
       ...,
       [ 0.00000000e+00,  3.26892198e-03,  1.67826509e-03, ...,
        -8.13151629e-20, -1.69406589e-21, -3.38813179e-21],
       [ 5.84538250e-04,  1.55948712e-03,  2.28571545e-04, ...,
         0.00000000e+00, -3.38813179e-21, -3.38813179e-21],
       [ 5.10233572e-04,  4.11021862e-03,  1.24048649e-03, ...,
        -2.71050543e-20,  1.69406589e-21, -3.38813179e-21]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">master_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Article</th>
      <th>Top topic</th>
      <th>Summaries</th>
      <th>Keywords</th>
      <th>Most_similar_article</th>
      <th>Tf_idf_clusters</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2019_1167.txt</td>
      <td>1</td>
      <td>We add to the literature on the economics of c...</td>
      <td>bitcoin bitcoins user users transactions trans...</td>
      <td>1306</td>
      <td>5</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2017_27218.txt</td>
      <td>3</td>
      <td>*NLM Title Abbreviation:*     J Appl Psychol *...</td>
      <td>journal journals psychology psychological pers...</td>
      <td>2090</td>
      <td>4</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2012_40089.txt</td>
      <td>3</td>
      <td>Service Quality in Software-as-a-Service: Deve...</td>
      <td>service services saas research researchers cus...</td>
      <td>28</td>
      <td>4</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2012_2684.txt</td>
      <td>2</td>
      <td>RESEARCH ARTICLE  UNDERSTANDING USER REVISIONS...</td>
      <td>use uses usefulness features feature user user...</td>
      <td>1796</td>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2015_16392.txt</td>
      <td>1</td>
      <td>*Document Type:*     Article *Subject Terms:* ...</td>
      <td>automation automated automate work working hum...</td>
      <td>1280</td>
      <td>2</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>2121</th>
      <td>2016_2942.txt</td>
      <td>1</td>
      <td>{rajiv.kohli@mason.wm.edu} Sharon Swee-Lin Tan...</td>
      <td>data ehr ehrs research researchers health pati...</td>
      <td>301</td>
      <td>8</td>
    </tr>
    <tr>
      <th>2122</th>
      <td>2016_81.txt</td>
      <td>2</td>
      <td>J Bus Ethics (2016) 138:349364 DOI 10.1007/s10...</td>
      <td>organizational relationship relationships altr...</td>
      <td>2070</td>
      <td>4</td>
    </tr>
    <tr>
      <th>2123</th>
      <td>2018_13221.txt</td>
      <td>6</td>
      <td>Sinha Carlson School of Management, University...</td>
      <td>firms firm production product products modelin...</td>
      <td>1942</td>
      <td>5</td>
    </tr>
    <tr>
      <th>2124</th>
      <td>2016_23236.txt</td>
      <td>1</td>
      <td>*Document Type:*     Article *Subject Terms:* ...</td>
      <td>reviewer reviews review reviewers reviewed rev...</td>
      <td>1917</td>
      <td>5</td>
    </tr>
    <tr>
      <th>2125</th>
      <td>2016_38632.txt</td>
      <td>4</td>
      <td>To understand the trade-offs involved in decid...</td>
      <td>time timing hire hiring hired management manag...</td>
      <td>412</td>
      <td>5</td>
    </tr>
  </tbody>
</table>
<p>2126 rows × 6 columns</p>
</div></div></div>
</div>
<p>Let’s collect the ten most important words for each cluster.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">centroids</span> <span class="o">=</span> <span class="n">km</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[:,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># Sort the words according to their importance.</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_clusters</span><span class="p">):</span>
    <span class="n">j</span><span class="o">=</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cluster </span><span class="si">%d</span><span class="s2"> words:&quot;</span> <span class="o">%</span> <span class="n">j</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="n">centroids</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><span class="mi">10</span><span class="p">]:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39; </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">id2word</span><span class="o">.</span><span class="n">id2token</span><span class="p">[</span><span class="n">ind</span><span class="p">],</span><span class="n">end</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cluster 1 words: patent, movie, tweet, invention, citation, inventor, innovation, twitter, follower, release,

Cluster 2 words: analytic, innovation, digital, capability, governance, supply_chain, sustainability, right_reserve, big, platform,

Cluster 3 words: persistent_linking, site_ehost, true_db, learning_please, academic_licensee, syllabus_mean, newsletter_content, electronic_reserve, authorize_ebscohost, course_pack,

Cluster 4 words: auction, bidder, bid, advertiser, bidding, price, pricing, combinatorial_auction, seller, valuation,

Cluster 5 words: team, employee, job, organizational, project, ethic, security, task, member, community,

Cluster 6 words: consumer, privacy, web_delivery, rating, disclosure, investor, participant, website, app, wom,

Cluster 7 words: brand, marketing, retailer, consumer, advertising, purchase, marketer, elasticity, promotion, sale,

Cluster 8 words: optimization, optimal, theorem, algorithm, node, approximation, scheduling, constraint, parameter, distribution,

Cluster 9 words: patient, hospital, physician, care, healthcare, medical, clinical, hit, health, health_care,

Cluster 10 words: price, inventory, retailer, supplier, pricing, contract, profit, demand, forecast, optimal,
</pre></div>
</div>
</div>
</div>
<p>With a smaller corpus, we could use fancy visualisations, like multidimensional scaling and Ward-clustering, to represent the document relationship. However, with over 2000 documents, that is not meaningful. Below are examples of both (not related to our analysis).</p>
<p><img alt="mds" src="_images/mds.png" /></p>
<p><img alt="ward" src="_images/ward.jpg" /></p>
<p>With pandas <strong>crosstab</strong>, we can easily check the connection between the LDA topics and the K-means clusters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">crosstab</span><span class="p">(</span><span class="n">master_df</span><span class="p">[</span><span class="s1">&#39;Top topic&#39;</span><span class="p">],</span><span class="n">master_df</span><span class="p">[</span><span class="s1">&#39;Tf_idf_clusters&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Tf_idf_clusters</th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
    </tr>
    <tr>
      <th>Top topic</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>14</td>
      <td>151</td>
      <td>91</td>
      <td>10</td>
      <td>175</td>
      <td>157</td>
      <td>28</td>
      <td>77</td>
      <td>32</td>
      <td>59</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10</td>
      <td>122</td>
      <td>44</td>
      <td>13</td>
      <td>138</td>
      <td>124</td>
      <td>21</td>
      <td>37</td>
      <td>39</td>
      <td>56</td>
    </tr>
    <tr>
      <th>3</th>
      <td>7</td>
      <td>79</td>
      <td>21</td>
      <td>4</td>
      <td>76</td>
      <td>114</td>
      <td>15</td>
      <td>25</td>
      <td>18</td>
      <td>43</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>30</td>
      <td>6</td>
      <td>1</td>
      <td>31</td>
      <td>84</td>
      <td>16</td>
      <td>8</td>
      <td>9</td>
      <td>20</td>
    </tr>
    <tr>
      <th>5</th>
      <td>1</td>
      <td>8</td>
      <td>0</td>
      <td>2</td>
      <td>12</td>
      <td>34</td>
      <td>6</td>
      <td>2</td>
      <td>7</td>
      <td>9</td>
    </tr>
    <tr>
      <th>6</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>13</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>6</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</div>
<div class="section" id="nlp-example-imdb">
<h2><span class="section-number">4.9. </span>NLP example - IMDB<a class="headerlink" href="#nlp-example-imdb" title="Permalink to this headline">¶</a></h2>
<p>In this example, we build a simple neural network model to predict the sentiment of movie reviews.</p>
<p>First, we load the IMDB data that is included in the <strong>Keras</strong> library (part of <strong>Tensorflow</strong>). Also, we load the <strong>preprocessing</strong> module.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.datasets</span> <span class="kn">import</span> <span class="n">imdb</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">preprocessing</span>
</pre></div>
</div>
</div>
</div>
<p>This is a dataset of 25,000 movies reviews from IMDB, labelled by sentiment (positive/negative). The reviews have been preprocessed, and each review is encoded as a list of word indexes (integers).</p>
<p>Words are ranked by how often they occur (in the training set) and only the <strong>num_words</strong> most frequent words are kept. Any less frequent word will appear as <code class="docutils literal notranslate"><span class="pre">oov_char</span></code> value in the sequence data. If we use <strong>num_words = None</strong>, all words are kept.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">imdb</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="n">num_words</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The following commands pad sequences to the same length, in this case, to 20 words.</p>
<p><strong>pad_sequences()</strong> creates a 2D Numpy array of shape (number of samples x number of words) from a list of sequences.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_train</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">sequence</span><span class="o">.</span><span class="n">pad_sequences</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">sequence</span><span class="o">.</span><span class="n">pad_sequences</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(25000, 20)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(25000,)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="densely-connected-network">
<h2><span class="section-number">4.10. </span>Densely connected network<a class="headerlink" href="#densely-connected-network" title="Permalink to this headline">¶</a></h2>
<p>We first build a traditional densely connected feed-forward-network. We also need an Embedding layer to code our words efficiently and a Flatten layer to transform our 2D-tensor to 1D-vector.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Embedding</span>
</pre></div>
</div>
</div>
</div>
<p>Our embedding layer codes 10000 words to 8-element vectors. The output layer has one neuron and a sigmoid-activation function that gives a probability for positive/negative. <strong>model.sequential()</strong> defines the network type, and the <strong>add()</strong> -functions are used to add layers to the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="mi">20</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Like with the examples of the computer vision section, we can stick with the <strong>RMSprop</strong> gradient descent optimiser. Because we are doing positive/negative classification, binary_crossentropy is the correct loss function. We measure the model performance with prediction accuracy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;rmsprop&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;acc&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>The model has 80161 parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 20, 8)             80000     
_________________________________________________________________
flatten (Flatten)            (None, 160)               0         
_________________________________________________________________
dense (Dense)                (None, 1)                 161       
=================================================================
Total params: 80,161
Trainable params: 80,161
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<p>The data is split into training and validation parts with 80/20% division. We go through the data ten times (<strong>epochs=10</strong>). The data is fed to the model in 32 unit batches and, thus, each epoch has 625 steps (32 * 625 = 20000). Our prediction accuracy with the validation data is 0.75. However, the model appears to be overfitting as the validation loss is increasing, and there is a wide gap between the training accuracy and the validation accuracy in the last epochs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span>
                    <span class="n">y_train</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                    <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/10
625/625 [==============================] - 1s 2ms/step - loss: 0.6664 - acc: 0.6326 - val_loss: 0.6148 - val_acc: 0.7040
Epoch 2/10
625/625 [==============================] - 1s 2ms/step - loss: 0.5389 - acc: 0.7514 - val_loss: 0.5266 - val_acc: 0.7302
Epoch 3/10
625/625 [==============================] - 1s 2ms/step - loss: 0.4611 - acc: 0.7863 - val_loss: 0.5022 - val_acc: 0.7434
Epoch 4/10
625/625 [==============================] - 1s 2ms/step - loss: 0.4223 - acc: 0.8091 - val_loss: 0.4948 - val_acc: 0.7552
Epoch 5/10
625/625 [==============================] - 1s 2ms/step - loss: 0.3964 - acc: 0.8225 - val_loss: 0.4934 - val_acc: 0.7582
Epoch 6/10
625/625 [==============================] - 1s 2ms/step - loss: 0.3738 - acc: 0.8342 - val_loss: 0.4964 - val_acc: 0.7578
Epoch 7/10
625/625 [==============================] - 1s 2ms/step - loss: 0.3538 - acc: 0.8462 - val_loss: 0.5011 - val_acc: 0.7562
Epoch 8/10
625/625 [==============================] - 1s 2ms/step - loss: 0.3347 - acc: 0.8584 - val_loss: 0.5081 - val_acc: 0.7564
Epoch 9/10
625/625 [==============================] - 1s 2ms/step - loss: 0.3171 - acc: 0.8663 - val_loss: 0.5143 - val_acc: 0.7548
Epoch 10/10
625/625 [==============================] - 1s 2ms/step - loss: 0.3000 - acc: 0.8749 - val_loss: 0.5226 - val_acc: 0.7516
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;bmh&#39;</span><span class="p">)</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;acc&#39;</span><span class="p">]</span>
<span class="n">val_acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_acc&#39;</span><span class="p">]</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span>
<span class="n">val_loss</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">]</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/10_NLP_in_accounting_174_0.png" src="_images/10_NLP_in_accounting_174_0.png" />
<img alt="_images/10_NLP_in_accounting_174_1.png" src="_images/10_NLP_in_accounting_174_1.png" />
</div>
</div>
<p>As our first improvement, we could try to use pre-trained embeddings in our model. Word embeddings include semantic information about our words (words appearing in similar contexts are close to each other). Pretrained embeddings are trained using vast amounts of text (billions of words). One could assume that the semantic information in these pre-trained embeddings is of higher quality and should improve our predictions. Let’s see…</p>
<p>To be able to use this approach, we need the original IMBD data. Search for aclimdb.zip from the internet.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
</pre></div>
</div>
</div>
</div>
<p>My raw data is in the <em>aclImdb</em> -folder under the work folder</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">imdb_raw</span> <span class="o">=</span> <span class="s1">&#39;./aclImdb/&#39;</span>
</pre></div>
</div>
</div>
</div>
<p>First, we define empty lists for the reviews and their sentiment labels. Then we collect the negative reviews from <em>./aclImdb/train/neg</em> -folder. We also add to the labels-list zero for these cases. A similar approach is repeated for the positive reviews. Thus, in our lists, we have first the negative reviews and the positive reviews.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">labels</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">texts</span> <span class="o">=</span> <span class="p">[]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Collect negative reviews</span>
<span class="n">train_neg_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">imdb_raw</span><span class="p">,</span><span class="s1">&#39;train&#39;</span><span class="p">,</span><span class="s1">&#39;neg&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">train_neg_dir</span><span class="p">):</span>
    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">train_neg_dir</span><span class="p">,</span> <span class="n">file</span><span class="p">))</span>
    <span class="n">texts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
    <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="n">labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Collect positive reviews</span>
<span class="n">train_neg_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">imdb_raw</span><span class="p">,</span><span class="s1">&#39;train&#39;</span><span class="p">,</span><span class="s1">&#39;pos&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">train_neg_dir</span><span class="p">):</span>
    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">train_neg_dir</span><span class="p">,</span> <span class="n">file</span><span class="p">))</span>
    <span class="n">texts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
    <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="n">labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Below is an example text and its’ sentiment (0=negative).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">texts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;There are some bad movies out there. Most of them are rather fun. &quot;Criminally Insane 1&quot; was one of those flicks. So bad that it was enjoyable and had re-watch value to it. &quot;Criminally Insane 2&quot; has to be one of the worst movies ever made and coming from me, that\&#39;s saying a lot because I am not the type of person to say anything is the worst. But trust me, this was just completely awful and running just 1 hour is 1 hour too long.&lt;br /&gt;&lt;br /&gt;The movie has a rather incoherent storyline, but who cares about story when all you want to see is a big fat woman running around killing people because she isn\&#39;t being fed. Well, you don\&#39;t see that in this movie, except for all of the flashback sequences that are from the first one. The new storyline could have been really funny with Ethel being sent to a halfway house and murdering everyone in there, but nothing happens until the last 20 minutes of the movie and at that point you are already falling asleep.&lt;br /&gt;&lt;br /&gt;The camera work in this movie is just atrocious. This literally reminds me of something I shot with friends of mine back when I was 15. The sound quality is something else as you can\&#39;t understand a word most of the characters are saying. To give an example of how bad it is, go into a New York Subway and try to understand what is being said over the loud speakers, that is what this movie sounds like. Not that it matters what they are talking about anyway because the actors are about as dry as a dead piece of wood.&lt;br /&gt;&lt;br /&gt;Now I know that saying this is the worst movie out there is pretty harsh but words can\&#39;t describe just how bad this movie is. If you don\&#39;t believe me, see it for yourself. 1/10&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">labels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0
</pre></div>
</div>
</div>
</div>
<p>We need Numpy and text-processing tools from the Keras libary.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
</div>
</div>
<p>The following commands tokenise words into vectors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The following commands transform each text in texts to a sequence of integers.</p>
<p>Only words known by the tokenizer will be taken into account. It will take into account only the 10000 most frequent words.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now, we use longer texts. We keep the 200 first words from each review.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The following command transforms the labels list to a numpy array.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(25000, 200)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">labels</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(25000,)
</pre></div>
</div>
</div>
</div>
<p>Because the reviews are in order (all the negative reviews first and then the positive reviews), we have to shuffle the data before feeding it to the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">25000</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>80 / 20 % separation of the data to training and validation parts.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_train</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:</span><span class="mi">20000</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[:</span><span class="mi">20000</span><span class="p">]</span>
<span class="n">x_val</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">20000</span><span class="p">:</span> <span class="mi">25000</span><span class="p">]</span>
<span class="n">y_val</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="mi">20000</span><span class="p">:</span> <span class="mi">25000</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>The Stanford NLP group offers GLOVE pre-trained embeddings. You can download them from <a class="reference external" href="https://nlp.stanford.edu/projects/glove/">nlp.stanford.edu/projects/glove/</a>. We use the glove6B.zip that is trained using 6 billion tokens. Each word is represented as a 100-dimensional vector.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># we use 100-dimensional vectors</span>
<span class="n">embeddings_index</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;./glove.6B/&#39;</span><span class="p">,</span> <span class="s1">&#39;glove.6B.100d.txt&#39;</span><span class="p">))</span>
<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="n">word</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">coefs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
    <span class="n">embeddings_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">coefs</span>
<span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>GLOVE has 400k tokens.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">embeddings_index</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>400000
</pre></div>
</div>
</div>
</div>
<p>We build the embedding matrix by going through our word index and adding its’ embeddings from the Glove model (if it is found).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">word_index</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">10000</span><span class="p">:</span>
        <span class="n">embedding_vector</span> <span class="o">=</span> <span class="n">embeddings_index</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">embedding_vector</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">embedding_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding_vector</span>
</pre></div>
</div>
</div>
</div>
<p>Because our model uses now 100-dimensional word vectors, the network also has a lot of more parameters. Our network also has a new 32-neuron dense layer after the Flatten-layer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="mi">200</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_5&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_5 (Embedding)      (None, 200, 100)          1000000   
_________________________________________________________________
flatten_3 (Flatten)          (None, 20000)             0         
_________________________________________________________________
dense_7 (Dense)              (None, 32)                640032    
_________________________________________________________________
dense_8 (Dense)              (None, 1)                 33        
=================================================================
Total params: 1,640,065
Trainable params: 1,640,065
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<p>We set the weights of the embedding layer using the Glove weights in the embedding matrix. The weights need to be locked so that we are not retraining them with our small dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_weights</span><span class="p">([</span><span class="n">embedding_matrix</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
</div>
<p>Again, we use the RMSprop optimiser, the binary_crossentropy loss function and accuracy as our performance metric.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;rmsprop&#39;</span><span class="p">,</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span><span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;acc&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span><span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/10
625/625 [==============================] - 2s 3ms/step - loss: 0.6591 - acc: 0.6439 - val_loss: 0.5575 - val_acc: 0.7138
Epoch 2/10
625/625 [==============================] - 2s 3ms/step - loss: 0.5045 - acc: 0.7554 - val_loss: 0.5326 - val_acc: 0.7366
Epoch 3/10
625/625 [==============================] - 2s 3ms/step - loss: 0.4155 - acc: 0.8091 - val_loss: 0.5454 - val_acc: 0.7398
Epoch 4/10
625/625 [==============================] - 2s 3ms/step - loss: 0.3605 - acc: 0.8390 - val_loss: 0.7307 - val_acc: 0.6964
Epoch 5/10
625/625 [==============================] - 2s 3ms/step - loss: 0.3113 - acc: 0.8618 - val_loss: 0.6184 - val_acc: 0.7422
Epoch 6/10
625/625 [==============================] - 2s 3ms/step - loss: 0.2661 - acc: 0.8821 - val_loss: 0.6603 - val_acc: 0.7364
Epoch 7/10
625/625 [==============================] - 2s 3ms/step - loss: 0.2195 - acc: 0.9068 - val_loss: 0.8240 - val_acc: 0.7044
Epoch 8/10
625/625 [==============================] - 2s 3ms/step - loss: 0.1820 - acc: 0.9250 - val_loss: 0.9258 - val_acc: 0.7028
Epoch 9/10
625/625 [==============================] - 2s 3ms/step - loss: 0.1494 - acc: 0.9391 - val_loss: 1.0480 - val_acc: 0.6916
Epoch 10/10
625/625 [==============================] - 2s 3ms/step - loss: 0.1251 - acc: 0.9506 - val_loss: 1.4292 - val_acc: 0.6804
</pre></div>
</div>
</div>
</div>
<p>Not a good performance. Heavy overfitting and worse accuracy. Let’s try something else.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;bmh&#39;</span><span class="p">)</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;acc&#39;</span><span class="p">]</span>
<span class="n">val_acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_acc&#39;</span><span class="p">]</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span>
<span class="n">val_loss</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">]</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training acc&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">,</span> <span class="s1">&#39;b--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation acc&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="s1">&#39;b--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/10_NLP_in_accounting_217_0.png" src="_images/10_NLP_in_accounting_217_0.png" />
<img alt="_images/10_NLP_in_accounting_217_1.png" src="_images/10_NLP_in_accounting_217_1.png" />
</div>
</div>
</div>
<div class="section" id="recurrent-neural-networks">
<h2><span class="section-number">4.11. </span>Recurrent neural networks<a class="headerlink" href="#recurrent-neural-networks" title="Permalink to this headline">¶</a></h2>
<p>Next thing that we can try is to use Recurrent neural networks. They are especially efficient for sequences like texts.</p>
<p><img alt="RNN" src="_images/rnn.svg" /></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">SimpleRNN</span>
</pre></div>
</div>
</div>
</div>
<p>Now, instead of a Flatten() layer, we have a SimpleRNN() layer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="mi">200</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">SimpleRNN</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_6&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_6 (Embedding)      (None, 200, 100)          1000000   
_________________________________________________________________
simple_rnn_2 (SimpleRNN)     (None, 100)               20100     
_________________________________________________________________
dense_9 (Dense)              (None, 1)                 101       
=================================================================
Total params: 1,020,201
Trainable params: 1,020,201
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<p>Again, we use the GLOVE weights.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load GLove wieghts</span>
<span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_weights</span><span class="p">([</span><span class="n">embedding_matrix</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_6&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_6 (Embedding)      (None, 200, 100)          1000000   
_________________________________________________________________
simple_rnn_2 (SimpleRNN)     (None, 100)               20100     
_________________________________________________________________
dense_9 (Dense)              (None, 1)                 101       
=================================================================
Total params: 1,020,201
Trainable params: 20,201
Non-trainable params: 1,000,000
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<p>Nothing has changed in the compile() and fit() -steps.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;rmsprop&#39;</span><span class="p">,</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span><span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;acc&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span><span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/10
625/625 [==============================] - 24s 38ms/step - loss: 0.6528 - acc: 0.6186 - val_loss: 0.6096 - val_acc: 0.6786
Epoch 2/10
625/625 [==============================] - 28s 45ms/step - loss: 0.5957 - acc: 0.6887 - val_loss: 0.5760 - val_acc: 0.7066
Epoch 3/10
625/625 [==============================] - 29s 46ms/step - loss: 0.5761 - acc: 0.7047 - val_loss: 0.5618 - val_acc: 0.7272
Epoch 4/10
625/625 [==============================] - 28s 44ms/step - loss: 0.5618 - acc: 0.7147 - val_loss: 0.5384 - val_acc: 0.7428
Epoch 5/10
625/625 [==============================] - 28s 45ms/step - loss: 0.5534 - acc: 0.7227 - val_loss: 0.5499 - val_acc: 0.7248
Epoch 6/10
625/625 [==============================] - 28s 45ms/step - loss: 0.5553 - acc: 0.7169 - val_loss: 0.5894 - val_acc: 0.6918
Epoch 7/10
625/625 [==============================] - 28s 45ms/step - loss: 0.5374 - acc: 0.7325 - val_loss: 0.5234 - val_acc: 0.7494
Epoch 8/10
625/625 [==============================] - 28s 45ms/step - loss: 0.5137 - acc: 0.7479 - val_loss: 0.5315 - val_acc: 0.7498
Epoch 9/10
625/625 [==============================] - 28s 45ms/step - loss: 0.5132 - acc: 0.7489 - val_loss: 0.5621 - val_acc: 0.7284
Epoch 10/10
625/625 [==============================] - 28s 44ms/step - loss: 0.4911 - acc: 0.7660 - val_loss: 0.6154 - val_acc: 0.6886
</pre></div>
</div>
</div>
</div>
<p>Well, overfitting is not such a serious problem any more, but the performance is not improving still.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;bmh&#39;</span><span class="p">)</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;acc&#39;</span><span class="p">]</span>
<span class="n">val_acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_acc&#39;</span><span class="p">]</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span>
<span class="n">val_loss</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">]</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/10_NLP_in_accounting_230_0.png" src="_images/10_NLP_in_accounting_230_0.png" />
<img alt="_images/10_NLP_in_accounting_230_1.png" src="_images/10_NLP_in_accounting_230_1.png" />
</div>
</div>
</div>
<div class="section" id="long-short-term-memory">
<h2><span class="section-number">4.12. </span>Long short-term memory<a class="headerlink" href="#long-short-term-memory" title="Permalink to this headline">¶</a></h2>
<p>As our last idea, we try the LSTM-version of RNN. It has achieved very good performance in practice, so, let’s hope for the best.
<img alt="lstm" src="_images/lstm.svg" /></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">LSTM</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="mi">200</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_7&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_7 (Embedding)      (None, 200, 100)          1000000   
_________________________________________________________________
lstm (LSTM)                  (None, 100)               80400     
_________________________________________________________________
dense_10 (Dense)             (None, 1)                 101       
=================================================================
Total params: 1,080,501
Trainable params: 1,080,501
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load GLove wieghts</span>
<span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_weights</span><span class="p">([</span><span class="n">embedding_matrix</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_7&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_7 (Embedding)      (None, 200, 100)          1000000   
_________________________________________________________________
lstm (LSTM)                  (None, 100)               80400     
_________________________________________________________________
dense_10 (Dense)             (None, 1)                 101       
=================================================================
Total params: 1,080,501
Trainable params: 80,501
Non-trainable params: 1,000,000
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;rmsprop&#39;</span><span class="p">,</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span><span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;acc&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span><span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/10
625/625 [==============================] - 8s 12ms/step - loss: 0.5607 - acc: 0.7138 - val_loss: 0.4525 - val_acc: 0.7844
Epoch 2/10
625/625 [==============================] - 8s 12ms/step - loss: 0.4275 - acc: 0.8061 - val_loss: 0.6175 - val_acc: 0.7750
Epoch 3/10
625/625 [==============================] - 7s 12ms/step - loss: 0.3619 - acc: 0.8458 - val_loss: 0.3399 - val_acc: 0.8524
Epoch 4/10
625/625 [==============================] - 8s 12ms/step - loss: 0.3254 - acc: 0.8616 - val_loss: 0.3284 - val_acc: 0.8594
Epoch 5/10
625/625 [==============================] - 7s 12ms/step - loss: 0.2937 - acc: 0.8752 - val_loss: 0.3091 - val_acc: 0.8728
Epoch 6/10
625/625 [==============================] - 7s 12ms/step - loss: 0.2712 - acc: 0.8884 - val_loss: 0.2998 - val_acc: 0.8716
Epoch 7/10
625/625 [==============================] - 8s 12ms/step - loss: 0.2455 - acc: 0.9002 - val_loss: 0.3528 - val_acc: 0.8560
Epoch 8/10
625/625 [==============================] - 7s 11ms/step - loss: 0.2222 - acc: 0.9116 - val_loss: 0.3320 - val_acc: 0.8600
Epoch 9/10
625/625 [==============================] - 8s 12ms/step - loss: 0.1977 - acc: 0.9230 - val_loss: 0.3221 - val_acc: 0.8760
Epoch 10/10
625/625 [==============================] - 8s 12ms/step - loss: 0.1756 - acc: 0.9322 - val_loss: 0.3241 - val_acc: 0.8684
</pre></div>
</div>
</div>
</div>
<p>Finally, we see some progress! Now the accuracy is around 87 %. So, a very significant improvement in performance. For the exact evaluation of performance, we should use a separate test set. However, the validation dataset accuracy gives a good indication of the performance of our model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;bmh&#39;</span><span class="p">)</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;acc&#39;</span><span class="p">]</span>
<span class="n">val_acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_acc&#39;</span><span class="p">]</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span>
<span class="n">val_loss</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">]</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/10_NLP_in_accounting_240_0.png" src="_images/10_NLP_in_accounting_240_0.png" />
<img alt="_images/10_NLP_in_accounting_240_1.png" src="_images/10_NLP_in_accounting_240_1.png" />
</div>
</div>
<p>As our final model, let’s test what kind of effect the predetermined weights have for the performance and train an LSTM model from scratch.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="mi">200</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">32</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_8&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_8 (Embedding)      (None, 200, 32)           320000    
_________________________________________________________________
lstm_1 (LSTM)                (None, 32)                8320      
_________________________________________________________________
dense_11 (Dense)             (None, 1)                 33        
=================================================================
Total params: 328,353
Trainable params: 328,353
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;rmsprop&#39;</span><span class="p">,</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span><span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;acc&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span><span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/10
625/625 [==============================] - 7s 11ms/step - loss: 0.4124 - acc: 0.8138 - val_loss: 0.3248 - val_acc: 0.8686
Epoch 2/10
625/625 [==============================] - 6s 10ms/step - loss: 0.2546 - acc: 0.9002 - val_loss: 0.3000 - val_acc: 0.8792
Epoch 3/10
625/625 [==============================] - 7s 11ms/step - loss: 0.2125 - acc: 0.9188 - val_loss: 0.3030 - val_acc: 0.8734
Epoch 4/10
625/625 [==============================] - 7s 11ms/step - loss: 0.1897 - acc: 0.9281 - val_loss: 0.3205 - val_acc: 0.8782
Epoch 5/10
625/625 [==============================] - 8s 14ms/step - loss: 0.1685 - acc: 0.9387 - val_loss: 0.3162 - val_acc: 0.8716
Epoch 6/10
625/625 [==============================] - 7s 11ms/step - loss: 0.1592 - acc: 0.9424 - val_loss: 0.3449 - val_acc: 0.8752
Epoch 7/10
625/625 [==============================] - 6s 10ms/step - loss: 0.1416 - acc: 0.9490 - val_loss: 0.3605 - val_acc: 0.8742
Epoch 8/10
625/625 [==============================] - 7s 11ms/step - loss: 0.1298 - acc: 0.9536 - val_loss: 0.3191 - val_acc: 0.8720
Epoch 9/10
625/625 [==============================] - 7s 11ms/step - loss: 0.1181 - acc: 0.9588 - val_loss: 0.3642 - val_acc: 0.8778
Epoch 10/10
625/625 [==============================] - 8s 13ms/step - loss: 0.1088 - acc: 0.9633 - val_loss: 0.3751 - val_acc: 0.8652
</pre></div>
</div>
</div>
</div>
<p>Because there are no locked parameters, the number of trainable parameters increases, and this causes some overfitting. However, the performance is at the same level as in the previous model. So, the predetermined weights do not appear to improve the accuracy, but they help at fighting overfitting.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;bmh&#39;</span><span class="p">)</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;acc&#39;</span><span class="p">]</span>
<span class="n">val_acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_acc&#39;</span><span class="p">]</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span>
<span class="n">val_loss</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">]</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/10_NLP_in_accounting_246_0.png" src="_images/10_NLP_in_accounting_246_0.png" />
<img alt="_images/10_NLP_in_accounting_246_1.png" src="_images/10_NLP_in_accounting_246_1.png" />
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="9_Deep_learning_in_accounting.html" title="previous page"><span class="section-number">3. </span>Deep learning in accounting</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Mikko Ranta<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>